{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3cc5bb-b250-4069-af88-ed3424ca8077",
   "metadata": {},
   "source": [
    "Model evaluation is the process of using different evaluation metrics to understand a machine learning model's performance, as well as its strengths and weaknesses. Model evaluation is important to assess the efficacy of a model during initial research phases, and it also plays a role in model monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cfd85-2781-4000-b747-617d1b7d705f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaaeaff-bad3-4a49-974d-58a45bbbdc68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0238e-2793-4467-b095-9b76537b2c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72336ff8-30cc-4981-8aa3-5dcb7136c8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1ae1a-5d9b-407a-88e6-d1f9b0bd35a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e808c51-1123-4a27-85da-2307a80d8827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20633ab0-2497-41e3-b265-57642762ad3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe89259-6e9f-4407-bf50-f84f2ef34170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c81d373e-6fb3-4e8a-ba0c-ccb8ca029b1f",
   "metadata": {},
   "source": [
    "Loss function is used when you ara still taining your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e86205d-aaf1-45f1-b240-b4cdcc35417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37dff6d-dd30-4a32-bbe9-9b0bad768685",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\python files - Copy\\\\1.01. Simple linear regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cb0df2-ff72-4230-b9a4-60fe032de303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA\n",
       "0  1714  2.40\n",
       "1  1664  2.52\n",
       "2  1760  2.54\n",
       "3  1685  2.74\n",
       "4  1693  2.83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833780f4-48b3-4cb4-b24a-855d4cdc134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['SAT']# x is called input or feature\n",
    "y = data['GPA'] # y is called output or taget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6197f1-afa9-41b2-aae9-818d679776cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc89143-7878-42ba-a29b-53c1d7552e2f",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21454aa5-eb76-4544-8d6b-5ab280a4461a",
   "metadata": {},
   "source": [
    "Model Evaluation tells us how our model\n",
    "\n",
    "performs in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b508f40-c386-4018-9115-60fdb47e906c",
   "metadata": {},
   "source": [
    "In the previous model we talk about in-sample evaluation.\n",
    "\n",
    ">. In-sample evaluation tells how well our model will fit the data used to train it.\n",
    "\n",
    ">. Problem?\n",
    "\n",
    ". It does not tell us how well the trained model can be used to predict new data.\n",
    "\n",
    "> Solution?\n",
    "\n",
    "> The solution is to use the in-sample data or training data.\n",
    "\n",
    ". Out-of-sample evaluation or test set\n",
    "\n",
    "This data is then used to approximate how the model perform in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4aea43-3752-4c16-9417-5d783ebace2e",
   "metadata": {},
   "source": [
    "Separating data into training and testing sets is an important part of model evaluation.\n",
    "\n",
    "We use the test data to get an idea how our model will perform in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabbf72-d3ef-4667-a4b2-295da39e3b85",
   "metadata": {},
   "source": [
    "When we spilt a dataset usually the larger portion of data is used for trainting and the smaller part is used for testing.\n",
    "\n",
    "For example ,we can use 70% of the data for testing and then use 30% for testing.\n",
    "\n",
    ". We use training set to build the model and train the model with a training set.\n",
    "\n",
    ". We use testing set to assess the performance of the a predictive model.\n",
    "\n",
    ". When we have completed testing our model we should use all the data to train the model to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fc129-045f-4056-939f-f1cd048d29c1",
   "metadata": {},
   "source": [
    "A popular function in the sycic learn package  for spliting dataset is the  train_test_split function.()\n",
    "\n",
    ". This function randomly splits the dataset into training and testing subsets.\n",
    "\n",
    "X_train,x_test,y_train,y_test = \n",
    "\n",
    "train_test_split(x_data,y_data,test_size=0.\n",
    "\n",
    "3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803dfcd-e4e1-4a48-b3f3-28afc9aef1e5",
   "metadata": {},
   "source": [
    "From the example code sniper, this method \n",
    "\n",
    "is imported from sklearn.cross validation\n",
    "\n",
    ". x_data:features or independent variables\n",
    "\n",
    " x_data is the list of predicted variables.\n",
    " In this case,it will be all the other variables in the car dataset we are using to try to predict the price.\n",
    " \n",
    "The input parameter y_data is the target and in our appraisal example ,it will be the price.\n",
    "\n",
    ". y_data:dataset target:df['price']\n",
    "\n",
    "\n",
    "The output is an array.\n",
    ". x_train,y_train: parts of the available data as training set.\n",
    "\n",
    "The subset for training:\n",
    "x_test,y_test: parts of available data as testing set.\n",
    "\n",
    "The subset for testing :\n",
    "test_size: percentage of the data for testing(here 30%).\n",
    "\n",
    "Random state:\n",
    "\n",
    "The random_state: number generator used for random sampling.  for random dataset splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2c3da-b7df-4262-bf0d-b73d10b3e988",
   "metadata": {},
   "source": [
    "#### Generalization Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1b141-4eb5-46c9-b21c-b76e51e1d8bc",
   "metadata": {},
   "source": [
    ".Generalization error is a measure of how well our data does at predicting previously unseen data.\n",
    "\n",
    "The error we obtained using our testing data is an approximation of this error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ded7e-6457-4e11-b78b-b2fe16717861",
   "metadata": {},
   "source": [
    "Lots of data for training\n",
    "\n",
    "Using alot of data for training gives us an accurate means of determining how well our model will perform in the real world.\n",
    "\n",
    "But the perception of the performance will be low.\n",
    "\n",
    "Lets clarify this with an example.\n",
    "\n",
    "the central disk bullzer represents the correct the correct generalization error.\n",
    "\n",
    "Lets say we take a random sample of the data using 90% of the data for training and 10% of the data for testing.\n",
    "\n",
    "The first time we experiment ,we get good estimate of the data.\n",
    "\n",
    "If we experiment again training the model with a different combination of samples we also get a good result.\n",
    "\n",
    "But the result will be different relative to the first time we run the experiment.\n",
    "\n",
    "Repeating the experiment again with a different combination of training and testing samples.\n",
    "\n",
    "Results are relatively close to the generalization error but distinct from each other.\n",
    "\n",
    "Repeating the process again we get good estimation of the generalization error but the perception is poor.\n",
    "\n",
    "Ie all the results are extremely different from one another.\n",
    "\n",
    "If we use fewer data point to train our model and more to test the model, \n",
    "\n",
    "the accuracy of the generalization performance will be less but the model will have good perception.\n",
    "\n",
    "The figure above demonstates this,all our error estimates are relatively closely \n",
    "\n",
    "together but they are further away from the true generalization performance.\n",
    "\n",
    "To overcome this problem we use cross validation\n",
    "\n",
    ".One of the most common out_of_sample evaluation metrics.\n",
    "\n",
    ". More effective use of the data(each observation is used for both training and testing.\n",
    "\n",
    "In this method the dataset is split into k- equal groups. Each group ids referred to as a fold.\n",
    "\n",
    "for example, four folds.\n",
    "\n",
    "Some of the folds can be used as a training set which we use to train the model and the \n",
    "\n",
    "remaining part are used as a test set which we use to test the model.\n",
    "\n",
    "For example, we can use 3 folds for training and use 1 fold for testing.\n",
    "\n",
    "this is repeated until each patition is used for both training and testing .\n",
    "\n",
    "At the end we use the average result as the estimate of our out of sample error.\n",
    "\n",
    "The evaluationmatric depends on the model.\n",
    "\n",
    "For example, the r-squared.\n",
    "\n",
    "The simplest way to apply cross validation is to call the cross_val_score () function\n",
    "\n",
    "Which performs multiple out of sample evaluations.\n",
    "\n",
    "This method is imported from sklearn model selection package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e71e4f-c069-4c1e-b545-f999273e8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50a271-9fd5-4f74-a57c-26e6fde96bf4",
   "metadata": {},
   "source": [
    "We then use the function, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8049b5c-8821-46c2-8372-8edb71619492",
   "metadata": {},
   "source": [
    "scores=cross_val_score(lr,x_data,y_data,cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa184ef2-c2d3-44ef-8401-3af338c4970c",
   "metadata": {},
   "source": [
    "The first input parameter is the type of model we are using to do the cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bce8c9-03ce-42b8-ae17-7c236c548ed6",
   "metadata": {},
   "source": [
    "In this example, we initialize the linear regression model or object _Ir which we pass the cross_val_score funcion.\n",
    "\n",
    "The other parameters are x_data, the predicted variable data and y_score the target variable data.\n",
    "\n",
    "We can manage the number of partitions with the cv parameters.\n",
    "\n",
    "Here cv =3 ,which means the data set is split into 3 equal partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533283f-edf0-4e40-8c53-f1032dd817ba",
   "metadata": {},
   "source": [
    "The functions equally returns an array of scores.\n",
    "\n",
    "One for each partition that was chosen as a testing set.\n",
    "\n",
    "We can the result together to estimate out of sample R-squared using the mean function in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7329ec3-2ea8-4a6c-a8ab-ad751ed6ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2397a-22da-434d-a9aa-259b24840918",
   "metadata": {},
   "source": [
    "Lets see an annimation\n",
    "\n",
    "Lets see the result of the score arrayed in the last slide.\n",
    "\n",
    "First,we split the data into 3 folds.\n",
    "We use 2 folds for training and the \n",
    "\n",
    "remaining fold for testing.\n",
    "\n",
    "The model will produce an output.\n",
    "We will use the output to calculate a score.\n",
    "\n",
    "In the case of the R2 our e coefficient are determined by annimation .\n",
    "\n",
    "we will store that value in an array. we will repeat the process using two folds for training and one fold for testing.\n",
    "\n",
    "Save the score than use the differnt combination for training and the remaining folds for testing.\n",
    "\n",
    "We store the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b5422-1b8a-4c00-bbbb-05a81bd912a6",
   "metadata": {},
   "source": [
    "Model =>  R2=(1- MSE of regression line / \n",
    "\n",
    "MSE of y)   =>  Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee27c2-1c2f-42b5-89c2-ebdea6a34e89",
   "metadata": {},
   "source": [
    "The cross_val_score retuns the cross value\n",
    "\n",
    "to tell us the validation result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf44b40-870f-4169-9996-44b311fae96b",
   "metadata": {},
   "source": [
    "The cross_val_predict()function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e3101-cb9e-413b-8aea-e5c5479da638",
   "metadata": {},
   "source": [
    ". It returns the prediction that was \n",
    "\n",
    "obtained for each element when it was in the test set.\n",
    "\n",
    ". Has a similar interface to \n",
    "\n",
    "cross_val_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e2b24-2d99-473c-96fb-a7502d9ad345",
   "metadata": {},
   "source": [
    "What if we want a little more information?\n",
    "\n",
    "What if we want to know the actual \n",
    "\n",
    "predicted value supplied by our model \n",
    "before the R2 values are calculated.\n",
    "\n",
    "to do this , we use the cross _val_predict()\n",
    "\n",
    "The input parameters are exactly thesame as the cross_val_score score()\n",
    "\n",
    "But the output is the prediction()\n",
    "\n",
    "lets illustrate the process.\n",
    "\n",
    "First,we split the data into 3 folds\n",
    "We use two folds for training and the remaining folds for testing.\n",
    "\n",
    "The model will produce an output and we will store in an array.\n",
    "\n",
    "wewill repeatthe process using two folds for training and one for testing.\n",
    "\n",
    "the model produces an output again.\n",
    "\n",
    "Finally, we use the last two folds for training.\n",
    "\n",
    "Then we use the testing data.\n",
    "The final testing fold produces an output.\n",
    "\n",
    "These predictions are stored in an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41086e40-7116-4690-8005-254072a77bc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr2e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4352/3711162095.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0myhat\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr2e\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lr2e' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "yhat= cross_val_predict(lr2e,X_data,y_data,cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4050f63-c88f-4af7-a46f-9ba5a915c6a3",
   "metadata": {},
   "source": [
    "### Overfitting,Underfitting and Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c9ad0-e851-4527-92ee-6a33dd3fd73c",
   "metadata": {},
   "source": [
    "In the last model we discussed polynomial regression.\n",
    "\n",
    "here we be discussing how to pick the pick the best polynomial order.\n",
    "\n",
    "and Problems that arise when picking the wrong order polynomial.\n",
    "\n",
    "Consider the following function:\n",
    "\n",
    "We assume the training point come from a polynomial function + some noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00757437-bef1-4120-b46e-09585a5c1cf2",
   "metadata": {},
   "source": [
    "\n",
    "Binary means zero and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50724657-18c0-4a9c-9c45-57c8250c330e",
   "metadata": {},
   "source": [
    "#y(x)+noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4873c-2459-4006-b6a6-d15475371bd0",
   "metadata": {},
   "source": [
    "The goal of model selection is to determine the order of the polynomial and \n",
    "\n",
    "to provide the best estimate of the function y(x).\n",
    "\n",
    "If we try to fit the function with the linear function, the line is not complex enough to fit the data.\n",
    "\n",
    "As a result ,there are many errors, this \n",
    "\n",
    "is called underfitting, where the model is too simple to fit the data .\n",
    "\n",
    "y = b0 + b1 x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016eeb8-a3bf-4f60-b2db-79f16d16a6f6",
   "metadata": {},
   "source": [
    "If we increase the order polynomial, the model fits better.\n",
    "\n",
    "But the model is still not flexible enough and exhibits under fitting.\n",
    "\n",
    "This is an example of the eight order polynomial used to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34746742-20de-4e64-85e2-5e242c7dde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = b0 + b1 x + b2 x^2 .... bn x^n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b71c0-d503-453f-8a39-38ab375f5aa9",
   "metadata": {},
   "source": [
    "We see the model does well at fitting the \n",
    "\n",
    "data and estimating the function even at the inflation points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a95ea4-8686-48f1-b434-e46d4e37a4be",
   "metadata": {},
   "source": [
    "Increaasing it to 16 order polynomial, the model does extremely well at tracking the \n",
    "\n",
    "point but performs poorly at estimating the function.\n",
    "\n",
    "This is a special appearance where there are little training data.\n",
    "\n",
    "The estimated functtion is not tracking the function.\n",
    "\n",
    "This is called overfitting where the model\n",
    "\n",
    "is too flexible and fix the noise rather than the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a08014-873d-4bb0-b8d1-8c00d14c482a",
   "metadata": {},
   "source": [
    "Lets look at the plot of the mean square \n",
    "\n",
    "error o ftraining and testing set of the different order polynomials.\n",
    "\n",
    "The horizontal axis represents the order\n",
    "\n",
    "of the polynomial and the vertical axis is the mean squared error.(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f188c-ab65-4c2f-9b9e-bdaf3ecf4805",
   "metadata": {},
   "source": [
    "Th training error decreases with the order of the polynomial.\n",
    "\n",
    "Test error is a better means of estimating the order of the polynomial.\n",
    "\n",
    "The error decreases till the best order of the polynomial is determined.\n",
    "\n",
    "Then the error begins to increase.\n",
    "\n",
    "We select the order that minimizes the test error and in this case is 8,\n",
    "\n",
    "Anything on the left is considered underfitting.\n",
    "\n",
    "Anything on the right is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4db0a-03f7-4b99-b133-b8061e0e99b0",
   "metadata": {},
   "source": [
    "If we select the best order of the \n",
    "\n",
    "polynomial, we will still have some errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61951383-7983-4116-b03b-ce24ec2c288d",
   "metadata": {},
   "source": [
    "If you recall the original expression for \n",
    "\n",
    "the training point.\n",
    "\n",
    "y(x)+noise\n",
    "\n",
    "We will see a noise term . this term is one reason for the error.\n",
    "\n",
    "This is because the noise is random and we can we can predict it .\n",
    "\n",
    "This is sometimes referred to as an irreduceable error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06197745-c0cc-457d-8a84-4c5f0ccec227",
   "metadata": {},
   "source": [
    "There are other sources of errors.\n",
    "\n",
    "For example, our polynomial assumption may be wrong.\n",
    "\n",
    "Our sample points may have come from a different function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae43553-9375-4c94-b4f9-076522b3cf0f",
   "metadata": {},
   "source": [
    "In the plot above the data is generated from a sign waves.\n",
    "\n",
    "The polynomial assumption does not do a good job at fitting the sign waves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0fb3a-e257-44ef-8e2b-8974ad3922e5",
   "metadata": {},
   "source": [
    "For real data ,the model may be too difficult to fit or we may not have the \n",
    "\n",
    "correct type of data to estimate the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847cb7ca-2c39-42dd-9cc0-a68dde227ca6",
   "metadata": {},
   "source": [
    "Lets try a different order polynomial from a real data using huse power.\n",
    "\n",
    "The red point represents the training data.\n",
    "\n",
    "The green point represents the test data.\n",
    "\n",
    "If we just use the mean of the data , our model does not perform well.\n",
    "\n",
    "A linear function does fit the data better.\n",
    "\n",
    "A second order model looks similar to the linear function.\n",
    "\n",
    "A third order function also appears to increase.\n",
    "\n",
    "Like the previous two orders, here we see a fourth order polynomial.\n",
    "\n",
    "At around 200 horse power, the predicted \n",
    "\n",
    "price suddenly decreases, this seems erroneous.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955719e-d934-4553-812a-c08c4f5c0bd5",
   "metadata": {},
   "source": [
    "Lets use R^2 to see if our assumption is correct.\n",
    "\n",
    "Model selection\n",
    "\n",
    "The following is a plot of the R^2 value\n",
    "\n",
    "R^2 using Test Data.\n",
    "\n",
    "The horizontal axis represents the order polynomial models.\n",
    "\n",
    "The closer the r^2 is to 1 , the more accurate the model is.\n",
    "\n",
    "Here we that the R^2 is optimal when the colour of the polynomial is 3.\n",
    "\n",
    "R^2 drastically decreases when the order is increased to 4.\n",
    "\n",
    "Validating our initial assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40237e41-ba09-4413-b731-e8a3be97ac46",
   "metadata": {},
   "source": [
    "We can calculate differnt R^2 values as follows:\n",
    "\n",
    "We create an empty list to store the values.\n",
    "\n",
    "We create a list containing different polynomial orders\n",
    "\n",
    "We then itirate through the list using  a loop\n",
    "\n",
    "We create a polynomial faeture od\n",
    "object with the order of the polynonial as a pareameter\n",
    "\n",
    "We then transform the training and testing into a polynomial using the fit_transform method\n",
    "\n",
    "We fit the regression model using the transformed data.\n",
    "\n",
    "We then calculate the R^2 data using the test data and store it in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b31cf9-4194-4cc8-ad05-4c10c88a8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rsqu_test=[]\n",
    "#order=[1,2,3,4]\n",
    "# for n in order:\n",
    "    pr=PolynomialFeatures(deree=n)\n",
    "    x_train_pr=pr.fit_transform(x_train[['horsepower']])\n",
    "    x_test_pr=pr.fit_transform(x_test[['horsepower']])\n",
    "    \n",
    "    lr.fit(x_train_pr,y_train)\n",
    "    # Rsqu_test.append(lr.score(x_test_pr,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c42578-47e5-454d-b8e0-64974cade1f1",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e853dd9-3f1d-4acb-8262-2ed0aa7e4136",
   "metadata": {},
   "source": [
    "#### Machine learning model Evaluation metrics\n",
    "\n",
    "##### by Maria Khaluasova // Developer Advocate // JetBrains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7ed5f-6280-4b7a-9148-c294757021f8",
   "metadata": {},
   "source": [
    "What is evaluation metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80122ab0-7938-4532-8302-96b844115e72",
   "metadata": {},
   "source": [
    "Evaluation metics is a way to quantify performance of a machine learning model.\n",
    "\n",
    "It is basically numbered and you can use it to compare different models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f9794-c699-4cc8-b7a1-3a79b7366c22",
   "metadata": {},
   "source": [
    "Evaluation metric is not thesame as loss function although it can be.\n",
    "\n",
    "Loss function is something you use when you are training a model ,ie while you are optimizing.\n",
    "\n",
    "Evaluation metric is used already on a trained machine learning model to see their result if it is any good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329d549-846f-4e2a-b602-f4ed2a9e77c0",
   "metadata": {},
   "source": [
    "Supervised learning metrics:\n",
    "    \n",
    "Below are some of the things we need to know ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c6045b7-656c-4bc8-9bdb-d9ef16515f90",
   "metadata": {},
   "source": [
    "Classification                  Regresion\n",
    "\n",
    "Classification accuracy \n",
    "\n",
    "Precision                          R^2\n",
    "\n",
    " Recall                            MAE\n",
    " \n",
    " F1 score                          MSE\n",
    " \n",
    " ROC/AUC                           RMSE\n",
    " \n",
    " Precision/Recall AUC               RMSLE\n",
    " \n",
    " Matthews correlation coefficient   ...\n",
    " \n",
    " Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db12711-2996-4e3b-bf5d-c9584b18b1d4",
   "metadata": {},
   "source": [
    "#### Classification Metrics\n",
    "\n",
    "##### Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d73ee0-9903-4caa-8704-ad17aa8dae05",
   "metadata": {},
   "source": [
    "Classification accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60c208-2cdc-4026-882f-3a17815b081d",
   "metadata": {},
   "source": [
    "Accuracy = Number of correct predictions /\n",
    "\n",
    "Total number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8b75a-608a-4947-9b70-a231090cd6a0",
   "metadata": {},
   "source": [
    "It ranges from 0 - 100% or 0 to 1.\n",
    "\n",
    "It is very intuitive.\n",
    "\n",
    "you can easily get it in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc88f7-7f07-4bc0-8fa7-ae22f76a8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import logistic regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train,x_test,y_train, y_test = train_test_split(x,y, test_size=0.2, \\\n",
    "                                                  random_state=42)\n",
    "# model = LogisticRegression().fit(x_train,y_train)\n",
    "# model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6920ab-b3ad-48ad-a32b-43a75ea7b12c",
   "metadata": {},
   "source": [
    "When we run this we got 96% . This looks ammazing but it may not be that good because we dont know the context.\n",
    "\n",
    "we will be seeing why this is not a necessarily good thing and in this case we will be using dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316a62a-b25d-483e-897e-fa8b5afcbc47",
   "metadata": {},
   "source": [
    "DummyClasifier on the same dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308ae9f-2edc-4a14-a97c-2cb0cd3b3e84",
   "metadata": {},
   "source": [
    "We will be using thesame dataset to build a Dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68aaa5-d898-488c-9108-fa9e740a469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequency', random_state = 42)\n",
    "dummy.fit(x_train,y_train)\n",
    "dummy.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc738fd-1127-4c41-8acf-60e03f6efc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.942"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8bea9-4839-4e39-b7e5-0729dde5e36c",
   "metadata": {},
   "source": [
    "Dummyclassifier doesnt learn anything from the data.\n",
    "\n",
    "It follows a simple strategy, it can generate predictions and uniforms among them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475c19a-bb88-47bd-ac16-1c95c7fa2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "From sklearn.datasets import mae_classidication\n",
    "\n",
    "x,y = make_classification(n_samples=10000, n_classes=2, \\\n",
    "                          weights=[0.95,0.05]. random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da5d2d-059f-416b-ae2e-bd784b8e068a",
   "metadata": {},
   "source": [
    "Classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ea2e0-76aa-4d31-b591-f558109eb37a",
   "metadata": {},
   "source": [
    "96% Accuracy\n",
    "\n",
    ". Is it a good model?\n",
    "\n",
    ". What errors is the model making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719300a5-4d92-4e5c-98c5-1132692a67da",
   "metadata": {},
   "source": [
    "#### Confusing matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92ea32-008a-4539-8c4b-68c189ae9ce9",
   "metadata": {},
   "source": [
    "When we get the data, after data cleaning, pre-processing, and wrangling, the first step we do is to feed it to an outstanding model and of course, get output in probabilities.\n",
    "\n",
    "But hold on! How in the hell can we measure the effectiveness of our model. Better the effectiveness, better the performance, and that is exactly what we want. And it is where the Confusion matrix comes into the limelight. Confusion Matrix is a performance measurement for machine learning classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ca9cd-7ffd-4f0b-bb2d-aeed90f83e7e",
   "metadata": {},
   "source": [
    ". Not a metric\n",
    "\n",
    ". Helps to gain insight into the type of errors a model is making.\n",
    "\n",
    ". Helps to understand some other metrics that are derived from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62efbb96-67e1-4154-baa2-69e3f8a9bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42).fit(x_train, y_train)\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28edee15-35b1-470c-b4e1-d148438eb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8340807174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b24cc-fc84-4d67-ae53-734014646c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.matrics import confusing_matrix\n",
    "confusion_matrix(y_test, model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb239658-5e16-488f-a729-a8dcbb8a839e",
   "metadata": {},
   "source": [
    "#### Precision,Recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9f192-0298-4a9f-a76b-e057bb8564bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECISION IS PRECITICITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a04fd7-9f2f-4462-8c0c-a07ffd12bdb4",
   "metadata": {},
   "source": [
    "Confusing matix can also be represented as tables.\n",
    "\n",
    "On the rows we have the actual values and on the columns you get the predictive negative.\n",
    "\n",
    "On the diagonal, we have True negatives and True positive.\n",
    "\n",
    "We also have false positive and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96b221-5c29-43de-9f5d-3ae0f318af50",
   "metadata": {},
   "source": [
    "we care about False positives ,we dont want to say this is the thing you want when it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3d71a-6e00-4910-97e8-dfb698478471",
   "metadata": {},
   "source": [
    "Precision is calculated by dividing  true positves by the sum of true positives and false positives .\n",
    "\n",
    "If we dont have any false positives , this could be 1.\n",
    "\n",
    "But if we have some false positives then we have to reduce the number.\n",
    "\n",
    "We will have to improve the matrix to be a bit closer to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878f77c-594b-4000-9a37-3059ca2e5d5c",
   "metadata": {},
   "source": [
    "Precision = TP / TP + FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba9d46-d642-43d4-a225-d5e6856671ad",
   "metadata": {},
   "source": [
    "For example, if we do something with medical diagnosis, we dont want to send people home saying that they dont have a disease when they do actually.\n",
    "\n",
    "In this case, we are caring more about false negatives.\n",
    "\n",
    "In this case we need 'Recall', it is very similar to precision but in this case, we \n",
    "\n",
    "care more about false negative and about little of them as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebbc8aa-841e-4408-abaf-08768766a853",
   "metadata": {},
   "source": [
    "Another way of summarizing the confusuion matrix in one number taking into camp boat  precision, recall and f1 score which is a nemonic meaning of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7564a-7736-4798-b4fa-cbb32be9dd17",
   "metadata": {},
   "source": [
    "##### Precision or Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c3a8a-25a0-4429-93bf-c866cfc33b39",
   "metadata": {},
   "source": [
    "Depending on your business problem, you might not care so much about accuracy but also about precision \n",
    "\n",
    "If you want to minimize false positives and recall if you want to minimize false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf33f5b-7c59-49bf-9758-807121ffc55a",
   "metadata": {},
   "source": [
    "RECALL IS ALSO CALLED SENSITIVITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de9263-1517-445c-bb6d-df8b38e387aa",
   "metadata": {},
   "source": [
    "What do you care about?\n",
    "\n",
    ". Minimizing False Positives => Precision \n",
    "\n",
    ". Minimizing False Negatives => Recall\n",
    "\n",
    "Unlike confusion matrix, these are actually numbers which you can compare and you can get them easily from sklearn.matrics just the same way we got the confusion matrix by passing the actions on  the predictions.\n",
    "\n",
    "We can use them in grid search if we want to choose a model that gives us better recall. we will put the scoring parameters like recall.\n",
    "\n",
    "In this case ,it will give us the best recall among the possible version of hypo parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69949d-9964-4481-a87d-d404b7a1932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2',0.5],\n",
    "}\n",
    "gs = GridSearchCV(estimator=model, param_grid=param_grid, \\\n",
    "                  scoring = 'recall', n_jobs = -1)\n",
    "\n",
    "gs.fit(x_train,y_train)  \n",
    "\n",
    "recall_score(y_test, gs.best_estimator_.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b02ace-e1bc-40f3-ad4b-909d81f1ca65",
   "metadata": {},
   "source": [
    "#### Matthews correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ee5ca-59fb-4abb-8189-6c176954be73",
   "metadata": {},
   "source": [
    "Another was to summarize the confusion matrics is called the matthews correlation coefficient.\n",
    "\n",
    "What is important to notice about this formula is that it takes into consideration all four of the confusion matrix and this is important because it makes it different from F1 scores and some properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab570b-8762-40bd-a950-5fd5e3048b0e",
   "metadata": {},
   "source": [
    "This sums up the whole confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6802e1-a6f3-449d-9a6b-3b3126a42d6f",
   "metadata": {},
   "source": [
    "Another way to sum up confusion matrix\n",
    "\n",
    "MCC = TP*TN-FP*FN  / √ (TP + FP)(TP+FN)\n",
    "\n",
    "(TN+FP)(TN+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e90d0-a770-40b5-934e-62821495f7a0",
   "metadata": {},
   "source": [
    "##### MCC vs F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92211a01-667c-46b8-bdd8-f59d0d9b9b46",
   "metadata": {},
   "source": [
    "Example:\n",
    "    \n",
    "Data: 100 samples, 95 positive, 5 negative\n",
    "\n",
    "Model : DummyClassifier\n",
    "\n",
    "We will get 95% accuracy on this\n",
    "\n",
    "we will calculate f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bab7f-5703-41bc-b1a7-7a564a5c906e",
   "metadata": {},
   "source": [
    "F1 score = 2 *TP / 2* TP + FP + FN =\n",
    "\n",
    "2*95 / 2 * 95 + 5 = 190 /195 = 0.974"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b0635-5d7f-4187-8ec5-d89a57beff10",
   "metadata": {},
   "source": [
    "MCC =  = TP*TN-FP*FN  / √ (TP + FP)(TP+FN)\n",
    "\n",
    "(TN+FP)(TN+FN)\n",
    "\n",
    "=  95 * 0 - 5 * 0 / √ 100 * 95 * 5 * 0 = undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bccbf-0ded-44a8-929f-228e666e30fc",
   "metadata": {},
   "source": [
    "Example 2:\n",
    "Lets say we have thesame data bu now we have now managed to pacify one negative\n",
    "We have :\n",
    "F1 score = 0.952\n",
    "MCC = 0.135\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981421e-5512-4f2c-8c01-4c3d11c85c7b",
   "metadata": {},
   "source": [
    "Here what we called positive we will call negative, the data is the same is just the way we say it :\n",
    "\n",
    "this is a positive class\n",
    "this is a negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e849d1c-58c7-49dd-af5c-ab08e0cf912e",
   "metadata": {},
   "source": [
    "Using thesame model, we basically get the same matrix.\n",
    "\n",
    "Every thing is same for this matix except F1 score that changed to  0.182.\n",
    "\n",
    "This is because f1  score takes true positives and false positives and does not care about false negaives.\n",
    "\n",
    "So f1 score is very sensitive to which class is  positive and which is negative  but MCC does not consider that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc81a77-9d8e-4564-87dc-e23964741c7f",
   "metadata": {},
   "source": [
    "If you want to summarize a matrix of a number for a binary problem , MCC gives you a better feeling of what is going on\n",
    "\n",
    "But on the down side ,it does not really send the work into a multi class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c50d04-fb0e-4154-a1b2-ef603cb9ac06",
   "metadata": {},
   "source": [
    "So far we have looked at matrics that takes into account whether a prediction is correct or not.\n",
    "\n",
    "We will be loking at probability matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e18004-5f76-4b19-a954-e4a7054349be",
   "metadata": {},
   "source": [
    "#### ROC (Receiver Operating Characteristic)curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08394cc3-2d14-4c55-89ec-495a8bce1edf",
   "metadata": {},
   "source": [
    "It is a comparison between the true positive rate and the false positive rate.\n",
    "\n",
    "\n",
    "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. \n",
    "\n",
    "The method was originally developed for operators of military radar receivers starting in 1941, which led to its name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3511f-c11f-4dc6-946b-082b525c0b4d",
   "metadata": {},
   "source": [
    "Roc shows a plot where you have  the false positve rate on the x axis and True positive rate on the y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c06a9-419b-4618-9515-eb79f2adb823",
   "metadata": {},
   "source": [
    "True Positive Rate = TP / TP + FN\n",
    "\n",
    "\n",
    "False Positive Rate = FP / FP + TN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b79f7e-1bb3-44dd-81ed-86d2b6be23b7",
   "metadata": {},
   "source": [
    "Why is it a curve and not a dot?\n",
    "\n",
    "This is where the probability threshold comes into play.\n",
    "\n",
    "When we have a probability generated belonging to one class or the other.\n",
    "\n",
    "By default it is \n",
    "50%, is a decision threshold where we say that the probability is larger and 50% where we say the probability is Positive class.\n",
    "\n",
    "You can also change this threshold.\n",
    "\n",
    "You can move it around, we can only say there is a positive class if 60 to 70% \n",
    "\n",
    "We can move the threshold of number of true positive and true negative is going to change because if something was classified as positive with 60% probability ,if you move the threshold to 80% ,it will show as mis-classified as false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a574b7b-3028-4ab8-95e4-8d949a8a0580",
   "metadata": {},
   "source": [
    "So here we will be moving the threshold and each threshold we calculate the true positive phrase for the model and the false positive phrase and we will apply it as a dot and then connect them with a curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293faacb-6450-4b15-9f9a-b714abe00fd5",
   "metadata": {},
   "source": [
    "The next question is, is it a good curve or a bad curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512f501-baa4-4143-8f43-fa2174ef9320",
   "metadata": {},
   "source": [
    "Here we can split the two classes without making any mistake.\n",
    "\n",
    "Here the true positive phrase will be 1 and the false positive phrase will be 0 \n",
    "\n",
    "because we will not have any false negative in this case, we will have false positives.\n",
    "\n",
    "Aside from this curve we can also use the precision /Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba08309-dd36-43d7-9988-c69e25f1a38e",
   "metadata": {},
   "source": [
    "#### AUC ( Area Under Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7c961-60d9-4a6a-b177-e8b7eb8188b9",
   "metadata": {},
   "source": [
    "This calculates  the percentage of the plot under this curve.\n",
    "\n",
    "It is easier to understand this matrix when you understand what this curve means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02adda8-7578-4e0f-b82f-32e7021258ba",
   "metadata": {},
   "source": [
    "True Positive Rate = TP / TP + FN\n",
    "\n",
    "False Positive Rate = FP / FP + TN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633a635-d982-4b19-a58b-b025274b1160",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Precision,Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc892081-7924-4fa3-884a-892cd859b1ac",
   "metadata": {},
   "source": [
    "Precision = TP / TP + FP\n",
    "\n",
    "Recall = TP / TP + FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa53089-ec31-4ec3-98f4-c062754c5504",
   "metadata": {},
   "source": [
    "This holds exactly thesame principle, you move the Threshold and you plot precision \n",
    "\n",
    "and recall in this case and you get a curve and you calculate the area under it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ebe4ae-761d-4f31-b56c-1a496045eb6c",
   "metadata": {},
   "source": [
    "It is important to look at the data and see if you have a class inbalance.\n",
    "\n",
    "We will be using thesame example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313d487-f0e5-4d3d-b788-7d9ab1ddcdf1",
   "metadata": {},
   "source": [
    "#### ROC Curve vs Precision/Recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287c48f-7c43-47ea-897d-1d5b921df874",
   "metadata": {},
   "source": [
    "This holds the same threshhold.\n",
    "\n",
    "You check for class inbalance.\n",
    "\n",
    "It also depends on the type of data you are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77849ffe-d19e-4c6b-b09d-87b7e411640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc-auc_score, \\\n",
    "    prcision_recall_curve, auc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config inlineBackend.figure_format = 'svg'\n",
    "\n",
    "x, y = make_classification(n_samples=1000, n_classess=2\\\n",
    "                           weights=[0.95, 0.05], random_state=42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \\\n",
    "                                                    random_state=2)\n",
    "model = LogisticRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f15d2-f025-4d6f-820f-3e07ae1b76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_probe(x_test)\n",
    "probs = probs[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7dc3b-631c-45ee-9f9f-d260aa27d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "sns.lineplot([0, 1], [0, 1],  linestyle='--')\n",
    "plt = sns.lineplot(frp, tpr, marker=' . ')\n",
    "auc_score = roc_auc_score(y_test, probs)\n",
    "print('AUC: %.3f' % auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4642f1-f9bf-4bb4-bbe8-d12f15a9e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, probs).\n",
    "pr_auc_score = auc(recall, precision)\n",
    "sns.lineplot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "plt = sns.lineplot(recall. precision, marker='.')\n",
    "print('AUC: %.3f' % pr_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107ca5a-f875-49e6-8d9f-b99b3ba9ecd0",
   "metadata": {},
   "source": [
    "#### Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b102a-c997-472a-b848-be7dc45479e4",
   "metadata": {},
   "source": [
    "Log loss is often used as loss function. \n",
    "\n",
    ". It takes into account uncertainty of model predictions\n",
    "\n",
    ". Larger penalty for confident false predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9123b39-98bf-4ec7-823b-9971aa6c9e6a",
   "metadata": {},
   "source": [
    "-1/n  i=1Σ^n( y1log p1 +(1 - y1)log(1-p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de49ccc4-9002-445e-abc8-669c860b00ec",
   "metadata": {},
   "source": [
    "where :\n",
    " \n",
    "N = number of observations\n",
    "\n",
    "y = in binary case, this is the true \n",
    "\n",
    "label(0 or 1)\n",
    "\n",
    "for an observation i\n",
    "\n",
    "p = the model's predicted probability\n",
    "\n",
    "that observation i is 1\n",
    "\n",
    "Note the minus sign in the formula is to \n",
    "\n",
    "make it a positive number .\n",
    "\n",
    "logarithm of\n",
    "\n",
    "Something smaller than 1 will be \n",
    "\n",
    "negative.\n",
    "\n",
    "So it will be difficult \n",
    "\n",
    "If we want to compare models with \n",
    "\n",
    "negative numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588761b6-6e16-4139-8944-c6a4a9f3ecdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Log loss intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d9052-66be-40ef-84a7-d7d9f0fe99a4",
   "metadata": {},
   "source": [
    "Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification).\n",
    "\n",
    "The more the predicted probability diverges from the actual value, the higher is the log-loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56fdcb-e07c-42a0-aa24-ceb16e244274",
   "metadata": {},
   "source": [
    "Log loss when true label = 1 \n",
    "\n",
    "then your log loss will be small\n",
    "\n",
    "The more wrong your predictions are ,the\n",
    "\n",
    "more confidence your model is in the wrong prediction,the log loss will sky rocket.\n",
    "\n",
    "Hence you will care about log loss  when dont just care about the accuracy of your\n",
    "\n",
    "prediction but also about how confident your model is and predictions you can make."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b3d278d-1eb7-4b83-8f6e-a3ac7ae1a9fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "True      Predicted prob.     log loss\n",
    "label    prob. of class 1    \n",
    "\n",
    "1            0.9           \n",
    "                         0.105360515657\n",
    "\n",
    "1            0.55                                                0.597837000755\n",
    "\n",
    "1            0.10                                                 2.30258509299\n",
    "\n",
    "0            0.95                                                2.995732273553"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0dfe7-ec54-4617-b8d7-c1426d11bdba",
   "metadata": {},
   "source": [
    "#### Classification Metrics\n",
    "\n",
    "#####  Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c0703-3ff2-4c05-93bf-64c9bebcfde7",
   "metadata": {},
   "source": [
    "In machine learning and statistical classification, multiclass classification or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).\n",
    "\n",
    "While many classification algorithms (notably multinomial logistic regression) naturally permit the use of more than two classes, some are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.\n",
    "\n",
    "Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a3887-0228-46a1-b36b-091d255f0499",
   "metadata": {},
   "source": [
    "#### COnfusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614113fb-3ad7-475a-af80-e3461ad46217",
   "metadata": {},
   "source": [
    "A confusion matrix presents a table layout of the different outcomes of the prediction and results of a classification problem and helps visualize its outcomes. It plots a table of all the predicted and actual values of a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17fab1e-019f-432f-820c-4fbdaaa84c51",
   "metadata": {},
   "source": [
    "You can plot confusion matrix for multi class problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907dd3fb-6c9a-46d9-97f2-68f42ec9b4c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decebf3-76bb-488b-951a-477fe7c9a568",
   "metadata": {},
   "source": [
    "You can also get precisions,recall and f1 score .\n",
    "\n",
    "For a multi class problem the notions of truwe positive and true negative dont really apply directly but this matrix can be extended o a multi class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba03f2-34a1-433b-862f-f2465b44f6cc",
   "metadata": {},
   "source": [
    "You can make it a multi class problem by calculating them , then averaging them\n",
    "\n",
    "We have more than 3 ways of averaging but company use the ones we mentioned below more:\n",
    "\n",
    "micro, macro and weighted averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbcfc15-49da-4eae-be6f-30b02f095447",
   "metadata": {},
   "source": [
    "Examples of  how Macro, Micro and weighted average are used."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b4beaab-5864-4811-8637-b4a94a7fc0bd",
   "metadata": {},
   "source": [
    "Label        Predicted\n",
    "\n",
    "cat          cat\n",
    "\n",
    "cat          cat\n",
    "\n",
    "cat          cat\n",
    "\n",
    "cat          cat\n",
    "\n",
    "dog          dog\n",
    "\n",
    "dog          dog\n",
    "\n",
    "dog          cat\n",
    "\n",
    "bird        dog\n",
    "\n",
    "bird        bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe51d08-4b3d-4eb5-b399-c7eb032d17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "actuals = [\"cat\", \"cat\", \"cat\", \"cat\",\"dog\",\"dog\",\"dog\",\"bird\",\"bird\"]\n",
    "predictions = [\"cats\",\"cats\",\"cats\",\"cats\",\"dog\",\"dog\",\"cats\",\"dog\",\"bird\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e7827-1090-4af2-bc74-d4fdb2af59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(actuals, predictions, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378947e-df25-453f-b89c-9eacbc13319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(actuals, predictions, average=\"micro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb717a5-02f5-4b50-a78b-5d6b81613a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(actuals, predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3543a92-0ddf-4587-9c8f-43a3caa6f2f1",
   "metadata": {},
   "source": [
    "Below is an example of how they are \n",
    "\n",
    "calculated for precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af9803-e10e-4e68-8d37-bed1036a0528",
   "metadata": {},
   "source": [
    "#### precision : micro, macro , weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60aae6-101b-48ad-a8de-20be592594a6",
   "metadata": {},
   "source": [
    "First we will make a confusion matrix.\n",
    "\n",
    "We will calculate each matrix for each label\n",
    "\n",
    "For each class \n",
    "We will treat them like one verses all problems\n",
    "\n",
    "This means for example, if it is a bird, \n",
    "\n",
    "we will need to calculate the True positiive for a bird,\n",
    "\n",
    "False positive will be where we said it was a bird  meanwhile it was not .\n",
    "\n",
    "We will need to sum it up\n",
    "\n",
    "False negative will be where we said it was not a bird meanwhile it was.\n",
    "\n",
    "\n",
    "We can do this for all our classes and calculate the total as well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "042fdca7-1948-4e5b-89fa-fedab5a465ad",
   "metadata": {},
   "source": [
    "             TP             FP\n",
    "bird          1             0\n",
    "\n",
    "cat           4              1\n",
    "\n",
    "dog           2              1\n",
    "\n",
    "TOTAL         7              2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be5872d1-c67b-4fa3-9caa-52a4665f6de1",
   "metadata": {},
   "source": [
    "        TP       FP   Pr.    N.samples\n",
    "        \n",
    "bird     1       0     1        2\n",
    "\n",
    "cat      4       1     0.8      4\n",
    "\n",
    "dog      2       1     0.6666    1\n",
    "\n",
    "TOTAL    7      2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bb78a-92b0-4723-b971-468c8267f558",
   "metadata": {},
   "source": [
    "To get the micro ,macro and weighted average which will add the number of \n",
    "\n",
    "samples here as a column and micro \n",
    "\n",
    "precision will be calculated by just using the total number of this model.\n",
    "\n",
    "Every sample equally contribute to the average each sample is represented in the average. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fee57d-f0a8-40d4-8262-7276ea4584f8",
   "metadata": {},
   "source": [
    "micro pr  =  7 /7 + 2   = 0.7777\n",
    "\n",
    "\n",
    "macro pr =  1/3(1 + 0.8 + 0.6666)  = \n",
    "\n",
    "0.8222\n",
    "\n",
    "weighted pr  =  1 * 2 + 0.8 * 4 + 0.6666 \n",
    "\n",
    "* 3 / 2 + 4 + 3  = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f999a7-cb2e-43c2-b217-be1c4b4b62e2",
   "metadata": {},
   "source": [
    "For  a macro of precision we will \n",
    "\n",
    "calculate a precision for class, \n",
    "\n",
    "for example for bird, it will be 1/1 + 1\n",
    "\n",
    "For cat it will be 4 / 4 + 1 * 0.8\n",
    "\n",
    "and we will take the average of that column.\n",
    "\n",
    "And in this way, every class regardless\n",
    "\n",
    "of its size will contribute equally to macro precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4f655-2afc-4663-bfcf-b2e9fc1a2b03",
   "metadata": {},
   "source": [
    "weighted precision is similar\n",
    "\n",
    "We will take precision calculated for class but we need to weight it by the \n",
    "\n",
    "number of samples and how they are represented in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603bfaa-0fc5-4ff5-8582-b380bc785584",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Micro, macro weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e14ee-3638-4279-bac8-9a0f1dc5bd84",
   "metadata": {},
   "source": [
    ". Micro-averaged: all samples equally contribute to th average\n",
    "\n",
    "\n",
    ". Macro -averaged: all classess equally contribute to the average\n",
    "\n",
    ". Weigted -average:weighs  each classes contribution to the average is weighted by its size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5d202-6cb5-422c-bab1-223834aaf23c",
   "metadata": {},
   "source": [
    "When do you apply each one ?  \n",
    "It depends on your data and the business problem\n",
    "\n",
    "If you have class inbalance data and you have one class under presented and you want to get it right.\n",
    "\n",
    "Maybe ,you want to use macro average to make sure the class contribution is amplified and on same level for other classess\n",
    "\n",
    "A multi- class log loss case is a general case of a binary log loss\n",
    "\n",
    "The intuition is exactly thesame though the fomula looks a little bit different.\n",
    "\n",
    "Is essentially the sum of every sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53743234-c413-484e-a614-340655858bef",
   "metadata": {},
   "source": [
    "Every time we will be taking y ,and the essense is to know if the label is god for the sample.\n",
    "\n",
    "Then, we multiply it by the logarithm of the probability of each label that is right for the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa05685-be8b-4e66-89e6-5f910d3e6415",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Multi- class log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff760a-78ed-45dd-9b7d-06af7c679129",
   "metadata": {},
   "outputs": [],
   "source": [
    "-1/N Σ^N i=1Σ^M j=1Yylog py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192c4fa-9c85-4ba1-953b-23b348cbf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "actuals = [\"cat\", \"cat\", \"cat\", \"cat\",\"dog\",\"dog\",\"dog\",\"bird\",\"bird\"],\n",
    "pred_proba = np.asarray([[0.1, 0.9, 0.4], [0.05, 0.8, 0.3]. [0.2. 0.95, 0.1],\n",
    "                         [0.05, 0.95, 0.2], [0.1, 0.3, 0.85], [0.2, 0.4, 0.8],\n",
    "                        [0.3, 0.7, 0.65], [0.7, 0.4, 0.8], [0.9, 0.3, 0.6]]) \n",
    "log_loss(actuals, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5d927-8335-491f-9258-bf00e9c875e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0,1,0], [0,1,0], [0,1,0], [0,1,0],\n",
    "              [0, 0, 1], [0, 0, 1],[0, 0, 1],\n",
    "              [1, 0, 0],[1, 0, 0]])\n",
    "eps=le-15\n",
    "p = np.clip(pred_proba, eps, 1- eps)\n",
    "p = p/p.sum(axis=1) [:,np.newaxis]\n",
    "N = 9\n",
    "N = 3\n",
    "logloss = 0\n",
    "for i in range(N):\n",
    "    logloss +- y[i][j]*np.log(p[i][j])\n",
    "logloss = - logloss/N\n",
    "logloss\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36419b99-989a-4ee6-abfb-db376909884a",
   "metadata": {},
   "source": [
    "You can use fore-loop to make it more intuitive.\n",
    "\n",
    "In sklearn ,it is done with vertarized operations.\n",
    "\n",
    "It is also a little more optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb431cd-28d7-4fb1-9f8b-4c36ab0e6055",
   "metadata": {},
   "source": [
    "#### Regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f57161-0d36-42ab-a7ea-f207599fb84e",
   "metadata": {},
   "source": [
    "Metrics for regression involve calculating an error score to summarize the predictive skill of a model. How to calculate and report mean squared error, root mean squared error, and mean absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574334b-3adf-47fc-a2a4-92f0391f49d1",
   "metadata": {},
   "source": [
    "They are used in continuous values.\n",
    "How to evaluate models based on individual residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffd465-a600-4df0-ac87-4813680a1f67",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### R Squared (coefficient of determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595ccd6-cc01-4de0-8874-72e5b3eb2493",
   "metadata": {},
   "source": [
    "All classifiers sklearn gives you a default evaluation metrics which is accuracy and for regressor ,it will be R Squared which is also called R Cofficient of determination\n",
    "\n",
    "The same way you can get accuracy with scoremath ,you can get R Squared with a scoremath for regressors.\n",
    "\n",
    "R Squared shows you how well mild predictions approximates the true values.\n",
    "\n",
    "and it will be 1 for perfectfit and 0 for DummyRegression Precision average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b7440-a1d1-40fc-8ac0-b8615e4eb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, targets, \\\n",
    "                                                    test_size=0.2,  \\\n",
    "                                                    random_state=42)\n",
    "model = RandomForestRegressor(max_features=0.5, n_estimators=20)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a4ae7-38db-4c7b-8b47-abf47deda192",
   "metadata": {},
   "source": [
    ". Indicates how well the model predictions approximate the true values.\n",
    "\n",
    "\n",
    ". 1 = perfect fit vs 0 = DummyRegressor predicting average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f49cc4-b499-4c87-a0c5-3dbdd452ed79",
   "metadata": {},
   "source": [
    "Why is it so ?\n",
    "\n",
    "This actually sterms from the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e95877-5dde-4483-b453-910b1d7e5a67",
   "metadata": {},
   "source": [
    "R^2(y, ŷ ) = 1-Σ^n i=1(yi - ŷi)2   / \n",
    "\n",
    " Σ^n i=1 (yi - ȳ)2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f57372-85ce-48eb-a297-bccc6743409c",
   "metadata": {},
   "source": [
    " for the numerator we have the sum of squared residuals and the difference between the predicted and actual values.\n",
    " \n",
    "In the denominator we have the squared distance from the actual values and the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197d836-32cd-47dd-9ae4-e10aeb654e81",
   "metadata": {},
   "source": [
    "if you have a model that just predicts the mean, the top and the button part will be exactly thesame.\n",
    "\n",
    "This should be 1-1 = 0 but if your model is doing better then predicting the R Squared you will have the shorter \n",
    "\n",
    "distance between the actual values and the predicted values compared to the distance between the actuals and the mean.\n",
    "\n",
    "This will be somewhat closer to the mean and also closer to 1.\n",
    "\n",
    "But it will go negative if you predict infinitive for everything.\n",
    "\n",
    "Everything will now be 1-infinity.\n",
    "\n",
    "Techinically that will be negative.\n",
    "it means that something is very wrong with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077c697-441c-4979-ae94-44b35205c66c",
   "metadata": {},
   "source": [
    "The good thing about this model is that it has intuitive scale.\n",
    "\n",
    "Like accuracy ,you get the percentage value.\n",
    "\n",
    "It does not depend on your target units which is a good thing.\n",
    "\n",
    "But it also does'nt give you any informtion about the prediction error for your predictions from the actuals.\n",
    "\n",
    "Often times you care about the errors a model is making.\n",
    "\n",
    "Most intuitive one is the mean absolute error.\n",
    "\n",
    "It is basically the average of the absolute values of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01cde9d-c1bc-403e-8044-062840ba001e",
   "metadata": {},
   "source": [
    "you can get it from sycik learn.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66334d0-bf07-4cfa-9f16-910f3ffd6c46",
   "metadata": {},
   "source": [
    "y = actual values,\n",
    "\n",
    " ŷ  = predicted values\n",
    "    \n",
    " ȳ   =  mean of the actual values\n",
    " \n",
    " \n",
    " R squared has an intuitive scale and doesn't depend on y units.\n",
    " \n",
    " R sqaured gives you no information about prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157a3e0-c4b9-4a81-b0bf-cb5d029f45fb",
   "metadata": {},
   "source": [
    "#### MAE: mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fbf6c-2e7d-4bc7-9051-d8dd053bc608",
   "metadata": {},
   "source": [
    "MAE(y, ŷ) = 1/n Σ^n i=1 |yi - ŷ|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf6123-fe5d-4029-94eb-36ce9f38d22c",
   "metadata": {},
   "source": [
    "This is the most intuitive one.\n",
    "\n",
    "It down plays outliers\n",
    "\n",
    "It totals your error and shows you all the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa3ea2-6147-4bdb-98c3-d57e64e815cc",
   "metadata": {},
   "source": [
    "Average of absolute value of the residuals.\n",
    "\n",
    "You can get it from siciklearn.metrix\n",
    " \n",
    "Another way of summarizing average of the residual value instead of absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394d0677-4583-437e-ac35-ee2f2b950c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_7932/3130055746.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_7932/3130055746.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    np.average(np.abs(y_true - y_pred)\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "np.average(np.abs(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c793420-eb87-4ce3-af9c-0328e4302208",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7932/1748908622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a72f4-722c-4c17-be9f-ffb8b54e99ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### MAE : mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59244b13-2490-4dd5-9554-88e29f85c0c5",
   "metadata": {},
   "source": [
    "This measures the goodness of fit\n",
    "\n",
    "It shows \n",
    "how much your predicted result is different from the actual result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc3d28-27dd-4293-9e18-5ed727396ad9",
   "metadata": {},
   "source": [
    "MSE(y, ŷ) = 1/n Σ^n i=1 (yi - ŷ)2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6aef3bb-5a86-4cc2-925b-54c3bbb156e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ȳ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7932/1572441600.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mȳ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ȳ' is not defined"
     ]
    }
   ],
   "source": [
    "ȳ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1be674-eaa5-454f-9938-b30dfc2a9c87",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (Temp/ipykernel_7932/135000511.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_7932/135000511.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    np.average(y_true - y_pred)**2)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "np.average(y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c6b2a-8893-48b0-9a82-6fad5c160116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0d500-cbbf-493a-a34e-b681ffe00214",
   "metadata": {},
   "source": [
    "Another way of summarizing average of the residual using instead of absolute values , we use squared values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652958c-ef4e-494d-b223-60a0f998db84",
   "metadata": {},
   "source": [
    "By so doing, the metrix is now  not in the units of the target values, but rather in the sqaured units of the target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13bd4f-c3dd-4a19-b660-bb74af7d21d5",
   "metadata": {},
   "source": [
    "#### RMSE: root mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff194ea2-d2d3-429b-a554-4b00abfd6138",
   "metadata": {},
   "source": [
    "Most commonly you wil see RMSE wg=hich is basically the root of the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a3d6761-dd69-4c42-a63f-76731ff52d4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (Temp/ipykernel_7932/331045490.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_7932/331045490.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    RMSE(y, ŷ ) = √ 1 / n Σ^n i=1(yi - ŷi)2\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "RMSE(y, ŷ ) = √ 1 / n Σ^n i=1(yi - ŷi)2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a190e0-f595-4811-82b4-c6c5799d2dc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7932/2992856193.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "np.sqrt(np.average((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01317d7-1944-41c2-84b9-7f6242b10f72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7932/3819878964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrsme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "rsme = np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a350d04-636a-41df-b31f-101efa5a8a67",
   "metadata": {},
   "source": [
    "#### What do MAE and RMSE have in common?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a983c2-9997-4148-bc76-a4a682cbf517",
   "metadata": {},
   "source": [
    ". Range :0 -> ∞ (infinity)\n",
    "\n",
    ". MAE and RMSE have the same units as y values\n",
    "\n",
    ". Indifferent to the direction of the errors\n",
    "\n",
    ". The lower the metric value , the better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e65866-681b-499f-9e68-b30eb0234942",
   "metadata": {},
   "source": [
    "#### MAE vs RMSE : what is the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e224d-dc9f-489d-942c-15be6004d2f9",
   "metadata": {},
   "source": [
    ". RMSE gives a relatively high weight to larger errors.\n",
    "\n",
    ".MAE is more robust to outliers\n",
    "because they are not squared.\n",
    "\n",
    ".RMSE is differentiable ,it is often used as a loss function\n",
    "\n",
    "RMSE is also use as a loss function but for MAE it doesnt matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6f44a-ec9f-476f-aece-8f8f087ac001",
   "metadata": {},
   "source": [
    "There is a lot of debate.\n",
    "\n",
    "Alot of tutorials that favours the use of MAE(mean absolute error) for evaluating metrics.\n",
    "\n",
    "This is because\n",
    "RMSE disappropriates and mis interpretes the error.\n",
    "\n",
    "It is important to note that neither of these erors will be good enough for small test, for example when you have less than a 100 example\n",
    "\n",
    "\n",
    "In practice, we will have more examples in a test.\n",
    "\n",
    "In practice, RMSE is good to use if you really want to down play the outliers, you can use absolute errors as a second metric .\n",
    "\n",
    "In most case, RMSE seems to be doing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89865717-062f-4c1f-bfcd-07ba511a7694",
   "metadata": {},
   "source": [
    "##### MAE vs RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908b838-8d5c-4b2a-aa68-f96e9ab951a9",
   "metadata": {},
   "source": [
    ". Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assssing average model performance,cort J.Willmott.Kenji Matsuura.2005.\n",
    ". Root mean square error (RMSE) or mean absolute error(MAE)?\n",
    "Training Chai.R .R . Draxier, 2009.\n",
    "\n",
    ". Neither metric is robust on small test set(<100)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e10d8-89c6-40f2-957b-3bb42efd0ec2",
   "metadata": {},
   "source": [
    "#### RMSLE: root mean squared logarithmic error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe2bbf-2b3a-43b3-b05d-38dd516bd74a",
   "metadata": {},
   "source": [
    "It uses natural log of y + 1 instead of 1\n",
    "\n",
    "note there is no zero in log\n",
    "\n",
    "it shows us the relative error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998faf42-1554-412b-9cf7-26ad05201e9a",
   "metadata": {},
   "source": [
    "This is a version of RMSE that is used often . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae806f4b-1bca-45f0-bdde-f02269d9b100",
   "metadata": {},
   "source": [
    "#### RMSLE(y,ŷ)  =    \n",
    "\n",
    "√ 1/n  Σ^n (log e (ŷi + 1) - log e (ŷi + 1))2                     \n",
    "       i=1                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78a1f227-7ea0-4f11-b521-d578418d2600",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7932/2557513186.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrmsle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_log\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_squared_log' is not defined"
     ]
    }
   ],
   "source": [
    "rmsle = np.sqrt(mean_squared_log or (y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8d623-676d-4da5-b098-d12b9b521e60",
   "metadata": {},
   "source": [
    ". This is very much Similar to RMSE: it \n",
    "\n",
    "uses natural logarithm of (y+1) instead of y\n",
    "\n",
    ". +1 because log of 0 is not defined\n",
    "\n",
    ". Shows relative error and important in\n",
    "\n",
    "cases where your target has exponential growth.\n",
    "\n",
    "For exampleif your purchasing price has\n",
    "\n",
    "a wide range and you have a $5 error \n",
    "\n",
    "against a $50 will be a large error but\n",
    "\n",
    "if it is a $5 error on\n",
    "\n",
    "\n",
    "a $50,000 is not\n",
    "\n",
    "a big deal.\n",
    "\n",
    "logarithm helps you to level these things.\n",
    "\n",
    ". Penalizes under-predicted estimate \n",
    "\n",
    "more than over-predicted which is also \n",
    "\n",
    "some times useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d2ea3-6252-4ea6-a05e-befff360f712",
   "metadata": {},
   "source": [
    "#### Take Aways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409b6da-4a5c-465b-af19-d2eadf76fa71",
   "metadata": {},
   "source": [
    "There's no \"one fits all\" evaluation metric \n",
    "\n",
    "Get to know your data\n",
    "\n",
    "Keep in mind business objection of your ML problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ed9f7-e146-409c-80b2-03c0119bf581",
   "metadata": {},
   "source": [
    "There is a reason for the variety \n",
    "\n",
    "\n",
    "you need to know all your data, else \n",
    "\n",
    "your metric may not make sense.\n",
    "\n",
    "You need to know your business problems \n",
    "\n",
    "and understand what your model should care more about.\n",
    "\n",
    "In some cases , one metric will help \n",
    "\n",
    "you more in achieving your final goal \n",
    "\n",
    "and in some cases others.\n",
    "\n",
    "So you always need to start with \n",
    "\n",
    "knowing your data and knowing your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029e009-fece-4d00-a943-ead7acc062f5",
   "metadata": {},
   "source": [
    "#### REcommended resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b2622-374d-4f3d-bc86-16523fc52385",
   "metadata": {},
   "source": [
    ". Scikit-learn-User Guide\n",
    "\n",
    ". http://wiki.fast.al/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1673c-3994-4fbd-8152-7ee9c3e000f7",
   "metadata": {},
   "source": [
    "\"Root mean square error (RMSE) or mean absolute error(MAE)?\" by Tianfeng Chai.R.R Draxler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe172b82-f177-4600-a6fc-5707da23cbb7",
   "metadata": {},
   "source": [
    ". Tip 8 from \"Ten quick tips for machine learning in computational biology\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a738-6f7e-4398-a329-c1fa09325cb7",
   "metadata": {},
   "source": [
    ".\"Macro and micro-averaged evaluation measures\" by vincent van Asch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25422f51-e8da-42fd-86c6-8d43a86c74d0",
   "metadata": {},
   "source": [
    "To learn more about open source technology solutions  visit www.anaconda.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b0dee-effa-42d4-b8b4-dfa7a4c56d40",
   "metadata": {},
   "source": [
    "Model deployment is the process of putting machine learning models into production. \n",
    "\n",
    "This makes the model's predictions available to users, developers or systems, \n",
    "\n",
    "so they can make business decisions based on data, interact with their\n",
    "\n",
    "application (like recognize a face in an image) and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360700b4-2f7d-47ed-a7de-f7b7f5b8affe",
   "metadata": {},
   "source": [
    "Deploying a model into production represents the end of the iterative\n",
    "\n",
    "process that includes Feedback, Model Refinement, and Redeployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311fc92f-93ec-4c02-89f2-3e400d8e9beb",
   "metadata": {},
   "source": [
    "### Take home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19fd3-704f-4cb1-8544-f117e865d746",
   "metadata": {},
   "source": [
    "#### Normal Distribution Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45216cb7-0c02-41e5-9776-10f1627a43eb",
   "metadata": {},
   "source": [
    "Normal Distribution Exercise\n",
    "\n",
    "Example: \n",
    "\n",
    "(Computing non-standard Normal probabilities).\n",
    "\n",
    "Suppose that the average household income in some\n",
    "\n",
    "country is 900 coins, and the standard \n",
    "\n",
    "deviation is 200 coins.\n",
    "\n",
    "Assuming the Normal distribution of incomes,\n",
    "\n",
    "compute the proportion of “the middle class,”\n",
    "\n",
    "whose income is between 600 and 1200 coins.\n",
    "\n",
    "Solution.\n",
    "\n",
    "Standardize and use Table A4. \n",
    "\n",
    "For a Normal (μ = 900, σ = 200) \n",
    "\n",
    "variable X,\n",
    "\n",
    "P{600 < X < 1200} =P {600 – μ < X – μ <\n",
    "\n",
    "1200 − μ}\n",
    "\n",
    "σ σ σ\n",
    "\n",
    "= P {600 – 900 < Z < 1200 – 900} = P \n",
    "\n",
    "{-1.5 < Z < 1.5}\n",
    "\n",
    "200 200\n",
    "\n",
    "= ɸ (1.5) - ɸ (−1.5) = 0.9332 − 0.0668\n",
    "\n",
    "= 0.8664.\n",
    "\n",
    "So far, we were computing probabilities\n",
    "\n",
    "of clearly defined events.\n",
    "\n",
    "These are direct problems. \n",
    "\n",
    "A number of\n",
    "applications require solution of an inverse problem, \n",
    "\n",
    "that is, finding a value of x given the corresponding\n",
    "\n",
    "probability.\n",
    "\n",
    "Exercise: \n",
    "\n",
    "Refer to the country in the Example above, \n",
    "\n",
    "where household incomes follow Normal\n",
    "\n",
    "distribution with μ = 900 coins and σ =\n",
    "\n",
    "200 coins.\n",
    "\n",
    "(a) A recent economic reform made \n",
    "\n",
    "households with the income below 640 \n",
    "\n",
    "coins qualify for a free bottle of milk\n",
    "\n",
    "at\n",
    "\n",
    "every breakfast. What portion of the \n",
    "\n",
    "population qualifies for a free bottle \n",
    "\n",
    "of milk?\n",
    "\n",
    "(b) Moreover, households with an income\n",
    "\n",
    "within the lowest 5% of the population\n",
    "\n",
    "are entitled to a free sandwich.\n",
    "\n",
    "What income qualifies a household to \n",
    "\n",
    "receive free sandwiches?\n",
    "\n",
    "Hint: See Chapter 4 - Continuous \n",
    "\n",
    "Distributions for a thorough \n",
    "\n",
    "understanding of Normal Distribution and\n",
    "\n",
    "Chapter 12 - Appendix for access to \n",
    "\n",
    "Table A4 fro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9fca9-558e-4f8f-9e4c-a1c90151d0d0",
   "metadata": {},
   "source": [
    "#### hypothesis testing Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812e4e3-de09-4cc1-a9f2-f78abba66e9c",
   "metadata": {},
   "source": [
    "Hypothesis Testing Exercise\n",
    "\n",
    "Question:\n",
    "\n",
    "Below are details of weight Loss for \n",
    "\n",
    "Diet vs Exercise sample\n",
    "\n",
    "Diet Only:\n",
    "\n",
    "sample mean = 5.9 kg\n",
    "\n",
    "sample standard deviation = 4.1 kg\n",
    "\n",
    "sample size = n = 42\n",
    "\n",
    "standard error = SEM1 = 4.1/ √42 = \n",
    "\n",
    "0.633\n",
    "\n",
    "Exercise Only:\n",
    "\n",
    "sample mean = 4.1 kg\n",
    "\n",
    "sample standard deviation = 3.7 kg\n",
    "\n",
    "sample size = n = 47\n",
    "\n",
    "standard error = SEM2 = 3.7/ √47 = \n",
    "\n",
    "0.540\n",
    "\n",
    "measure of variability = [(0.633)2 + \n",
    "\n",
    "(0.540)2] = 0.83\n",
    "\n",
    "Did dieters lose more fat than the\n",
    "\n",
    "exercisers?\n",
    "\n",
    "With the details above, perform the \n",
    "\n",
    "following tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b78b4-aa96-4a1a-9b47-3d919f1c7dcc",
   "metadata": {},
   "source": [
    "1. Determine if the null or alternative \n",
    "\n",
    "hypothesis would be accepted or \n",
    "\n",
    "rejected as the\n",
    "\n",
    "case may be\n",
    "\n",
    "a. Null hypothesis:\n",
    "\n",
    "No difference in average fat lost in \n",
    "\n",
    "population for two\n",
    "\n",
    "methods. \n",
    "\n",
    "Population mean difference is zero.\n",
    "\n",
    "\n",
    "b. Alternative hypothesis:\n",
    "\n",
    "There is a difference in average fat \n",
    "\n",
    "lost in population\n",
    "\n",
    "for two methods.\n",
    "\n",
    "Population mean difference is not zero.\n",
    "\n",
    "2. Collect and summarize the data into \n",
    "\n",
    "a statistic i.e., find the “z” value\n",
    "\n",
    "3. Determine the p-value\n",
    "\n",
    "4. Make a decision regarding task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f033d6-fd66-4bef-a2ba-3dd6790470df",
   "metadata": {},
   "source": [
    "#### AB testing exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b8af0-49ff-43cc-a87e-85812c56867c",
   "metadata": {},
   "source": [
    "AB Testing Data Analysis Exercise\n",
    "\n",
    "Executive Summary\n",
    "\n",
    "Two web pages (simply called ‘new_page’\n",
    "\n",
    "and ‘old_page’) were distributed to \n",
    "\n",
    "users to\n",
    "investigate if the landing page that \n",
    "\n",
    "the users visited impacted their conversion rate.\n",
    "\n",
    "The users\n",
    "of the old page were selected to be in\n",
    "\n",
    "the control group, while its variant, the new page, was\n",
    "\n",
    "provided to users in a contrast group. \n",
    "\n",
    "The number of users that were converted after visiting\n",
    "\n",
    "each page was recorded.\n",
    "\n",
    "\n",
    "Data\n",
    "\n",
    "The data can be downloaded from the link in a description below.\n",
    "\n",
    "\n",
    "Hypothesis\n",
    "\n",
    "\n",
    "Investigate if the landing page that a user visits has any effect on their conversion rate. \n",
    "\n",
    "The\n",
    "null hypothesis is that there is no difference in effectiveness of the new_page in converting\n",
    "\n",
    "users compared to the old_page. Check\n",
    "\n",
    "to see if the null hypothesis could be rejected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da776d-c9fd-4667-9dc0-735ba6829879",
   "metadata": {},
   "source": [
    "Task\n",
    "\n",
    "1. Explore the data using pandas\n",
    "\n",
    "• Check the shape of the data: \n",
    "\n",
    "There was an excess of around 400,000 \n",
    "\n",
    "users that\n",
    "\n",
    "visited the web-pages more than once (possibly visited both web-pages).\n",
    "\n",
    "\n",
    "• Check the number of unique entries \n",
    "\n",
    "from the “user_id” column\n",
    "\n",
    "\n",
    "Having some users access both pages would make the results of the analysis invalid.\n",
    "\n",
    "\n",
    "Therefore, the users in the control group were paired only with the ‘old’ landing page and\n",
    "\n",
    "\n",
    "those in the treatment group were paired only with the ‘new’ landing page\n",
    "\n",
    "\n",
    "2. With the information above, determine the number of users that landed on both\n",
    "\n",
    "\n",
    "pages (old and new) and drop the duplicate entries.\n",
    "\n",
    "\n",
    "3. Plot a bar chart to visualise the number of users in the control group and treatment\n",
    "\n",
    "\n",
    "groups.\n",
    "\n",
    "4. Plot a bar chart to show the proportion of users that were converted\n",
    "\n",
    "by either webpages against those that were not converted to investigate the \n",
    "\n",
    "overall effectiveness\n",
    "of either pages in converting users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa434f1a-8463-40a3-8be5-200dfb7079c8",
   "metadata": {},
   "source": [
    "5. Calculate the mean for the number of\n",
    "\n",
    "users that were converted for the \n",
    "\n",
    "control group\n",
    "\n",
    "and that for the treatment group.\n",
    "\n",
    "6. Plot a bar chart showing the\n",
    "\n",
    "proportion of users that were converted in the control\n",
    "\n",
    "condition vs the treatment condition for comparison.\n",
    "\n",
    "7. Determine if the null hypothesis could be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28195177-e99e-4ecb-836d-4d50a67f0f8e",
   "metadata": {},
   "source": [
    " #### Module evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9971a74-7981-4149-9ebd-649485034c77",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "\n",
    "In pattern recognition and classification, \n",
    "\n",
    "we use precision(fraction of retrieved instances that are relevant) and recall(fraction of relevant instances that are retrieved).\n",
    "\n",
    "\n",
    "In other words precision helps us understand whether there is overlap in the model and whether the model can distinguish between the items. \n",
    "\n",
    "\n",
    "Recall, also called sensitivity, helps us gauge how many positives we can identify out of all the positives.\n",
    "\n",
    "Suppose we design a model to identify iphones from a video that also contain android phones.\n",
    "\n",
    "\n",
    "If the program identifies 5 iphones in \n",
    "a scene containing 7 iphones and some android phones. \n",
    "\n",
    "\n",
    "if 3 of the identification are correct \n",
    "\n",
    "but 2 are actually android phones.\n",
    "\n",
    "\n",
    "a. What is the precision of the model?\n",
    "\n",
    "b. What is the recall of the model?\n",
    "\n",
    "Exercise 2\n",
    "\n",
    "Suppose we created the model and want to assess how well it predicted.\n",
    "\n",
    "We will compare the actual value from \n",
    "\n",
    "the Test set to the predicted value \n",
    "\n",
    "that is derived from the model.\n",
    "\n",
    "FALSE TRUE\n",
    "\n",
    "0 94 23\n",
    "\n",
    "1 24 100\n",
    "\n",
    "Using the confusion matrix above, \n",
    "\n",
    "answer the following questions\n",
    "\n",
    "a. Number of true positives?\n",
    "\n",
    "b. Number of false negatives?\n",
    "\n",
    "c. Number of true negatives?\n",
    "\n",
    "d. Number of false positives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60549b0b-11e1-4bbe-b2c4-0dd99c141127",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ba330-e818-44c8-a902-62e767c3d69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ad29d-d9c2-4197-a912-d7afbe9e5098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b34ed-dc27-4930-891a-e3f14549ffed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a36b7-a4f8-4872-9721-f45cc46ee08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dbb30-1736-4d19-a673-4f8a295f4f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e674283-2255-4853-8083-ffa59a8d216b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df183a10-a1cc-4e47-9e2c-85a7cbc2fed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46049ebe-8d50-499c-9944-850fcdaefd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706deb90-a267-4281-854b-d951c3627a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2548c-2e50-4297-a716-644a3f176987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8c126-1306-4ce4-81ca-8ffa45aaa9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f1ef9-5cd6-46fa-ad07-1e6f6995aac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d917f90-a15b-49ee-834f-1678e5ecccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc5f57-ab71-4924-982c-4674847ec586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a926be-06da-4d4b-b2f0-1d1b300b72a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20f433-a235-4d58-a52b-143f493c68d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486bb9a-dbf7-4353-b7f0-f819ac901f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ee401-ed2c-4e12-b78b-ba1fe6d50379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f066f451-cb0b-4fcd-a597-2325407e409c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2570c2d-1ab2-4687-b9ce-a274025b501f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53cb17-4656-4203-88ca-0facb9090cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf6161-0062-492c-b321-05856a0fbefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03223b10-2f6d-47a3-9abd-4af21867358e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bad09-54d7-4610-b9fd-4e1d5b6955cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b857d-b844-481c-bca1-31816f5599bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941e0d1-d035-46ca-857a-8dd3faeb1a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e531821-440e-4119-8007-d4646755a3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e2188-3a4f-406d-a994-2bf3ed7c4faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5004de-4dbb-4c88-88ff-8b0f9552fb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
