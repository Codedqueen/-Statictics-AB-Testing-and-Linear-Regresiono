{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66957186-8131-42c4-8e76-1c6dab395fd5",
   "metadata": {},
   "source": [
    "## Week 11: Day 1 - K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d550bd-5452-44ed-bf28-7b3db06c4c98",
   "metadata": {},
   "source": [
    "### K- Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af537edb-7fdc-4376-b3b8-a9e5cfbdd661",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Intro to KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e4ccb-4c17-457b-b9d9-1cae5e507355",
   "metadata": {},
   "source": [
    "##### What is K-Nearest Neighbor(or KNN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560c3ad-6346-4a7c-87be-cff2021f9278",
   "metadata": {},
   "source": [
    "The KNN algorithm is a classification algorithm that takes a bunch of level points and uses them to learn how to label other points.\n",
    "\n",
    "+ This algorithm classifies case based on their similarity to other cases.\n",
    "\n",
    "+ In KNN data points that are ner each other are said to be \"neighbors\"\n",
    "\n",
    "+ KNN is based on similar cases with same value class labels are near each other, thus the distance between two cases is a measure of their disimilarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec268d-7934-4891-8fd1-d66023a640a9",
   "metadata": {},
   "source": [
    "K is used to identify unidentified number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4658950-cdc4-40ee-a28e-0c1435423112",
   "metadata": {},
   "source": [
    "##### Intuition behind KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac64ca6-76d9-4aae-86ef-023de032385e",
   "metadata": {},
   "source": [
    "Imagine that a tele communication provider has segmented its costumer base by service usage product categorizing the customers into 4 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f1cef-5b88-4ec1-8300-3cff666f980f",
   "metadata": {},
   "source": [
    "Value   Label\n",
    "\n",
    "1       Basic Service \n",
    "\n",
    "2       E-service\n",
    "\n",
    "3       Plus Service \n",
    "\n",
    "4       Total Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684eb506-156a-4bef-be51-1cee4e6b1c43",
   "metadata": {},
   "source": [
    "If Demographic data can be used to explain group membership, the company can customize offer for individual prospective customer. \n",
    "\n",
    "This is a classification problem ie given the dataset with pre-defined labels we need to build a model to be used to build the class of a new or unknown case.\n",
    "\n",
    "\n",
    "The example focuses on using demopgraphic data such as region , age and marital staus to predict usage product.\n",
    "\n",
    "The target filled called 'custcat' has four possible values that corresponds to the four customer groups as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c50e40-4c0f-472e-ac83-f598e726fe7c",
   "metadata": {},
   "source": [
    "Value   Label\n",
    "\n",
    "1       Basic Service \n",
    "\n",
    "2       E-service\n",
    "\n",
    "3       Plus Service \n",
    "\n",
    "4       Total Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3f511-abdd-4c7a-80ce-43b90e0adfdd",
   "metadata": {},
   "source": [
    "Our objective is to build the classifier for example, using the rows 0-7 to predict the class of row 8 using specific type of classification called K_Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0d010-8266-4dd5-a429-2f26f6d38d25",
   "metadata": {},
   "source": [
    "For demonstration ,lets use only two fields as predictors, specifically ,age and income and then plot the customers based on their group membership"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c9a16-8fa7-4b39-84a8-add2e84c9801",
   "metadata": {},
   "source": [
    "Now,lets say that we have a new customer for example,record number 8 with a known age (50) and income ($166) how can we find the classs of this customer?\n",
    "\n",
    "Can we find one of the closest cases and assign the same class level to our new customer?\n",
    "\n",
    "Can we also say that the class of our new customer is most probably group 4  ie total service because its closest neighbour is also of class 4?\n",
    "\n",
    "Yes we can, infact ,it is the first nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230387a-88bd-49eb-8729-1af7e5e675fd",
   "metadata": {},
   "source": [
    "1-NN + 4: total Service\n",
    "\n",
    "The question is,to what extent can we trust our judgement.\n",
    "\n",
    "Which is based on the 1st nearest neighbor, it might be a poor judgement especially if our first nearest neighbor is a very specific case or an outlier,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7d024-e677-4249-a812-412e4b48e79e",
   "metadata": {},
   "source": [
    "Taking a look at our scatter plot again, rather than choose the first nearest neighbor we can choose the 5 nearest neighbour and do a majority vote among them to define the class of the new customer.\n",
    "\n",
    "In this case , we see that 3 out of 5 nearest neighbor tell us to go for class 3, which is Plus service.\n",
    "Doesnt this make more sense?\n",
    "\n",
    "It actually does and in this case the value of k nearest neighbor is 5,\n",
    "\n",
    "5-NN + 3: total Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fc982-f1d3-4398-b156-7ee7f8a2ed0f",
   "metadata": {},
   "source": [
    "This example, highlights the intuition behind the K-nearest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7deb22c-8a09-4342-97cb-2cb632e5286e",
   "metadata": {},
   "source": [
    "There are different ways to calculate dissimilarity or conversely the distance or dissimilarity of two data point.\n",
    "\n",
    "For example this can be done using Euclenian distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e48cf-041b-427c-81f6-b0fcac306903",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### How the KNN works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718af52-b606-4687-9e96-c036087c9b55",
   "metadata": {},
   "source": [
    "In a classification problem, KNN algorithm works as follows:\n",
    "\n",
    "+  Pick a value for k\n",
    "\n",
    "+ Calculate the distance of unknown case from all cases in the dataset.\n",
    "\n",
    "+  Search for the data of  the K-observations in the training data that are \"nearest\" to the  maesurement unknown data point.\n",
    "\n",
    "+ Predict the response of the unknown data point using the most popular response value from the K-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df638e50-78da-4964-a82d-26883c00701c",
   "metadata": {},
   "source": [
    "There are two parts in this algorithm that might be a bit confusing.\n",
    "\n",
    "+  How to select the correct k \n",
    "\n",
    "+  How to compute the similarity between cases. For example among customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f184f1-bf8e-479d-9c8c-af9030d6e313",
   "metadata": {},
   "source": [
    "We will start with the second concern , ie how to calculate the similarity between two data points.\n",
    "\n",
    "Assuming we have two customers, customer 1  and customer 2 and  lets assume that these two customers have only one feature age,Customer 1 ,Age (34)\n",
    "\n",
    "    Customer 2 ,Age (30)\n",
    "\n",
    "we can easily use a specific type of \n",
    "Mancuskick ...distance to calculate the distance of these two customers,\n",
    "\n",
    "This is called the Eucledian distance\n",
    "\n",
    "Dis (X1,X2) = √ Σ^n i=0(X1i – X2i )2\n",
    "\n",
    "Calculating the similarity / distance in a 1- dimentional space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b01540-b3a9-4edd-841e-18188cbfe9a0",
   "metadata": {},
   "source": [
    "Dis (X1,X2) = √ Σ^n i=0(X1i – X2i )2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d43a6-99e7-45c4-8371-6eae3c586d60",
   "metadata": {},
   "source": [
    "Dis tance of x1 from x2 is root of 34 minus 30 to power of 2 which is 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5de5a-1eb7-49c3-bd9d-dec32ca5b3a5",
   "metadata": {},
   "source": [
    "Dis (X1,X2) = √ (34 - 30 )2 = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fcd47-953c-462e-af9f-e1deb3ecadec",
   "metadata": {},
   "source": [
    "What if we have more than one feature , for example Age and Income,\n",
    "\n",
    "If we have age and income and age for each custome , we can still use thesame formula in a two dimensional space\n",
    "\n",
    "\n",
    "    Customer 1 ,Age (34)  Income (190)\n",
    "\n",
    "    Customer 2 ,Age (30)  Income (200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119dc06-558a-4069-a6d5-1fcf5834aae0",
   "metadata": {},
   "source": [
    "Dis (X1,X2) = √ (34 - 30 )+ (190 -200)2 = 10.77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca1ec1-f351-4aca-a0cf-7f2961636125",
   "metadata": {},
   "source": [
    "We can also use thesame matrix for multi dimensional vectors\n",
    "\n",
    "We have to normalize our feature set to get the accurate dissimilarity measure.\n",
    "\n",
    "There are also other dissimilarity measures that can be used for this purpose but it is highly dependent on data type and also determined by classification is done for.\n",
    "\n",
    "As said earlier, K in kNn is the number of nearest neighbors that is to be examined. It is supposed to be specified by the user.\n",
    "\n",
    "So how do we choose the right K , Assume that we want to find the class of the customer noted as question mark in our dataset.\n",
    "\n",
    "What happens if we choose a very low value of k, lets say k = 1, from our diagram, the first nearest point will be blue which is class 1, this will be a bad prediction since more of the points around it are gender which is class 4.\n",
    "\n",
    "Since, its nearest neighbor is blue, we will say that our data captured the noise in the data or one of the data that is an anormally in the data.\n",
    "\n",
    "A low value of k causes a highly complex model as well which might result in over-fitting of the model,it means the prediction process is not generalized enough to be used for out of sample cases.\n",
    "\n",
    "Out of sample data is data that is outside of the dataset used to train the model. In otherwords , it cannot be trusted again to be used for prediction of unknown samples.\n",
    "\n",
    "It is important to remember that overfitting is bad as we want a general model that works for any data and not just a data used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89c305-05c4-40f0-b312-e8f892bbf389",
   "metadata": {},
   "source": [
    "Now on the opposite side of the spectrum, if we choose a very high value of k\n",
    "\n",
    "k = 20\n",
    "\n",
    "the anormally becomes overly generalized so how can we find the best value for k .\n",
    "\n",
    "The general solution is to reserve a part of your data for testing the accuracy of the model.\n",
    "\n",
    "Once you have done so, choose k =1 , then use the training part for modelling and calculate the accuracy of prediction using all samples in your test set.\n",
    "\n",
    "Repeat this process ,increasing the k and see if k is best for your model,\n",
    "\n",
    "For example, in our case, k=4 will give us the best accuracy for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709bf95-875e-47ae-b993-846a1f05402f",
   "metadata": {},
   "source": [
    "+ K-Nearest Neighbors analysis can also be used to compute values for a continuous target (regression)\n",
    "\n",
    "In this situation the average or median target value of the knn is used to obtain the predicted value for the new case.\n",
    "\n",
    "For example,assume that you are predicting tje price of a home based on its feature set such as number of rooms , square footage, the year it was built etc\n",
    "\n",
    "you can easily find the 3 nearest neighbor houses not only based on distance but also based on all the attributes and then predict the price of that house as the median of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f8414-294e-4c64-a603-458851090d06",
   "metadata": {},
   "source": [
    "### How KNN Algorithm Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c3f55-b9a6-467c-a206-f4f23048371d",
   "metadata": {},
   "source": [
    "#### What is in it for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205b526-b5ec-41c7-a889-324dfc0ebd75",
   "metadata": {},
   "source": [
    "> Why do we need KNN?\n",
    "\n",
    "> What is KNN?\n",
    "\n",
    "> How do we choose the factor k?\n",
    "\n",
    "> When do we use KNN?\n",
    "\n",
    "> How does KNN Algorithm work ?\n",
    "\n",
    "> Use case predict whether a person will have daibetics or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84359e-0c16-4f2c-821b-e70dd72365a5",
   "metadata": {},
   "source": [
    "#### Why KNN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bd79a-0c6e-40a5-b7c3-e9277b9f71ec",
   "metadata": {},
   "source": [
    "It is the popular and commonly used dataset as far as testing out model and  learning how to use the differnt models in machine learning is concerned.\n",
    "\n",
    "By now we all know that Machine lerning models make prdictions by learning from the past data avaiable.\n",
    "\n",
    "We have our input values - machine learning models - predicted output.\n",
    "\n",
    "Example, we can differenciate betwen a cat and a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9f928-0189-4f60-9507-dd0abc4314c2",
   "metadata": {},
   "source": [
    "Because KNN is based on feature similarity, we can do classification using KNN Classifier.\n",
    "\n",
    "Input value - KNN - Predicted Output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb00b8-5f3a-44d1-b442-ad8be3842b84",
   "metadata": {},
   "source": [
    "#### What is KNN Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f8cba-aebd-4d59-a718-11a5774a2fc3",
   "metadata": {},
   "source": [
    "KNN - K Nearest Neighbors, is on of the simplest Supervised Machine Learning algorithm mostly used for Calssification.\n",
    "\n",
    "Example, is it  a dog or not a dog, a cat or not a cat\n",
    "\n",
    "it classifiers a data point based on how its neighbors are classified. \n",
    "\n",
    "KNN stores all available cases and classifiers new cases based on a similarity measure.\n",
    "\n",
    "KNN is a parameter that refers to the number of nearest neighbors to include in the majority voting process.\n",
    "The example is identifying th nearset neighbor to a red wine.\n",
    "\n",
    "From the dataset ,we have 5 nearest neighbor\n",
    "A data point is classified from its % nearest neighbors.\n",
    "Hers, the unknown point would be classified as red ,since 4 out of 5 neighbors are red "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716cef7-4935-497b-8a7b-5c7d5de86b18",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### How do we choose the factor k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eec795-1d4e-4e81-8a6f-14b5b2c948b9",
   "metadata": {},
   "source": [
    "How do we know that k=5\n",
    "\n",
    "KNN Algorithm is based on feature similarity. Choosing the right value of k is a process called parameter tuning and is important for better accuracy ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b440a-97d0-4372-a944-57adf37d4a43",
   "metadata": {},
   "source": [
    "If we say k =3 we will be looking at the 3 nearest neighbors.\n",
    "And if we put k=7 we will classify as a triangle depending on what the data around it are.\n",
    "As the k changes depending on what that point is,it drastically changes your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485f1bc-9b64-4022-83b2-7315db851faa",
   "metadata": {},
   "source": [
    "Now how do we choose the factor of k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9523e-3652-4685-a09e-14d5903e9bb5",
   "metadata": {},
   "source": [
    "We find it often in machine learning , choosing this factors, is the phase you get, did i choose the right k? Did i set it right as in the values? So that you dont have a huge bias in one direction or the other.\n",
    "\n",
    "In terms of KNN, the number of k you choose is too low, the bias will be too noisy, it will pick the noise and you may get a skewed answer and if your k is\n",
    "too big, it may take forever to process ,and you will run into processing issues\n",
    "and resource issues.\n",
    "\n",
    "The class of unknown data point was at k=3 but changed at k=7, so which k should we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c533c8-73bf-4c9c-8572-0fc98472da6e",
   "metadata": {},
   "source": [
    "So to choose a value of k: \n",
    "\n",
    "> Sqrt(n), where n is the total number of data points.\n",
    "Coosing the squareroot of the total number of values you have.\n",
    "\n",
    "> Odd values of k is selected to avoid confusion between two classes of data\n",
    "In most cases if you have even numbers , you may want to make your k values odd and that makes you select better.\n",
    "And you can have a balanced between two differnt factors.\n",
    "If is even you substract or add one to it to nmake it odd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbf3ad-707b-4f7f-b8eb-19b1b61d56d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### When do we use KNN Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a5654-6dc3-4dcc-b8c1-e29d53b7d16f",
   "metadata": {},
   "source": [
    "we can use KNN when: \n",
    "\n",
    "+ Data is labeled example Dog\n",
    "\n",
    "+ Data is noise free\n",
    "\n",
    "+ when Dataset is small, because KNN is a lazy learner ie it doesn't learn a discrimative function from the training set.\n",
    "\n",
    "iF you have very complicated and large amount of data, you cannot use KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b55bf9-5681-49ad-b484-ae8006f7051d",
   "metadata": {},
   "source": [
    "#### How does KNN Algorithm work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8ae39-1289-4126-9af4-25eb03d1ba7d",
   "metadata": {},
   "source": [
    "Consider a dataset having two variables: height (cm) & weight (kg) and each point is classified as Normal or Underweight.\n",
    "\n",
    "On the basis of the given data, we have to classify the  below data as normal or underweight using KNN,Assuming we dont know how to calculate BMI\n",
    "\n",
    "57kg       170cm       ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4c83a-c417-471e-8236-3f92710a55d2",
   "metadata": {},
   "source": [
    "To find the nearest neighbors,we will calculate Euclidean distance.\n",
    "\n",
    "What is Eucleadian distance?\n",
    "\n",
    "According to the Euclidean distance formula , the distance between two points in the plane with coordinates(x,y) and (a,b) is given by:\n",
    "\n",
    "dist(d) =  √(x-a)2 + (y -b)2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829bc388-2b44-4bce-9ad9-0214b6bc0def",
   "metadata": {},
   "source": [
    "Lets calculate it clearly to understand clearly:\n",
    "\n",
    "The dist(d1) = √(170-167)2 + (57-51)2 -=6.7 \n",
    "\n",
    "dist(d2) =  √ (170-182)2 + (57-62)2 -= 13\n",
    "\n",
    "dist(d3) = (170-176)2 + (57-69)2 -= 13.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f33c6-14a2-492f-93a3-35fc2e71bb26",
   "metadata": {},
   "source": [
    "Similarly, we will calculate Euclidean distance of unknown data point from all the points in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53c4c0-9619-4958-a765-8c85152ad6aa",
   "metadata": {},
   "source": [
    "Where (x1,y1) = (57,170) whose class we have to classify, the Eucledian distance has to be the nearest neighbors.\n",
    "\n",
    "Now ,lets calculate the nearest neighbor of k=3\n",
    "\n",
    "So majority neighbors are pointing towards 'Normal'\n",
    "\n",
    "Hence, as per KNN algprithm the class of (57,170) should be 'Normal'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80514d1-e26a-4c4f-b6b2-9c19ac93f358",
   "metadata": {},
   "source": [
    "#### Recap of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e142d4-3f5d-4cab-a88b-5c413fea3d74",
   "metadata": {},
   "source": [
    "+ A positive integer k is specified, along with a new sample\n",
    "\n",
    "+ We select the K enteries in our database which are closet to the new sample.\n",
    "\n",
    "+ We find the most common Classification of these enteries\n",
    "\n",
    "+ This is the classification we give to the new sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1bd9d-5d12-41c1-bb5b-ab941c47e7c0",
   "metadata": {},
   "source": [
    "#### Use Case: Predict Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750668a9-2f77-4aa6-bdba-3272af98ca82",
   "metadata": {},
   "source": [
    "Objective : Predict whether a person will be diagnosed with diabetes or not\n",
    "\n",
    "We have a dataset of 768 people who were or were not diagnosed with diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3a725-7fd5-4d22-b32c-3a11d99c0ebc",
   "metadata": {},
   "source": [
    "##### KNN -Predict whether a person will have diabetes or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612d4993-cf91-4322-aa8d-827f0cb095db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16d039-96a3-4e28-b409-4ed2c5c8c58c",
   "metadata": {},
   "source": [
    "##### load the dataset and have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298ebb7-5037-45c9-af20-e6d576baff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C\")\n",
    "print( len(dataset) )\n",
    "print( data.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b1be0-52bd-44ae-8dee-ab78a706f018",
   "metadata": {},
   "source": [
    "Value of columns like 'Glucose', BloodPressure' cannot be accepted as Zeros because it will affect the outcome\n",
    "We can replace such values with the mean of the respective column:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e61e3-4ae9-42aa-90ba-d9943330aeda",
   "metadata": {},
   "source": [
    "We need to replace the  zero with either a NaN value(np.NaN) or drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f756ad0-6e2f-4964-b578-725f17786ab7",
   "metadata": {},
   "source": [
    "First,we create a nice list on the data we want to do some transformation on.\n",
    "we nead to also create the mean\n",
    "we will  ow replace all the np.NaN with the means .\n",
    "\n",
    "\n",
    "you replace it with the average mean, if you dont want to usw NaN.\n",
    "After you run so as to prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c148c-aa40-4526-8e0d-99df69ade508",
   "metadata": {},
   "outputs": [],
   "source": [
    "printdataset["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb7f35-c8cd-4e2f-9c9e-4bca56c183e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeroes\n",
    "zero_not_accepted = ['Glucose', 'BloodPressure', 'BMI', 'Insulin']\n",
    "\n",
    "for column in zero_not_accepted:\n",
    "    dataset[colum] = dataset[column].replace(0, np.NaN)\n",
    "    mean = int(dataset[column].mean(skipna=True))\n",
    "    dataset[column] = dataset[column].replace(np.NaN, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96317c83-0f9b-4acf-a8bc-4e95e892da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['Glucose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31975ed0-0d9d-4893-aa72-f9ae60ba318a",
   "metadata": {},
   "source": [
    "Before we go into preprocessing, we need to split our data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e85860-d8d4-4153-8ee2-9a95ebf1cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset\n",
    "X = dataset.iloc[: , 0:8]\n",
    "y = dataset.iloc[:, 0]\n",
    "X_train,X-test,y_train,y_test = train_test_split(X,y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc924417-fb3e-4240-8add-a5df1f47dfa5",
   "metadata": {},
   "source": [
    "X = dataset.iloc[: , 0:8]\n",
    ": says that the first one in pandas,wihin the datset is all rows ,so we will keep all the rows, but we are looking at columns 0 -8, note that column 9 is outcome and it is not part of the training data rather is part of the answer.\n",
    "So 0-8 is 9 columns\n",
    "Note, that 0:8 is 1-7 as it does not include the last one\n",
    "\n",
    "y = dataset.iloc[:, 0]\n",
    "Here is our answer and we want just the last one column 8 (outcome)\n",
    "\n",
    "Random_state means you dont want to see it but random_state = 1 means you want to see it.\n",
    "then our train test split\n",
    "Then we run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f790d-dcc6-47ef-ba5d-fb27e15f1166",
   "metadata": {},
   "source": [
    "Rule of thumb: Any algorithm that computes distance or assumes normality ,scale your features! (feature Scaling)\n",
    "\n",
    "Fitting the data on a standard scaler which means instead of the data being from 5 to 3 in pne column and the next column is 1 -6 , we will set it up so that all the data is between -1 and 1.\n",
    "it keeps it standardized we  only want to fit the scaler with ou training set and we want to make sure the testing set is the X test going in is also transformed so its processing is the same.\n",
    "\n",
    "We are creating the scaler on the X-train variable\n",
    "\n",
    "Note, you dont train the y part, the y train and y test, is only ythe data going in we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a604b3d-d232-4f0c-bf82-253d464b7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc_X = SyandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "K_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ce3f1-2abd-4b57-9aee-25f039172aee",
   "metadata": {},
   "source": [
    "We define the model using k neighbors classifier and fit the trained data in the model.\n",
    "\n",
    "N_neighbors here is 'K'\n",
    "p is the power parameter to define the metric used , which is \"Euclidean\" in our case.\n",
    "\n",
    "first lets first import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed91ae-3156-4faa-a0cc-f2018ec4e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc9818-a79f-4014-a3c8-1a818b9974d9",
   "metadata": {},
   "source": [
    "The result we got is 12 which is an even number and we want an odd number, so we take out 1 and make it 11.\n",
    "\n",
    "P=2 , we want to know whether the person is diabetic or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00c033-93dc-4ca3-888e-b8ae2ecf2b4b",
   "metadata": {},
   "source": [
    "We will create a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b63cd6-bf5a-4b78-8ed5-da8f7a3874bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model: init K-NN\n",
    "Classifier = KNeigbhorsClassifier(n_neighbors_neighbors=11, p=2,metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acb65b-183c-4c44-9fd8-766cedde002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d65013-bccd-4eac-9850-dad96cc94f65",
   "metadata": {},
   "source": [
    "It's important to evalute the model, lets use a cofusion matrix to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e53c50-f4f4-4bcf-a163-d303fe0d43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate Model\n",
    "cm = confusion_matix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d6a88-d922-4132-8e06-af70c8bae70d",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "From the result above  [ 94, 13]\n",
    "\n",
    "                       [15   32]\n",
    " \n",
    "The important column is 94 and 32 are the actual\n",
    "\n",
    "15 and 13 are the ones that are wrong\n",
    " The zero represents the 94 that dont have diabetes but the prediction says that 13 of those people do have diabetes with high risk.  \n",
    "The 32 that have diabetes are correct but our prediction says that 15 are classiied as incorrect.\n",
    " \n",
    " This is how classification comes in and how it works in confusion matrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7faf76-ec83-49db-8d6b-524a90afcd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the f1-score\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba9446d-e737-4293-b848-32f4495bbc10",
   "metadata": {},
   "source": [
    "We got a 0.61 in f1-score\n",
    "The f1 score takes into account both sides of the balance of false positives.\n",
    "It is more telling than the accuracy.\n",
    "In the business world you will be asked about the f1-score. \n",
    "from the result we have more alse positive\n",
    "\n",
    "from the accuracy report 82 % is not too bad for a quick flash look at people,s different statistics in running an sklearn and runnung K_Nearest Neighbor on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd629d74-5914-4036-b57e-25ea27cb6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print accuracy score\n",
    "print(accuracy_score(y,test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297380ca-4794-41d7-b0a3-8492621cea5a",
   "metadata": {},
   "source": [
    "So, we have created a model using KNN which can predict whether a person will have diabetes or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb0275-cffe-494a-a011-fa3cce1d92b1",
   "metadata": {},
   "source": [
    "#### K- Nearest Neighbors: Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68af06a-f7e6-4635-ac37-cc8ec38ab0fc",
   "metadata": {},
   "source": [
    "1. k-NN algorithm does more computation on test time rather than train time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405303d1-893b-48df-ae35-db6b34170772",
   "metadata": {},
   "source": [
    "Ans.\n",
    "\n",
    "True\n",
    "The k-NN algorithm does more computation on test time rather than train time. That is absolutely true. The idea of the kNN algorithm is to find a k-long list of samples that are close to a sample we want to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90239ce-f71d-47cd-88fe-2b575ad4e5e0",
   "metadata": {},
   "source": [
    "2. Which of the following option is true about k-NN algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d06ee6-609c-4e05-92e4-1184efcbec4f",
   "metadata": {},
   "source": [
    "As we saw above, KNN algorithm can be used for both classification and regression problems. The KNN algorithm uses 'feature similarity' to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff933bd-f1e2-4b91-9935-b00b9c801421",
   "metadata": {},
   "source": [
    "3. Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuous variables?\n",
    "Incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8cf6c-4eb2-4309-acc2-d1cea442b26d",
   "metadata": {},
   "source": [
    "ans :The k-NN algorithm can be used for imputing the missing value of both categorical and continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b05447-8d19-449f-96e0-7763e9d5e37e",
   "metadata": {},
   "source": [
    "4. Which of the following statement is true about k-NN algorithm?\n",
    "\n",
    "1- k-NN performs much better if all of the data have the same scale.\n",
    "\n",
    "2-k-NN works well with a small number of features (X’s), but struggles when the number of inputs is very large\n",
    "\n",
    "3-k-NN makes no assumptions about the functional form of the problem being solved\n",
    "\n",
    "\n",
    "All the above is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877340e8-3c9c-4040-a0fa-fa2aa6770d0d",
   "metadata": {},
   "source": [
    "5. When you find noise in data which of the following option would you consider in k-NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a39f7a-6399-4b52-800b-747850d12f6b",
   "metadata": {},
   "source": [
    "ans: I will increase the value of k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b69332-807d-42f9-935c-9dfe1e288414",
   "metadata": {},
   "source": [
    "When you find noise in data which of the following option would you consider in k-NN? To be more sure of which classifications you make, you can try increasing the value of k. 19) In k-NN it is very likely to overfit due to the curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c029e-7901-44fc-b198-d61b1969fa2e",
   "metadata": {},
   "source": [
    "### Week 11: Day 2 – Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eed4b4-781b-4542-a753-ed03fb03d152",
   "metadata": {},
   "source": [
    "#### Introduction to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1611d-dbe2-43f0-8feb-283782839467",
   "metadata": {},
   "source": [
    "What is Decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c40f6-885a-43a6-b525-31512e639274",
   "metadata": {},
   "source": [
    "How do we use them to help us classify ?\n",
    "\n",
    "Can i grow my own decision tree?\n",
    "\n",
    "The basic intuitio n behind decision tre is to map out all possible decision paths in the form of a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196de69-4a57-4d3c-bf56-c43c84de649f",
   "metadata": {},
   "source": [
    "How to build a decision tree?\n",
    "\n",
    "Imagine that you are a medical researcher compiling data for a study. You have already collected data about a set of patients all of whom sufferred from thesame ailment.\n",
    "\n",
    "During your course of treatment each patient responded to one of two medication.\n",
    "We wil call them Drug A and Drug B\n",
    "Part of your job is to build a model to find out which drug might be appropriate for a future patient with thesame illness.\n",
    "\n",
    "The futurs set of this dataset are Age, Gender, Blood Pressure and cholestrol of a group of pateints and the target is the drug that each patint responded to.\n",
    "\n",
    "It is  a sample o fbinary classifiers and you can use the training part of the dataset to build the decision tree and then use it to predict the class of an unknown patient, in essence to come up with the decision on which drug to prescribe to a new patient. Lets see how a decision tree is built for this dataset.\n",
    "Decision trees are built by splitting the training set into the stake notes while one node contains all of or most of one category of the data.\n",
    "\n",
    "the diagram we have here is a patient's classifier, so as mentioned , we want to prescribe the drug to a new patient but the decision to choose drug A or B will be infleunced by the patient's situation.\n",
    "\n",
    "We start with age which can be\n",
    "\n",
    "Young , middle age or senior\n",
    "If the patient is middle age ,we will go for drug B,on the other hand if we have a young or senior Patient we will need more details to help us determine what drug to Prescribe.\n",
    "\n",
    "The additional decision variable can be such as cholestrol levels, Gender or Blood Preesure.\n",
    "\n",
    "For example, if the patient is female we would recommend Drug A ,but if th epatient is male we will go for Drug B.\n",
    "\n",
    "As you can see decision tree s are about testing an attribute and branching the cases based on the result of the test.\n",
    "\n",
    "Each internal node corresponds to a test\n",
    "\n",
    "And each branch corresponds to a result of the test\n",
    "\n",
    "And each leaf node assigns a patient to a class (classification) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494c998-28c6-4a68-94f2-ee6f94cda3b3",
   "metadata": {},
   "source": [
    "Now the question is , how can we build such a decision tree?\n",
    "\n",
    "Here is the way that a decision tree is built.\n",
    "\n",
    "A decision tree can be constucted by considering the attribute one by one\n",
    "\n",
    "+ Choosing attribute from a dataset\n",
    "\n",
    "+ Calculate the algorithm of the attribute in the splitting of the data.\n",
    "\n",
    "+ Split the data based on the value of the best attribute.\n",
    "\n",
    "+ Go to step 1: Go to each branch and repeat it for the rest of the attribute,\n",
    "\n",
    "After building this tree ,you can use it to predict the class of unknown cases or in our case the proper drug for a new patient based on his or her characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9819aa8-1c64-4f04-920b-85f4ba37b135",
   "metadata": {},
   "source": [
    "Enthropy Is the major of inpurity in a node ie non homogenuity.\n",
    "example your data wants to identify male or female through shoes size.\n",
    "\n",
    "we have 40 persons to test for size 45, 35 are male and 5 are female . the inpurity in the data is the 5.\n",
    "\n",
    "The enthropy helps the decision tree to determine how it will split the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9158f18-340d-4575-ab28-ee6a84347134",
   "metadata": {},
   "source": [
    "### Building Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f8e27-d276-4aca-8647-defa44e9190b",
   "metadata": {},
   "source": [
    "#### How do we build decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e52b1-20bd-41cc-a81b-43af188f006c",
   "metadata": {},
   "source": [
    "Using the drug dataset we discussed earlier, the question is How do we build decision trees based on our dataset?\n",
    "\n",
    "Decision trees are built using regresseive partitioning to classify the data.\n",
    "\n",
    "Lets say we have 14 patients in our dataset ,the algorithm chooses the most predictive feature to split the data on.\n",
    "\n",
    "###### What is important in making a decision tree is to determine which attribute is best or more predictive to split dat based on the feature.\n",
    "\n",
    "Let's say we picked cholesterol as the first attribute to split data, it will split our data into two branches, \n",
    "cholestorol  : High or Normal\n",
    "\n",
    "###### As you can see if the patient have high cholestorol we cannnot say with high confidence that drug B might be suitable for him, also if the patient cholestorol is normal, we still dont have sufficient information to determine either Drug A or Drug B is infact suitable.\n",
    "\n",
    "###### It is  a sample of bad attribute selection for splitting data, so lets try another attribute.\n",
    "\n",
    "Again we have our 14 cases, this time we picked the sex attribute of patients,it will split our data into two branches, male and female, as you can see if the patient is female, we can say that drug B is suitable for her with high certainty.but if the patient is male , we dont have substantial evidence or information to determine if drug A or drug B is suitable, however, it is still a better choice in comparising with the cholestrol attribute because the result in the nodes are more pure, it means nodes that are either mostly drug A or drug B\n",
    "(More Predictiveness less Impurity ,Lower Entropy).\n",
    "\n",
    "So we can say that the sex attribute is more significant than cholestrol or more predictive than the other attribute.\n",
    "\n",
    "Indeed predictive node is based on decrease and inpurity of nodes.\n",
    "\n",
    "We are looking for the best feature to decrease the impurity of patients in the leaves after splitting them up based on that feature.\n",
    "\n",
    "So the sex feature is a good candidate in the following case because it almost found the pure patient.\n",
    "\n",
    "If we go one step further, for the male patient branch we again test other attributes to split the sub tree, we test cholestrol again here. This results in even more pure leaves. So we can easily make a decision here.\n",
    "\n",
    "For example, if the patient is male and his cholestrol is high, w can certainly prescribe drug A but if it is normal we can prescribe drug B with high confidence.\n",
    "As you might notice, the choice of attribute to split data is very important and is all about purity of the leaves after the split and noes in the trees is considered pure if in 100% of the cases the nodes fall into a specific category of the target filled.\n",
    "\n",
    "Infact , the method uses recursive partitioning to split the training records into segments by minimizing the impurity at each step. Inpurity of nodes is calculated by entropy of data in the node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1107aa-7960-4162-bd4f-f101963cbc76",
   "metadata": {},
   "source": [
    "#### What is Enthropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74dcc7-5912-41ce-8a9e-efc22e2f60de",
   "metadata": {},
   "source": [
    "This is the amount of information disorder.\n",
    "or the amount of randomness or uncertainty in the data.\n",
    "The entropy in the node depends on how much random  data is in that node and is calculated for each node.\n",
    "\n",
    "In decision tress we are looking for the tress that have the smallest entropy in the nodes ,\n",
    "\n",
    "Entropy is used to calculate the homogeinity of the samples in that node.\n",
    "If the samples are completely homogeinius, the entropy is zero and if the samples are equally divided the entropy is 1 .\n",
    "\n",
    "This means if all the data in the node are either drug B or drug A ,then the entropy is zero but if half of the data are drug A and the other half are drug B then the entropy is 1\n",
    "\n",
    "##### The lower the Entropy the less uniform the distribution, the purer the node\n",
    "\n",
    "1 Drug A \n",
    "\n",
    "7 Drug B  - Entropy is Low (good)\n",
    "\n",
    "3 Drug A\n",
    "\n",
    "5 Drug B - Entropy is high (bad)\n",
    "\n",
    "0 Drug A\n",
    "\n",
    "8 Drug B  - Entropy is 0 (pure)\n",
    "\n",
    "4 Drug A\n",
    "\n",
    "4 Drug B - Entropy = 1 (bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d32ac-0ee3-4cad-ae70-17911f14457b",
   "metadata": {},
   "source": [
    "You can easily calculate the entropy of a node using the frequency table of the attribute through the entropy formula or piece for the proportion or ratio of a category such as drug A or Drug B.\n",
    "\n",
    "Entropy = -p(A)log(A)) - p(B)log(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf6e4a-c0ad-40ba-a0db-49edc542e0e4",
   "metadata": {},
   "source": [
    "+ Entropy is the measure or randomness or uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816eda18-d61d-46b8-9e27-de09a7c73d0f",
   "metadata": {},
   "source": [
    "+ Please remember though that you dont have  to calculate this as it is easily calculated by the library or packages that you use .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca4464-3ee1-4ca9-9a10-061936ff6702",
   "metadata": {},
   "source": [
    "+ Which attribute is the best to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320326c-2e18-49e1-8a09-68d2581d0b07",
   "metadata": {},
   "source": [
    "Let's calculate the entropy of the dataset before spitting it.\n",
    " We have 9 occurances of drug B and 5 of Drug A, You can enbed this numbers into the entropy formula to calculate the inpurity of the target attribute before splitting it, in this case it is 0.94.\n",
    " \n",
    "So,what is entropy after spliting?\n",
    "\n",
    "Is cholestrol the best attribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad4a21-a3cd-4349-891d-279c3cf9dc47",
   "metadata": {},
   "source": [
    "S: [9 B, 5 A]\n",
    "\n",
    "E = - p(B)log(p(B)) - p(A)log(P(A))\n",
    "\n",
    "E = - ((9/(14)log(9(14)) - p(5/14)log(5(14))\n",
    "\n",
    "E = 0.940"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87a9c8-baaf-4fce-9650-932ec137ba1a",
   "metadata": {},
   "source": [
    "Now we can test different attributes to find the one with the most predictiveness which results in two more pure branches."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d19be600-670c-4ac7-9f78-c8df53863084",
   "metadata": {},
   "source": [
    "            Cholestrol\n",
    "            \n",
    "            |          |\n",
    "            Normal     High"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e8f17-f542-42d3-97bc-442b19a91d10",
   "metadata": {},
   "source": [
    "Lets first select the cholestrol of the patient and see how the data gets split based on its values.\n",
    "\n",
    "For example, when it is normal, we have 6 for drug B and 2 for drug A\n",
    "\n",
    "We can calculate the entropy of this node based on the distribution of Drug B and drug A which is 0.811 in this case but when cholestrol level is high, the data is split into 3 for drug B and 3 for Drug A, calculating its entropy we could see it will be 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e9f12-9d5a-49f0-9af6-a45af621ac61",
   "metadata": {},
   "source": [
    "S : [ 6 B, 2 A]\n",
    "\n",
    "E = 0.811"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346dbc73-d0f9-4db7-9735-247686b65406",
   "metadata": {},
   "source": [
    "S : [ 3 B, 3 A]\n",
    "\n",
    "E = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12159a9-c944-4f2e-8b93-2448f988521f",
   "metadata": {},
   "source": [
    "We could go through all the attribute and calculate the entropy after the split and then choose the best attribute.\n",
    "\n",
    "Lets try another field, lets choose the sx attribute for the next chart.\n",
    "\n",
    "As you can see when you use the sex attribute to split the data,when its value iis female, we have 3 patients that responded to drug b and 4 patients that responded to Drug A, the entropy for this node is 0.98 which is not very promising.\n",
    "\n",
    "However, on the otherside of the branch when the value of the sex attribute is male , the result is more pure, with 6 for drug B and only 1 for drug A, the entropy for this group is 0.59."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c43b643-0270-46c7-a63f-a2fa88185ebb",
   "metadata": {},
   "source": [
    "               Sex\n",
    "                  \n",
    "               |   |\n",
    "            \n",
    "               F    M\n",
    "          0.985     0.592"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb447296-d28e-43e7-a6e3-2230987ae521",
   "metadata": {},
   "source": [
    "S : [ 9 B. 5 A]\n",
    "\n",
    "E = 0.940"
   ]
  },
  {
   "cell_type": "raw",
   "id": "816a2153-cf24-4452-adb5-4955ef09d13c",
   "metadata": {},
   "source": [
    " Female                      Male\n",
    "     \n",
    "S : [3 B, 4 A]             S : [6 B, 1 A]\n",
    "\n",
    "E = 0.985                    E = 0.592"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894ca5f-a3f0-480d-b861-4f189dd63914",
   "metadata": {},
   "source": [
    "+ Which attribute is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e49a36-785d-4189-9d61-ff73f45e5bcc",
   "metadata": {},
   "source": [
    "Now the question is between the Cholestrol and sex attribute, which one is a better choice.\n",
    "\n",
    "Which one is better as the first attribute to divide the data into the two branches or in other words , which attribute results in more pure nodes for our drugs or which tree do we have less entropy after splitting rather than before splitting.\n",
    "\n",
    "The sex attribute with entropy of 0.98 and 0.59 or the cholestrol attribute with entropy of 0.81 or 1.0 in its branches.\n",
    "\n",
    "The answer is :\n",
    "\n",
    "###### The tree with the higher information gain after splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aeda36-cbae-4d99-9a0b-39360e68e60e",
   "metadata": {},
   "source": [
    "##### What is information gain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac0835-36a6-4779-9aba-999b18ea075e",
   "metadata": {},
   "source": [
    "Information gain is the information that can increase the level of certainty after splitting.\n",
    "\n",
    "It is the entropy of the tree before the split minus the weighted entropy after the split by an attribute\n",
    "\n",
    "\n",
    "Information gain = [Entropy before split] - (weighted entropy after split)\n",
    "\n",
    "We can think of information gain and entropy as opposites  as entropy or the amount of randomness decreases the information gain or amount of certainty increases and vice versa.\n",
    "\n",
    "So constructing a decision tree is all about finding attributes that return the highest infomation gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f1ff62-9bc0-4d84-a90f-b59e694b25af",
   "metadata": {},
   "source": [
    "    Entropy before split (cholestrol)\n",
    "      E = 0.940\n",
    "    \n",
    "  Normal (E = 0.811)       High (E =1.00)\n",
    "\n",
    " Weighted entropy after split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b286fd9-8a27-4396-a58c-528b52e8fb6e",
   "metadata": {},
   "source": [
    "+ How to calculate information gain for the sex attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725a1f4-8370-4db7-95e8-f8242439e0c7",
   "metadata": {},
   "source": [
    "As mentioned, the information gain is the entropy of the tree before the split minus the weighted entropy after the split, the entropy of the tree attribute before the split is 0.94, the portion of female patints is 7 out of 14 and its entropy is 0.985, also the portion of men is 7 out of 14,and the entropy of the male node is 0.592, the result of a squared bracket here is the weighted entropy after the split .\n",
    "\n",
    "So the information gain of the tree if we use the sex attribute to split the data set is 0.151"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afa2b1-b1a5-4678-a0b7-208fbfed57c9",
   "metadata": {},
   "source": [
    "Gain (s, Sex)\n",
    "\n",
    "= 0.940 - [ (7/14)0.985 + (7/ 14)0.592]\n",
    "\n",
    "= 0.151"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee58cf-b4b9-416b-b174-cd5fe7f56a38",
   "metadata": {},
   "source": [
    "As we can see we will consider the entropy over the distribution of samples falling under each mid node and we will take a weigthed average of that entropy, weigthed by the proportion of sample falling under that leaf.\n",
    "\n",
    "We can calculate the information gain of the tree if we use cholestrol as well\n",
    "it is 0.048.\n",
    "\n",
    "+ Now the question is , which attribute is more suitable?\n",
    "\n",
    "As mentioned , the tree with the higher information gain after splitting.\n",
    "\n",
    "+ This means the sex attribute\n",
    "\n",
    "So we select the sex attribute at the first spiltter.\n",
    "Now, what is the next attribute after branching by the sex attribute?\n",
    "\n",
    "\n",
    "We should repeat the process for each branch and test each of the other  attributes and continue to reach the most pure leaf and this is the way you build the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3687e-bf52-471d-8c2e-8c8b2220dcac",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5caec-75a4-4df7-9fb8-d48792968feb",
   "metadata": {},
   "source": [
    "Decision tree is a type of classification algorithm which comes under the supervised learning technique.\n",
    "\n",
    "+ Agenda\n",
    "\n",
    "+ What is classification?\n",
    "\n",
    "+ Types of classification\n",
    "\n",
    "+ Classification Use cases\n",
    "\n",
    "+ What is Decision tree?\n",
    "\n",
    "+ Terminologies associated to a Decision tree\n",
    "\n",
    "+ Visualizing a Decision Tree\n",
    "\n",
    "+ Writing a Decision Tree Classifier from scratchin python uing CART A lgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70389920-a32d-4490-b795-e8cd4b70e034",
   "metadata": {},
   "source": [
    "#### What is classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8da7a8-a850-472c-bacf-ba668e405250",
   "metadata": {},
   "source": [
    "Classification is the process of dividing the datasets into different categories or groups by adding label.\n",
    "\n",
    "It is a technique of categorizing the observation into different category.\n",
    "\n",
    "Basically ,what you are doing is taking the data and  lising it and on the basis of some condition  finally divided into various categories.\n",
    "\n",
    "+ Note : it adda the data points to a particular labelled group on the basis of some condition\n",
    "\n",
    "+ Why do we classify it?\n",
    "\n",
    "We classify it to perform  predictive analysis on it. For example, when you get a mail, the machine predicts it to be a spam or not a spam mail and on the basis of that prediction it adds the irrelevant or spam mail to the respective folder.\n",
    "\n",
    "In gender,the classification algorithm handles questions like, this data belongs to A category or B category e.g is it a male or a female.\n",
    "\n",
    "Now the question arises , where will you use it.\n",
    "You can use it on fraud protection or to check whether the transaction is genuinue or not.\n",
    "\n",
    "For example, if am using the card in nigeria and to some reason i have to fly to Dubai, if am using the credit card over there i will get a notification alert regarding my transacton, they will ask me to confirm the transaction, and this is a kind of predictive analysis as the machiine predicts that something fishy is going on in the transaction as twenty four hours ago i made a transaction using thesame credit card in Nigeria  and twenty four hours later thesame credit card is been used for payment in Dubai.\n",
    "\n",
    "So the machine predicts that something fishy is going on in the transaction and in other to confirm it, it snds you a notification alert. \n",
    "This is one of the use case of classification.\n",
    "\n",
    "You can even use it to classify different kinds of fruits on the basis of size ,colour ,taste . A machine when traine can easily predict the color or type of fruit when evr a new data is given.\n",
    "\n",
    "Not just type of fruit , it can be any item, it can be a car, house, signboard  or anything.\n",
    "\n",
    "For example, when you want to log in to a site , you get a picture capcha for that,you have to arrange giving the images of the car or whatever image they want you to about three images out of it.\n",
    "\n",
    "In that way, you are training the machine, you are saying that this tree are the picture of  a car and the rest are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7b33e-df1e-4a5d-8969-d540a4e2baab",
   "metadata": {},
   "source": [
    "#### Types of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1cc98-8d2e-404b-b161-34a3359f3bda",
   "metadata": {},
   "source": [
    "There are several different ways to perform a syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f60cd1-0992-471b-aa4f-c266c725f257",
   "metadata": {},
   "source": [
    "+ Decision Tree\n",
    "\n",
    "+ Random Forest\n",
    "\n",
    "+ Naive Bayes\n",
    "\n",
    "+ K-Nearest Neighbor\n",
    "\n",
    "+ Logistic Regression\n",
    "\n",
    "+ Linear Regression\n",
    "\n",
    "+ Sector Vector Machines( SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba10290-2c28-4b01-a164-dbb3499aa3ec",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95385982-e4c0-4447-8935-f41e605df707",
   "metadata": {},
   "source": [
    "+ Graphical representation of all the possible solution to a decision\n",
    "\n",
    "+ Decisions are based on some conditions\n",
    "\n",
    "+ Decision made can be easily explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40bdd4-2cf1-47e4-83e9-cdd0e2aac8ca",
   "metadata": {},
   "source": [
    "For example , here is a task, should i go to a resturant or should i buy a hamburger.\n",
    "\n",
    "First , is to create a decision tree for it , the head node is to check whether you aree  hungry or not,  Am i hungry Yes or No)\n",
    "\n",
    "if you are not hungry then the next leaf is to go back to sleep, but if it is yes you are hungry , \n",
    "\n",
    "the next thing is do i have $125 ,\n",
    "\n",
    "if yes i wlll decide to go to a resturant , \n",
    "\n",
    "and if you dont have $125 dollars yo can decide to go for a hamburger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e170b-6c23-4ab6-a1d1-d537960f2866",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824721c-1130-4611-95bb-9807a500ab5c",
   "metadata": {},
   "source": [
    "+ Builds multiple decision trees and merges them together \n",
    "\n",
    "+ More accurate a nd stable predicton\n",
    "\n",
    "+ Random decision forest correct for decision trees' habit of overfitting to their training set\n",
    "\n",
    "+ Trained with the \"bagging\" method, the bagging method is based on the idea that the combination of learning model increases the overall result.\n",
    "\n",
    "If you combining the  learning from different models and clubbing it together, it will increase the overall result.\n",
    "\n",
    "If the size of your dataset is huge,one single decision tree will lead to overfit model , in the same way like a single person might have its own perspective  on a complete population as the population is very huge.\n",
    "\n",
    "However,if we implement the voting system and ask different individual to interprete the data , them they wil be able to cover the pattern in a much meticulous way.\n",
    "\n",
    "When we have a large trained dataset, DAta set A the first thing to do is to  divide it into n- sub samples and then create a decision tree for each sub sample.\n",
    "\n",
    "Then for the data set B we take a vote out of every decision made by every decision tree\n",
    "\n",
    "Finally ,we club the vote to get the  random forest decision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fddbc-9bf6-4d48-bde7-0c690e17f019",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c918d-a8c7-44ff-83a6-dc80bc8b7618",
   "metadata": {},
   "source": [
    "+ Classification tecnique based on Baye's Theorem\n",
    "\n",
    "+ Assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature\n",
    "\n",
    "It is a simple an easy to implement algorithm and due to its simplicity this algorithm might out perform most complex model when the size of the dataset is not large enough.\n",
    "\n",
    "The classical use case of Naiye bayes is document classification.\n",
    "You determine whether a given text correspond to one or more categories.\n",
    "In the text case, the features used might be the presence or absence of any key word.\n",
    "\n",
    "\n",
    "From the diagram you can see that using Naiye bayes we have to decide whether we have a disease or not.\n",
    "\n",
    "First, we check the probabiluty of having a disease or not having a disease.\n",
    "\n",
    "the probability of having a disease P(disease) = 0.1 and the probability of not having a disease P(no disease)is = 0.90\n",
    "\n",
    "When you have a disease,you call a doctor, when you have visited the doctor and the test is positive, the probability of having a positive test when you are having a dieaseP(+| disease) =  0.80\n",
    "\n",
    "And the probability of a negative test when you already have a disease P(-| disease) = 0.20. This is also a false negative statement as the test is testing negative and you still have the disease\n",
    "\n",
    "When you dont have the disease at all, so the probability of not having the disease P(-| disease) = 0.9 and when you visit the doctor and the doctor is like yes you have the disease but you already know that you dont have the disease. This  is a False Positive statement.\n",
    "\n",
    "The probability of having a disease when you actually know that there is no disease is P(+| disease) = 0.1\n",
    "This is a false positive\n",
    "\n",
    "And probabibilty of having a diesase when you actually know that there is no disease is 0.90, it is the same as probability of not having a disease.\n",
    "\n",
    "The test is showing the same reason . A true positive statement 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a1577-b3ec-4c53-b60d-763e0db8fd91",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b99b62-6bf2-4888-9a33-0ab8ff54a392",
   "metadata": {},
   "source": [
    "+ Stores all the available cases and classifies new cases based on a similarity measure.\n",
    "\n",
    "+ The \"K\" is KNN algorithm is the nearest neighbor we wish to take vote from,\n",
    "\n",
    "For example, if K= 1,then the object is assigned to the class of that single nearest neighbor.\n",
    "\n",
    "We have diference in the images  when K= 1, K= 3 and K=5\n",
    "\n",
    "The modern systems are not able to use the K-NN for visual pattern recognization to scan and detect hidden packages in the button ben of  a shopping card at the check out.if an object is detected which matches exactly to the object listed in the data base, then the price of the spotted product product can automatically be added to the customers bill,\n",
    "\n",
    "While this automated selling practice is not used extensively at this time but the tecnolology has been developed and is \n",
    "available and you can use it.\n",
    "\n",
    "KNN is also used in retail to detect patterns in the credit card usage.\n",
    "\n",
    "Many new transactions scrutinizing soft are application use KNN algorithm to analyze registered data and spot unusual pattern that indicates suspicious activity.\n",
    "\n",
    "For example,if registered data indicates that alot of customers information has been entered manually rather than to automated scanning and scrapping.\n",
    "This could indicate that the employees are using the register and stealing customers personal information or for registered data indicates that a particular code is being returned or exchanged multiple times, this could indicate that employees are using the returned policy or trying to make money from doing the fake product.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f46bc-3f9f-4762-a99e-0bd216d7987a",
   "metadata": {},
   "source": [
    "#### What is decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d375fd4-abec-4399-8158-80855441ffba",
   "metadata": {},
   "source": [
    "+ Why choose Decision tree?\n",
    "\n",
    "+ We choose decision tree because it is very easy to read and understand.\n",
    "\n",
    "+ It belongs to  one of the pure model that is interpretaeable and you can understand exactly why the classifier has made that particular decision.\n",
    "\n",
    "For a given dataset you cannot say this algorithm performs better than the other e.g you cannot say that Decision tree performs better than Naive Bayes or that Naive bayes performs better than decision tree, it depends on the dataset, you have to apply the trial method with all the algorithms one by one and then compare their results. The model which gets the best result is the model you can use for better accuracy for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e69471-2e6a-40c6-b345-ec10534f776d",
   "metadata": {},
   "source": [
    "#### What is decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09657464-6c16-4362-abcc-3b4536438b02",
   "metadata": {},
   "source": [
    "+ \"A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions\"\n",
    "\n",
    "\n",
    "+ Why is it called Decision Tree?\n",
    "\n",
    "It is called a decision tree because it starts with a root and it branches up to a number of solutuion just like a tree.\n",
    "Even the tree starts from a root and starts growing its branches oce it gets bigger and bigger. \n",
    "\n",
    "Similarly , in a decision tree ,it has a root and it keeps on growing with an increasing number of desicions and conditions.\n",
    "\n",
    "Example using a real life scenario, when ever you dial a toll free number of your credit card  company , it re- directs you to its intelligent computerized assistant , it asks you questions like press 1 for English, prees 2 for hindus, press 3 for \n",
    "\n",
    "this and so on, once you select 1 ,it again re directs you to certain type of questions like press 1 for this a nd press 5 for that and similarly, it keeps on re -directing you until you finally gets to the right person.\n",
    "You might think you are cut in a voice mail stuf, but what the company is actually doing is using  a decision tree to get you to the right person.\n",
    "\n",
    "Another example is should i accept a new job offer or not , in other to do that you create a decision tree.\n",
    "\n",
    "First is the salary , salary should be \n",
    "\n",
    "atleast $50,000 \n",
    "\n",
    "and if it is not $50,000 \n",
    "\n",
    "you will not accept the offer. \n",
    "\n",
    "if it is greater than $50,ooo you will \n",
    "\n",
    "further check whether the commute is more than 1 hour,if yess you accept ,if no you decline\n",
    "\n",
    "Does the company offer free cofee , if yes accept if no reject the offer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003da652-596f-4f04-b57e-97e1408497cc",
   "metadata": {},
   "source": [
    "#### Understanding a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797429dd-1752-4bab-8cc4-08c80be5120f",
   "metadata": {},
   "source": [
    "In other to build a tree we use an algorithm called CART which repreents Classification and regression  tress algorithm,\n",
    "\n",
    "To start we add a node for the  tree and each node will ask a yes and no question"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33bb5c11-b71c-415d-95bc-9cd7a0405da1",
   "metadata": {},
   "source": [
    "Color        Diameter      label\n",
    "\n",
    "Green           3           Mango\n",
    "\n",
    "Yellow           3           Lemon\n",
    "\n",
    "Red              1          Grape \n",
    "\n",
    "Yellow           3          Mango\n",
    "\n",
    "Red              1          Grape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9aa2c-4ffe-447b-8c30-70b541085964",
   "metadata": {},
   "source": [
    "This question will split our dataset into two different sub set. the goal of the quesion is to produce the purest possible distribution of the label at each node.\n",
    "\n",
    "For example the input of this node contains only one single type of label, so we could say that it is perfectly unmixed .\n",
    "\n",
    "The first question , is the diameter less than 3 (>3), \n",
    " False or True\n",
    " \n",
    "group A we have \n",
    "R grape  1 \n",
    "R grape 1 \n",
    "\n",
    "Group B we have \n",
    "\n",
    "G  3   Mango\n",
    "\n",
    "Y   3   Mango\n",
    "\n",
    "Y   3   Lemon\n",
    "\n",
    "\n",
    "We can now ask another question, but we need to understand which question to ask and when,and to do that we need to quantifies how much question helps to unmix the label,we can quantify the amount of uncertainty of single node using a metric called \"Gini Impurity = 0\" and we can quantify how much a question uses that uncertainty using a concept called information gain. we use this to select the best question to ask at each point.\n",
    "\n",
    "We now iterate our steps to label the tree on each of the new node. We continue dividing the data until there are no further questions to ask and finally we get to our leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910480e-4828-49d2-8bf4-ea0a860ba1b8",
   "metadata": {},
   "source": [
    "In other to create a decision tree, we have to identify different set of questions that  we can ask our tree like , is this colour green?\n",
    "Is the diameter >=3? is the color yellow?\n",
    "Quesions that are relevant to your dataset.\n",
    "\n",
    "If the color is green, then the green mangp will be in the True box while the rest will be in the false "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672e554-2c30-4c20-960e-e8dd06dd2588",
   "metadata": {},
   "source": [
    "#### Decision Tree Terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04259cc-6da5-4e29-a22b-1430e54e6f61",
   "metadata": {},
   "source": [
    "+ Root Node: It represents the entire population or sample and this further gets divided into two or more homogenous sets.\n",
    "\n",
    "+ Pruning : Opposite of Splitting ,basically removing unwanted branches from the tree.\n",
    "\n",
    "+ Parent/Child Node:Root node is the parent node and all the other nodes branched from it is known as child node,\n",
    "Simply put, all the top nodes belongs to the parent's node and all the buttom nodes derived from a top node is child node.\n",
    "The node producing a child node is a parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606dcdb-0d53-493d-800b-1c6163c49b71",
   "metadata": {},
   "source": [
    "#### CART Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e74c2-d9a5-4125-8e87-6a60231e9665",
   "metadata": {},
   "source": [
    "We will be using the CART algprithm to design a tree manually.\n",
    "\n",
    "First, lets visualize te Decision Tree\n",
    "\n",
    "Which Questions to ask and When?\n",
    "\n",
    "Which one among them should you pick first?\n",
    "\n",
    "In our dataset we have ,\n",
    "\n",
    "outlook  temp  humidity  windy  play\n",
    "Answer: \n",
    "Determine the attribute that best classifies the training data.\n",
    "\n",
    "But how will you choose the best attribute?\n",
    "\n",
    "or How does a tree decide where to split?\n",
    "\n",
    "Before we move on to split the tree ,there are certain terminologies we need to split.\n",
    "\n",
    "First is the\n",
    "\n",
    "+ Gini Index: The measure of impurity(or purity) used in building decision tree in CART is Gini Index.\n",
    "\n",
    "+ Information Gain:\n",
    "The information Gain is the decrease in entropy after a dataset is split on the basis of an attribute .Constructing a decision tree is all about finding attribute attribute that returns the highest information gain.\n",
    "\n",
    "+ Reduction in variance : Reduction in variance is an algorithm used for continuous target variables (regression problems).The split with lower variance is selected as the criteria to split the population.\n",
    "Variance is how much your data is varing, if your data is less impure or more pure, in that case the variation will be less as all the data will be almost similar, this is also thesame with decision tree,a split with low variance is selected as a criteria to split the population.\n",
    "\n",
    "+ Chi Square \n",
    "\n",
    "It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e9c1f-f4ff-42d3-90d0-0fadf6225b41",
   "metadata": {},
   "source": [
    "#### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b007340-e3ce-4886-b909-769f93e516dd",
   "metadata": {},
   "source": [
    "+ Entropy is a matric that measures the impurity in your dataset.\n",
    "\n",
    "+ It is the first step you take before you solve the problem of decision tree.\n",
    "\n",
    "+ Defines randomness of the data\n",
    "\n",
    "\n",
    "\n",
    "+ What is impurity?\n",
    "\n",
    "Suppose you have a basket full of apple and another baket with same label that says apple Now if you are asked to pick one item from each basket,then the probability of getting the apple and its correct label is\n",
    "1, \n",
    "Impurity = 0\n",
    "\n",
    "So in this case , you can say that impurity is 0\n",
    "\n",
    "Now what if there are 4 different fruits in the basket and 4 different labels in the bowl, then the probability of matching the 2 is obviously not one but something less than that.\n",
    "It coul be possibe we pick banana from the basket and when we pick a label from the bowl it says cherry.\n",
    "\n",
    "In this cae, we will say that impurity is not 0\n",
    "impurity ≠ 0\n",
    "\n",
    "\n",
    "Entropy(s) = P(yes)log2 P (yes) - P(no) lpg2 P(no)\n",
    "\n",
    "where ,\n",
    "\n",
    "+ S is the total sample space \n",
    "\n",
    "+ P(yes) is the probability of yes\n",
    "\n",
    "If number of yes = number of no ie P(s) = 0.5 =>  Entropy(s) = 1\n",
    "\n",
    "if it contains all yes or all no ie P(s) = 1 or 0  => Entropy(s) - 0\n",
    "\n",
    "formula of Entropy\n",
    "\n",
    "E(s) = -P(Yes)log2 P(Yes)\n",
    "\n",
    "E(s) = 0.5 log2 0.5 - 0.5 log2 0.5\n",
    "\n",
    "E(s) = 0.5 (log2 0.5 - log2 0.5)\n",
    "\n",
    "= E(s) = 1\n",
    "\n",
    "when P(Yes) = P(No) = 0.5 ie Yes + No = Toal Sample(S)\n",
    "\n",
    "\n",
    "The next one is either you have total yes or total no. lets see the formula\n",
    "\n",
    "E(s) = P(Yes) = 1 ie YES = Total Sample(s)\n",
    "\n",
    "E(S) = 1 log2 1\n",
    "\n",
    "E(s) = 0 \n",
    "\n",
    "\n",
    "Similarly , when it is No\n",
    "\n",
    "E(S) = P(NO) log 2 P (No)\n",
    "\n",
    "When P(No) = 1 ie No = Total Sample(S)\n",
    "\n",
    "E(S) = 1 log2 1\n",
    "\n",
    "E(S) = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab4048-0fe3-4896-b09b-fd6c0ae8e193",
   "metadata": {},
   "source": [
    "#### What is Information Gain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957fc7b-5338-40e9-a9a1-7027da7b1da2",
   "metadata": {},
   "source": [
    "Now,the main question is how do we decide the best attribute.\n",
    "\n",
    "We need to calculate the \"information Gain\"\n",
    "The attribute with the highest information gain is considered the best.\n",
    "\n",
    "+ Informatiion gain measures the reduction in entropy.\n",
    "\n",
    "+ Decides which attribute should be selected as the decision node\n",
    "\n",
    "\n",
    "If S is our total collection,\n",
    "information gain = Entropy(S) - [(Weighted Avg) X Entropy(each teature)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528827d-2a55-4573-bf90-4076817e9c23",
   "metadata": {},
   "source": [
    "#### Let's build our decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53f5f4-95b0-4955-9f04-43a419397665",
   "metadata": {},
   "source": [
    "+ Step 1 : Compute the entropy for the Data set\n",
    "From our data set:\n",
    "\n",
    "> out of 14 instances we have 9 yes and 5 No\n",
    "\n",
    "> so we have the formula\n",
    "\n",
    "E(S) = - P(Yes)log2 P(Yes) -P(No)log2P (No)\n",
    "\n",
    "E(S) = -(9/14) * log2 9/14 - (5/14) * log2 5/14\n",
    "\n",
    "E(S) = 0.41 + ).53 = 0.94\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d0303-1f1e-4274-8805-1ad8b157c0e2",
   "metadata": {},
   "source": [
    "\n",
    "we are calculated the entropy of outlook when it goes sunny.\n",
    "We use this formula:\n",
    "Total number of yes = 2 and total number of No = 3\n",
    "\n",
    "E(Outlook = Sunny) = 2/5 log2 2/5 - 3/5 log2 3/5 = 0.971\n",
    "\n",
    "Next ,lets calculate the entropy of Overcost, remember , overcost is all yes, so the probability of yes = 1\n",
    "\n",
    "E(Outlook)  = Overcost) = -1 log2 1 - 0 log2 = 0\n",
    "\n",
    "E( Outlook = Sunny) = -3/5 log2 3/5 -2/5 log2 2/5 = 0.971\n",
    "\n",
    "\n",
    "+ Now you have to calculate how much information we have from outlook,\n",
    "\n",
    "1(Outlook) = 5/14 X 0.971 + 4 /14 X 0 + 5/14 X 0.971 = 0.693\n",
    "\n",
    "weighted average is total number of yes and total number of no\n",
    "\n",
    "Weighted avg. for sunny will be 5/14, since the formal is 5/14 multiplied by entropy of each feature.\n",
    "\n",
    "As calculated , the entropy of sunny is 0.971.\n",
    "\n",
    "We will do the same for overcost weighted average 4/14 multiplied by entropy 0 and for sunny 5/14 (3 yes and 2 nos) multiplied by its entropy ie 0.971 finally we take the sum of all of them which  is equal to 0.693"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae70420-5fc4-42b4-aa94-7f22543b18ec",
   "metadata": {},
   "source": [
    "##### Information Gained from Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b17d4-674d-4323-a0fd-07188f10d368",
   "metadata": {},
   "source": [
    "Informat gained = total entropy minus information taken from outlook\n",
    "\n",
    "Gain (Outlook) = E(S) - 1(Outlook)\n",
    "\n",
    "0.94 - 0.693 = 0.247"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abedc63f-3476-49e7-91fa-75d2c6cacaa1",
   "metadata": {},
   "source": [
    "+ Next is windy False and True\n",
    "\n",
    "When windy has false as its parameters it has 6 yesand 2 Nos and when it has True as its parameter it has 3 yes and 3 nos,\n",
    "\n",
    "\n",
    "First, we wil calculate the entropy of each feature starting with windy = True. incase of true , we have equal number of yes and equal number of No , so the entropy will be 1, so  entropy of true when is windy is 1.\n",
    "\n",
    "E(Windy = True) = 1\n",
    "\n",
    "Entropy of false when it was windy\n",
    " (e have 6 yes and 2 nos)\n",
    "E(S) = - P(Yes)log2 P(Yes) -P(No)log2P (No)\n",
    "\n",
    "E(windy = -6/8 log2  6/8   - 2/8 log2 * 2/8\n",
    "\n",
    "E(Windy = False) = 0.811\n",
    "\n",
    "###### lets calulate the information from windy,\n",
    "i stands for information\n",
    "i(Windy) = 8/ 14 X 6/14 X 1 = 0.892\n",
    "\n",
    "Information gained from outlook,\n",
    "\n",
    "Gain (Windy) = E(S) = I(windy)\n",
    "\n",
    "0.94 - 0.892 = 0.048\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e790f30-e52a-4fec-a65b-92302886012c",
   "metadata": {},
   "source": [
    "Similarly ,we calculate it for the rest two \n",
    "\n",
    "For Outlook: \n",
    "\n",
    "Info  0.693\n",
    "\n",
    "GAin : 0.940 - 0.693  = 0.247\n",
    "\n",
    "\n",
    "Temperature:\n",
    "\n",
    "Info                  0.911\n",
    "\n",
    "Gain : 0.940 - 0.911   = 0.029\n",
    "\n",
    "\n",
    "Humidity:\n",
    "\n",
    "Info      0.788\n",
    "Gain :  0.940 -0.788 = 0.152\n",
    "\n",
    "Windy:\n",
    "Info                    0.892\n",
    "\n",
    "Gain : 0.940 -0.982      = 0.048\n",
    "\n",
    "\n",
    "We will select the attribute with the maximum gain\n",
    "\n",
    "Since Max gain = 0.247\n",
    "\n",
    "Outlook is our Root Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cbadd3-11b4-463e-949f-9ee1cad122ca",
   "metadata": {},
   "source": [
    "+ Which Node to select further"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59b766ab-161c-4d8d-8ebc-7955e295042e",
   "metadata": {},
   "source": [
    "         Outlook\n",
    "     /      |     \\ Rain\n",
    "  sunny   Overcast       |\n",
    "             |\n",
    "   |         Yes\n",
    "             |\n",
    "        Outlook = Overcast\n",
    "        Contains only yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302f4f3-52fc-4e11-a46e-c778eac187c1",
   "metadata": {},
   "source": [
    "But in case of sunny and rainy we have yes and no ,so we need to recalculate things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a2166-0147-4c3f-8621-4f1c29cd17f4",
   "metadata": {},
   "source": [
    "Now we calculate the entropy when outlook = sunny next is sunny , first , we assume that outlook is our root node and we are calculating our information gain for it.\n",
    "we select how many number of yes and how many number of no. \n",
    "\n",
    "which node to select further?\n",
    "\n",
    "Outlook under outlook we have \n",
    "Sunny ,Overcast and Rainy.\n",
    "\n",
    "Outlook = Overcast\n",
    "Consider only Yes so we consider it as a leaf node.\n",
    "\n",
    "For Sunny and Rainy ,we have yes and no so we need to recalculate them.We have to select the attribute that has the maximum information gain\n",
    "\n",
    "\n",
    "+ This is how your complete Tree will look like."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4d7d9fe-49b3-400d-9927-cd70824dfaac",
   "metadata": {},
   "source": [
    "          Outlook\n",
    "                  |      Rain\n",
    "  sunny |     Overcast       |\n",
    "                  |\n",
    "  Humidity        Yes           Windy\n",
    "  /      \\                      /     \\\n",
    "  High   Normal              Strong   Weak\n",
    "  /          \\                /        \\\n",
    "  No         Yes              No        Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e93162-0f1f-43c0-86f2-33d238508e3e",
   "metadata": {},
   "source": [
    "You can play arround with it  ,if it is sunny, you can check the humidity condition. if the outlook is rainy ,you can check whether it is windy or not, if it is a weak rain then you will you will accept yes and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a13449-a4d0-4666-8edc-968cd334e6af",
   "metadata": {},
   "source": [
    "+ What Should I Do To Play- Pruning\n",
    "\n",
    "\n",
    "Prunning will decide how you will play.\n",
    "\n",
    "+ Pruning is nothing but cutting down the nodes in order to get the optimal solution.\n",
    "\n",
    "+ 'A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions\"\n",
    "\n",
    "\n",
    "+ Pruning : Reduces the Complexity.\n",
    "\n",
    "It shows only the results that shows that you can play ie the results for yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478a75f-dfda-47b8-9fb7-151927ac6ad4",
   "metadata": {},
   "source": [
    "+ Are tree based models better than linear models?\n",
    "\n",
    "You can think if i can use logistic regression for classification problems and linear regression for regreesion problem why the need to use a tree.\n",
    "\n",
    "You can use any algorithm, it depends on the type of problem you are solving.\n",
    "\n",
    "lets look at some key factors that wil help you decide which algorithm to use and when\n",
    "\n",
    "+ The relationship between dependent and independent variables is well approximated by a linear model, then linear regression will out perform tree based model.\n",
    "\n",
    "+ If there is a high non linearity and complex relationship between the dependent and independent variable, a tree model will out persorm a classical regression model,\n",
    "\n",
    "+ If you need to build a model you want to explai to people, a decision tree model will always do better than any linear model as decision tree models a re easier to interprte than linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48dcc2-a584-4889-a66f-1d744398f205",
   "metadata": {},
   "source": [
    "+ How to write the decision tree classifier from scratch in python using the CART Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e4fe9-eb3d-4f37-9892-14bf38dc9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For python 2 /3 compatibility\n",
    "# from _future_import print_function\n",
    "\n",
    "# Sinple dataset,\n",
    "# format : each row is the label,\n",
    "# The first two columns are features,\n",
    "# If you want you can add more features & examples,\n",
    "# Interesting note: 2nd and 5th examples have same features , but diferent labels.\n",
    "# let's see how tree handles this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fb8767-2246-4db7-9dca-4b1302ee4f35",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m49\u001b[0m\n\u001b[1;33m    'column value' (e.g., Green). The 'match' method is used to compare\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "train_data = [\n",
    "    ['Green', 3, 'Mango'],\n",
    "    ['Yellow', 3, 'Mango'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['yellow, 3, 'Lemon'],\n",
    "]\n",
    "    \n",
    "# Add some column labels\n",
    "# These are usde only to print the trees\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "\n",
    "# we define the function as unique value and pass the rows and columns\n",
    "#it will find the unique valeus for the columns in the dataset\n",
    "# We are passing trained data as our row and column number as 0 \n",
    "# we are finding the unique values in terms of diameter\n",
    "    \n",
    "def unique_vals(rows,col):\n",
    "    \"\"\"Find the unique values for a column in a dataset,\"\"\"\n",
    "    return set([row[col] for in rows])\n",
    "\n",
    "########\n",
    "# Demo:\n",
    "#Unique_vals(training_data, 0)\n",
    "# Unique_vals(training_data, 1) \n",
    "########\n",
    "    \n",
    "# We define a function as class count and pass the row into it    \n",
    "    \n",
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset,\"\"\"\n",
    "    counts = {} # a dictionary of label => count.\n",
    "    for row in rows:\n",
    "    # in our dataset format, the label is always the last column\n",
    "    label = row[-1]\n",
    "    if label not in counts:\n",
    "        counts[label] = 0\n",
    "    counts[label] += 0\n",
    "    return counts\n",
    "    \n",
    "#######\n",
    "# Demo :\n",
    "# Class_Counts(training_data)\n",
    "#######\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98f510-b97b-4c4d-954b-6903f0cdfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function as numeric and pass the value , it will just test if the value is numeric or not and it will return the value is an interger or a float\n",
    "def is numeric(value):\n",
    "    \"\"\"Test if a value is numeric,\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float) \n",
    "    \n",
    "######\n",
    "# Demo: \n",
    "# is_numeric(7)\n",
    "# is_numeric(\"Red\")\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18fba92a-410b-420a-81bf-b1c303e0207a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (Temp/ipykernel_8988/3901149733.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_8988/3901149733.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    question. See the demo below.\u001b[0m\n\u001b[1;37m                                 \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "# We define a class in this question\n",
    "# We will now ask a question to partition the dataset \n",
    "# the class is used to recall a column number e.g 0 and the column value e.g green\n",
    "    \n",
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\n",
    "        \n",
    "     This class just records a 'column number' (e.g., 0 for ) and a\n",
    "    'column value' (e.g., Green). The 'match' method is used to compare\n",
    "     the feature value in an example to the feature value stored in the\n",
    "    question. See the demo below.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, column, value):\n",
    "           self.column = column\n",
    "           self.value = value\n",
    "# we difine a function as match ,which compares the feature value \n",
    "# for e.g the feature value in this question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959e6e5-daef-44d1-b235-b3e33f1fb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function as match ,it compares the feature value\n",
    "def match(self, example):\n",
    "        # Compare the feature value in an example to the \n",
    "        # feature value in this question.\n",
    "        Val = example[self.column]\n",
    "        if is numeric(Val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "        return val == self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d64885-9243-4c1e-bc51-3fe2a280cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s?\" (\n",
    "            header[self.column], condition, str(self.value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c121b5-12c7-46c6-be45-785ef8b29976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a functon partition ,which is used to partition th dataset.\n",
    "# Each row in the dataset it checks if it matches the question or not\n",
    "# if it does it adds it to the True rows if it does not it adds it to the false rows\n",
    "\n",
    "def partition(rows, question);\n",
    "    \"\"\"Partitions a dataset.\n",
    "    For eacg row in the dataset ,check if it matches the question.if\n",
    "    so,add it to 'true rows', otherwise, ass it to 'false rows',\n",
    "    \"\"\"\n",
    "    \n",
    "    true_rows,, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row)\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "        return true_rows, false_rows \n",
    "    \n",
    "######\n",
    "# Demo: \n",
    "# let's partitiom the training data based on wthether  rows are Red.\n",
    "# True_rows, false_rows = parttion(training_data, Question(0, 'Red'))\n",
    "# This will contain all the 'Red' rows.\n",
    "# true_rows\n",
    "#This will contain everything else.\n",
    "# fals_rows\n",
    "######    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba90ad-1aa1-442e-a8fa-7ac2e22aa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a gin impurity function and pass a list of rows\n",
    "# It will calculate the gini impurity for the list of rows\n",
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "    \n",
    "    There are a few different ways to do this , i thought this one was\n",
    "    the most concise. See:\n",
    "    https://en.Wikipedia.org/wiki/Decision_tree_learning# Gini_impurity\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for 1b1 in counts:\n",
    "        prob_of-1b1 = counts[1b1] / float(len(rows))\n",
    "        impurity  -= prob_of_1b1**2\n",
    "    return impurity\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Let's look at some examples to understand how gini impurity works\n",
    "# First we will look at a dataset with no mixing\n",
    "# No_mixing =      [['Mango']\n",
    "#                   ['Mango']\n",
    "#                  ['Grape'],\n",
    "#                 ['Grapefruir'],\n",
    "#                ['blueberry']]\n",
    "#this will return 0.8\n",
    "# gini(lots_of_mixing))\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae66ef-f9c3-45fc-817b-fce4be70994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function as information gain, this calculates the information gain\n",
    "# Uncertainty of the starting node minus the weighted impurity of the child node\n",
    "\n",
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain.\n",
    "    \n",
    "    The uncertainty of the starting node, minus the weighted impurity of \n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) + gini(right)\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Calculate the uncertainty of our training data\n",
    "#current_uncertainty = gini(training_data)\n",
    "#\n",
    "# How much information do we gain by partioning on 'Green'?\n",
    "# true_rows, false_rows = partitioning(training_data, Question( 0, 'Green'))\n",
    "# info_gain(true_rows, false_rows, current_uncertainty)\n",
    "#\n",
    "# what about if we partitioned on 'Red' instead?\n",
    "# true_rows, false_rows = partitioning(training_data, Question( 0, 'Red'))\n",
    "# info_gain(true_rows, false_rows, current_uncertainty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d90b6f-c3fc-447a-816a-cc6a7ab70556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the bset split ,This function is used to find the best question to ask\n",
    "# iterating or every feature value and then calculating the information gain\n",
    "\n",
    "def find_best_split(rows):\n",
    "    \"\"\"find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"'\n",
    "    best_gain = 0 # keep track of the best information gain\n",
    "    best_question = None # keep train of the feature / value that produced it\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1 # number of columns\n",
    "    \n",
    "    for col in range(n_features): #for each feature\n",
    "    \n",
    "    values = set([row[col] for in rows]} # unique values in the column\n",
    "    \n",
    "    for val in values: # for each value\n",
    "    \n",
    "    question = Question(col, val)\n",
    "    \n",
    "    # try splitting the dataset\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "    \n",
    "    #skip this split if it doesn't divide the \n",
    "    # dataset.\n",
    "    \n",
    "    if len(true_rows) == 0 or len(false_rows)  == 0:\n",
    "        continue\n",
    "        \n",
    "     # Calculate the information gain from this split\n",
    "     gain = info_gain(true_rows, false_rows,current_uncertainty)\n",
    "     \n",
    "     # you actually can use '>' instead of '>=' here\n",
    "     # but i wanted the tree to look a certain way for our\n",
    "     # toy dataset\n",
    "     \n",
    "     if gain >= best_gain:\n",
    "        best_gain, best_question = gain, question\n",
    "        \n",
    "  return best_gain, best_question\n",
    "  \n",
    "########\n",
    "# Demo: \n",
    "# Find the best question to ask first for toy dataset,\n",
    "# best_gain, best_question - find_best_split(training_data)\n",
    "# rvI : is color == Red is just as good. see the note in the code above\n",
    "# wher I used '>=',\n",
    "########\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a68f7-ca8a-4211-8eec-84c366f56650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaf:\n",
    "\"\"\"A leaf node classifies data:\n",
    " \n",
    "This holds a dictionary of class (e.g., \"Mango\") -> number of times\n",
    "It appears in the rows from the training data reach ths leaf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7fc41-55f7-4e2b-8989-a6e4584c4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, rows):\n",
    "   self.predictions = class_counts(rows)\n",
    "   \n",
    "class  Decision_Node:\n",
    "   \"\"\"A Decision Node asks a question:\n",
    "    \n",
    "    This holds a reference to the question, and in the child nodes: and which question to ask further and to which branch\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc38d0-4ea0-4b10-95c9-b7f645400385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self,\n",
    "                    question,\n",
    "                    true_branch,\n",
    "                    false_branch):\n",
    "            self.question = question\n",
    "            self.false_branch = false_branch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14426c-6308-4b53-89f6-26ff2eb682fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this function to build the tree\n",
    "def build_tree(rows):\n",
    "    \"\"\"Builds the tree.\"\"\"\n",
    "    \n",
    "    # Try partitioning the dataset on each of the unique attribute,\n",
    "    # Calculate the information gain,\n",
    "    # and return the question that produces the highest gain,\n",
    "    gain, question = find_best_split(rows)\n",
    "    \n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    \n",
    "    if gain == 0:\n",
    "        return leaf(rows)\n",
    "    \n",
    "    # if we reach here,we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_rows, false_rows = partition(rows, Question)\n",
    "    \n",
    "    # Recursively build the true branch.\n",
    "    true_branch = build_tree(true_rows)\n",
    "    \n",
    "    # Recursively build the false branch \n",
    "    false branch = build_tree(false_rows)\n",
    "    \n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # depending on the answer.\n",
    "    return Decision_Node(question, true_branch, false_branch\n",
    "    \n",
    " def print_tree(node, spacing=\"\"):\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a02577-c138-487e-9994-6533d2403564",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def print_tree(node, spacing=\"\"):\n",
    "    \n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, leaf):\n",
    "        print (spacing + \"predict\", node.predictions)\n",
    "        return\n",
    "    \n",
    "    # print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "    \n",
    "    # call this function recursively on the true branch\n",
    "    ptint (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spcing + \" \")\n",
    "    \n",
    "    # define_tree function that we will use to print the tree\n",
    "    # call this functiom recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_treee(node.false_branch, spacing + \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d069e260-c2e8-44b3-a4a1-ad64a88008a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m46\u001b[0m\n\u001b[1;33m    ]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# we use the classify function to decide whether to follow the true branch or the false branch\n",
    "\n",
    "def classify(row, node))\n",
    "\n",
    "# decide whether to follow the true branch or false branch.\n",
    "# Compare the feature / value stared in the node,\n",
    "# to the example we are considering.\n",
    "if node.question.match(row):\n",
    "    return classify(row, node.true_branch)\n",
    "else\n",
    "return classify(row, node.false_branch)\n",
    "\n",
    "#######\n",
    "# Demo: \n",
    "# printing that a bit nicer\n",
    "# print_leaf(classify(training_data[1], my_tree))\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "def print_leaf(counts):\n",
    "    \"\"\"print the predictions at  leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for 1b1 in counts.keys():\n",
    "        probs[1b1] = str(int(counts[1b1] / total * 100)) + \"%\"\n",
    "        return probs\n",
    "    \n",
    "   ####### \n",
    "   # Demo: \n",
    "    # on the second example, the confidence is lower\n",
    "    # print_leaf(classify(training_data[I], my_tree))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    my_tree = build_tree(training_data)\n",
    "    \n",
    "    print_tree(my_tree)\n",
    "    \n",
    "    # Evaluate\n",
    "    testing_data = [\n",
    "        ['Green', 3, 'Mango']\n",
    "        ['Yellow', 4, 'Mango']\n",
    "        ['Red', 2, 'Grape']\n",
    "        ['Red', 1, 'Grape']\n",
    "         ['Yellow', 1, 'Lemon'],\n",
    "    ]\n",
    "    \n",
    "    for row in testing_data:\n",
    "        print (\"Actual: %. Predicted: %s\" %\n",
    "               (row[-1], print_leaf(classify(row, my_tree))))\n",
    "        \n",
    "        # Next step\n",
    "        # add support for misssing (or unseen) attributes\n",
    "        # print the tree to prevent overfitting\n",
    "        #add support for regression\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f79490-73de-4509-bc7b-c0090af63036",
   "metadata": {},
   "source": [
    "We can check out scikit-learn Algorithm cheat-sheet teaches you which algorithm to use and when\n",
    "\n",
    "First condition to check:\n",
    "If you have a free sample or not\n",
    "If your sample is greater than 50 then we move ahead if it is less than 50 , then you need to collect more data.\n",
    "\n",
    "If your samples is greater than 50, then you need to decide whether you want to predict a category or not.\n",
    "\n",
    "If you want to predict a category, you need to check further if you have labelled data or not.\n",
    "\n",
    "If you have labelle data then that will be a classification algorithm problem ,if you dont have a labelled data then it will be a clustering problem .\n",
    "\n",
    "If you dont want to predict a category, then what do you want to predict, predict a quantity, if you want to predict a quantity, then it will be a regression problem.\n",
    "\n",
    "If you dont want to predict a quantity and you want to keep looking further, then in that case , you should go for dimensonality reduction  problems, still if you dont want to look at predicting strctures not working then you have a tough luck Smile!1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117941a-edbe-47ca-9b5e-8053e08fc058",
   "metadata": {},
   "source": [
    "### Decision Tree In Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aeee2f-ead6-4d20-847a-699f756eb5de",
   "metadata": {},
   "source": [
    "#### Decision Tree Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633f6e3-fb67-4a6f-9ccf-f876d2ad20ad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Decision tree begins with a problem , For eg,I think i have to buy a car. then the following questions follows:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc2f0645-67ee-4247-adcf-f293ca08ca46",
   "metadata": {},
   "source": [
    "\n",
    "      Will it be sufficient for 6 perple\n",
    "                       |\n",
    "  is price < $15?       Num of Airbags = 4\n",
    "  \n",
    "  Is Mileage>20?        Anti_lock Brakes>\n",
    "  \n",
    "After all these questions ,you now make a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7856d-e2a0-4e59-9769-87886f70b305",
   "metadata": {},
   "source": [
    "After all these questions ,you now make a decision.\n",
    "Hence, we go through these decision process using a decision tre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c2620-940e-4fe2-bbba-766215785eeb",
   "metadata": {},
   "source": [
    "#### What 's in it for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017f0d9-7fc2-49a6-9c73-94eb461eb8c9",
   "metadata": {},
   "source": [
    "> What is machine learning\n",
    "\n",
    "> Problems in Machine Learning \n",
    "\n",
    "> What are the problems a Decision Tree solves?\n",
    "\n",
    "> Advantages od Decision trees\n",
    "\n",
    "> Disadvantages of Decision tree\n",
    "\n",
    "> How does Decision Tree work \n",
    "\n",
    "> Use case - Loan repayment prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17b296-21d3-44dd-bfcd-6fe8b193b3a0",
   "metadata": {},
   "source": [
    "#### What is machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e571b-574d-458b-ae66-315c8d274c9b",
   "metadata": {},
   "source": [
    "Machine learning is a part of 'Artificial Intelligence\" Artificial intelligence helps you to make new decisions. It is trying to understand the world better.\n",
    "\n",
    "Machine learning is about \n",
    "\n",
    "+ learning\n",
    "\n",
    "+ Deciding  and\n",
    "\n",
    "+ Predicting things\n",
    "\n",
    "We can make a good guess whether our predictions will be a good investment or not.\n",
    "\n",
    "It helps us to analyze data in ways we never thought is possible.\n",
    "\n",
    "We can do facial recognition, medical regcognition.\n",
    "\n",
    "+ Machine learning is an application of artificial intelligence wherein the system gets the ability to automatically learn and improve based on experience.\n",
    "\n",
    "Machine learning with the use of artificial intelligence can help you try out things or see things you have not seen before through the use of artificial intelligence and be able to profer solution.\n",
    "\n",
    "With machine learning you make smarter choices with less work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a6bf1-b350-4fe4-b4de-455839f662ad",
   "metadata": {},
   "source": [
    "+ Types of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537d019-24c3-44d7-9c8c-819f4268f4c1",
   "metadata": {},
   "source": [
    "+ Supervised learning : \n",
    "\n",
    "You already have the data and the answer.\n",
    "For example ,you have a list of the loan and who defaulted on them,\n",
    "With the help of machine learning tools you will be able to project whether the next person on the list will be able to make his payment or not.\n",
    "\n",
    "\n",
    "+ Unsupersied Learning :\n",
    "\n",
    "You dont know the answers. you just have alot of information coming in.\n",
    "It allows you to group your information together.\n",
    "\n",
    "If you have photos, you can group all the images of trees together without even knowing where the house or tree is .\n",
    "\n",
    "Reinforcement learning: \n",
    "\n",
    "Unlike surpervised or unsurpervise learning , you dont have the data prior to starting so you need to get the data one line at a time. Whether you make a good or bad choice, the machine learning tool have to adjust the co ordinates to get a plus or minus feed back. you can liken this to the way a human learns. A human experience life one minute at a time and we learn from there. either our memory is good or we learn to avoid some."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd4b7e-ecba-4bdf-b04a-7858b81c5cf6",
   "metadata": {},
   "source": [
    "#### Problems in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045591a-069b-4ff4-b142-c7eadb87f6de",
   "metadata": {},
   "source": [
    "WE have to understand weether our Decision Tree fit in to ou machine learning tools.\n",
    "\n",
    "We have to understand the basis of some of machine learning problems\n",
    "\n",
    "The primary ones are as follows:\n",
    "\n",
    "+ Classification:\n",
    "\n",
    "problems with categorical solutions like 'Yes' or 'No' , 'True' or 'false' , '1' or '0'\n",
    "\n",
    "+ Regression:\n",
    "\n",
    "Problems wherein continuous value needs to be predicted like 'Product prices', 'profit'\n",
    "\n",
    "\n",
    "+ Clustering :\n",
    "\n",
    "problems where data needs to be organized to find specific patterns like in the case of 'Product Recommendation,\n",
    "\n",
    "Group products acording to the number of persons that bought them.\n",
    "\n",
    "Most commonly used for decision tree is classification , for e.g is it red or not, is it a fruit or vegestable, yes or no, true or false, left or right ,0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8cd4d-2134-4d8c-9d72-17a0d3e50614",
   "metadata": {},
   "source": [
    "+ Classification\n",
    "\n",
    "+ Four main tools used in classifications\n",
    "\n",
    "> Naive BAyes\n",
    "\n",
    "> Decision tree\n",
    "\n",
    "> Logustic Regression\n",
    "\n",
    "> Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cf97a-25c1-467e-b7f2-6483bb3c8417",
   "metadata": {},
   "source": [
    "The Naive Bayes and Logistic Regreesion are for simpler data. If your data is notvery  complex you can use either of them to do a very good representation by drawing a line through the data or curve through the data.\n",
    "\n",
    "\n",
    "For complicated data you need the decision tree and if you have large amount of data ,then you need to use the Random forest. \n",
    "So the decision tree is a part of the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be67e8f-0479-4d06-a126-9f3afb4419e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What is a Decision Tree ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e2612-0f74-4945-a173-609a59e3a9ab",
   "metadata": {},
   "source": [
    "+ Decision tree is a tree shaped diagram used to determine a course of action. Each branch of the tree represents a possible decision, occurance or reaction.\n",
    "\n",
    "Example:\n",
    "\n",
    "How do i identify a random vegetable from a shopping van.\n",
    "\n",
    "First, we have a script of vegetable  so we start by asking the following question.    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d08742eb-60dc-4e55-94f5-03f18a5e7444",
   "metadata": {},
   "source": [
    "         Which Vegetable?\n",
    "            /             \\\n",
    "Is color red = Red\n",
    " /           \\\n",
    " FALSE        TRUE\n",
    "              DIAMETER > 2\n",
    "              /     \\\n",
    "              FALSE  TRUE\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71551c-fe8e-4fb5-8895-71e93c461cca",
   "metadata": {},
   "source": [
    "+ Problems that Decision Tree can solve\n",
    "\n",
    "+ Classification\n",
    "\n",
    "+ Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caadc5d-a0fb-4a5e-9a9e-c59951ac2106",
   "metadata": {},
   "source": [
    "In Classification:\n",
    "\n",
    "A classification tree will determine a set of logical if-then conditions to classify problems. \n",
    "For example, discriminating between three types of flowers based on certain features\n",
    "\n",
    "In Regression:\n",
    "\n",
    "a regression tree is used when a target variable is nummerical or continuous in nature. We fit a regression model to the target variable using each of the independent variables.\n",
    "Each split is made based on the sum of squared error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba54be-70f0-4fc6-9b96-708c67692405",
   "metadata": {},
   "source": [
    "#### Advantages of Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64e603-e534-40af-bfe1-10ed20dc7984",
   "metadata": {},
   "source": [
    "+ It is simple to understand, interprete and visualize.\n",
    "\n",
    "+ Little effort is required for data preparation , so you dont have to do special scaling\n",
    "\n",
    "+ It can handle both numerical and categorical data\n",
    "\n",
    "+ Non linear parameters do not affect its performance,\n",
    "\n",
    "So even if the data doesn't fit uneasy curve graph ,you can still use it to create an effective decision or to prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059325d7-089a-433e-b58f-61d7a56a3749",
   "metadata": {},
   "source": [
    "#### Disadvantages of Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0a8a9-1da1-409f-a38d-810fb83932e2",
   "metadata": {},
   "source": [
    "+ Overfitting:\n",
    "This occurs when the algorithm captures noise in the data. this means observing from one specific instance instead of a general solution for all the data.\n",
    "\n",
    "+ High Variance:\n",
    "The model can get a unstable due to small variation in the data.\n",
    "\n",
    "+ Low biaed Tree:\n",
    "A high complicated Decision tree tends to have a low bias which makes it difficult for the model to work with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930a1e4-7b90-4044-ae55-f41d3e08ca60",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  Decision Tree - Important terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722b48b-1142-4296-99e6-c1004b64f023",
   "metadata": {},
   "source": [
    "+ Entropy :\n",
    "\n",
    "This is the measure or randomness or unpredictability in the dataset.\n",
    "\n",
    "For example , if you have a group of animal in a dataset and you are unable to pick out the particular type of animal in the dataset by just loking at the animals , it means that the dataset has a high Entropy.\n",
    "\n",
    "as we start splitting it into sub group we come up with a second definition which is information gain.\n",
    "\n",
    "INFORMATION GAIN\n",
    "\n",
    "It is the measure of Decrease in Entropy after the Dataset is split.\n",
    "We split the animal to true or false based on thier color and as we continue to split it the entropy continued to be less and less.\n",
    "\n",
    "So the Entropy is simply the E1 minus  E2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d275a0f4-ff24-4ba4-8da2-9432c68193f4",
   "metadata": {},
   "source": [
    "                         EXAMPlE\n",
    " High                   \n",
    " Entropy(E1)              Color == Yellow?\n",
    "   | after              True        /\\    False\n",
    "Lower split                   /                     Entropy    \n",
    " (E2)           Height= 10?         Height = 10?\n",
    "  |              True  /\\ False       True/\\False \n",
    "           \n",
    " Gain = (E1 -E2)                 \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24624b-e2cc-4b86-9304-389199e5b271",
   "metadata": {},
   "source": [
    "+ Leaf Node :\n",
    "\n",
    "leaf node carries the classification or the Decision.\n",
    "\n",
    "The decision node have 2 or more branches. this is where we are breaking the group up into different parts.\n",
    "\n",
    "\n",
    "+ Root Node \n",
    "\n",
    "The top most Decision Node is known as the Root Node\n",
    "    ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542cda6-e44b-46b5-b01a-cfb3c11c5d38",
   "metadata": {},
   "source": [
    "+ How does a Decision Tree work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ed51c-208f-4398-9b8f-701b37347f17",
   "metadata": {},
   "source": [
    "A hunter asking this question : I wonder what kind of animal will get in th jungle today.\n",
    "\n",
    "Let's try to classify different types of animals based on thier features using a Decision Tree.\n",
    "\n",
    "> Problem Statement\n",
    "\n",
    "To classify the diferent types of animals based on their features using Decision Tree\n",
    "\n",
    "\n",
    "The dataset is looking quite messy  and the Entropy is High in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0d806-bd3c-4b6d-aabe-825df183acc2",
   "metadata": {},
   "source": [
    "+            Training Dataset\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62e6bd22-364d-476b-bacb-e47ed0a84b88",
   "metadata": {},
   "source": [
    "Color               Height             Label\n",
    "\n",
    "Grey                   10               Elephant\n",
    "\n",
    "Yellow                 10                Giraffe\n",
    "\n",
    "Brown                   3                Monkey\n",
    "\n",
    "Grey                    10               Elephant\n",
    "\n",
    "Yellow                  4                 Tiger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46a6e9-4f4e-433a-a6e1-7cf44d6b3ee3",
   "metadata": {},
   "source": [
    "+ How to split the data\n",
    "\n",
    "We have to split the data in such a way that the information gain is the highest.\n",
    "\n",
    "\n",
    "Note, Gain is the measure of Decrease in Entropy after spliting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107930c3-f28d-4d9f-97db-a5259b2e8f13",
   "metadata": {},
   "source": [
    "Formula for Entropy :\n",
    "\n",
    "This is the sum of k, y= 1 to k, k represents the \n",
    "\n",
    "number of differnt animals, while value of p is \n",
    "\n",
    "the percentage of that animal.\n",
    "\n",
    "Σ^k i=1  P(value).log2(p(value,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740078a-6f62-4114-be25-4706fe1dba83",
   "metadata": {},
   "source": [
    "+ Let's try to calculate the Entropy for the current Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495d98a-1543-4ca3-879b-acea88909173",
   "metadata": {},
   "source": [
    "We have \n",
    "\n",
    "+ 3 giraffe\n",
    "\n",
    "+ 2 Tigers\n",
    "\n",
    "+ 1 Monkey\n",
    "\n",
    "+ 2 Elephant\n",
    "\n",
    "Total = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80eccc8-d41f-434b-9f54-b9127d4b4a7d",
   "metadata": {},
   "source": [
    "+ Let's use the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad65d46-5725-485a-aa36-ca203058a157",
   "metadata": {},
   "source": [
    "Σ^k i=1 P(value).log2 (p(value,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa72e46-fa78-4e25-9c20-007c210ddf7f",
   "metadata": {},
   "source": [
    "Entropy = (3/8)log2(3/8)+ (2/8)log2(2/8) +\n",
    "\n",
    "(1/8)log2(1/8)+(2/8)log2+(2/8)\n",
    "\n",
    "Entropy = 0.571"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c441b2-5587-45f3-84d1-c51f54934611",
   "metadata": {},
   "source": [
    "The program will ask you to calculate the entropy of the dataset , similarly ,after every split to calculate the gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422ea7a-8053-4ec7-b625-e2c61e9fcefd",
   "metadata": {},
   "source": [
    "This is the formula and mathematics behind it to be calculated by finding the diference of the subsequent entropy values after split.\n",
    "\n",
    "We wil now try to choose a condition that gives us the highest gain.\n",
    "\n",
    "We wil do that by splitting the data using each condition and checking the gain we get out of them.\n",
    "\n",
    "The condition that gives us the highest gain will be used to make the first split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fe7bb-5e61-4e05-a501-6121748a3223",
   "metadata": {},
   "source": [
    "+ Conditions\n",
    "\n",
    "Colour == Yellow ?\n",
    "\n",
    ">     Height >=10\n",
    "\n",
    ">     Color== Brown?\n",
    "\n",
    ">     Color==Grey\n",
    "\n",
    ">     Diameter<10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3c7ae-5d50-4664-8dd9-c29604975c93",
   "metadata": {},
   "source": [
    "+ Training Dataset     "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a17a864-c7d6-4f76-a5d2-288f388c8e54",
   "metadata": {},
   "source": [
    "Color               Height         Label    \n",
    "\n",
    "Grey                  10        Elephant      \n",
    "\n",
    "Yellow                10        Giraffe\n",
    "       \n",
    "\n",
    "Brown                   3       Monkey          \n",
    "\n",
    "Grey                    10     Elephant          \n",
    "\n",
    "Yellow                  4       Tiger          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430822b-460f-4433-9d4c-a404608d8617",
   "metadata": {},
   "source": [
    "Let's say the condition that gives us maximum gain is yellow, so we will split the data based on the colour yellow.\n",
    "if it is truw that group of animal  goes to the left and if it is false it goes to the right. The entropy after splitting has decreased considerably. \n",
    "\n",
    "However, we still need some splitting at both the branches to attain an entropy value equal to zero.\n",
    "\n",
    "Hence, we decide to split both the nodes using height as the condition and we will get a second split as seen below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65da8b69-3c9c-4835-8564-aaf1edead39e",
   "metadata": {},
   "source": [
    "           colour is yellow\n",
    "                  /\\\n",
    "        true              false\n",
    "        /\\                  /\\\n",
    "  Height>=10               Height<10\n",
    "        /\\                    /\\\n",
    "  True   false            True   false "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252ef4e-4597-4c6b-b378-2fe08fb5e0ab",
   "metadata": {},
   "source": [
    "Since, every branch now contains single label type  can say that the entropy in this case has reached the least value.\n",
    "\n",
    "We now have the girraffe ,monkeys, tigers and elephants all seperated to their own groups.\n",
    "This tree can now predict all the classes of animals that are present in the dataset with a 100% accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb06e1e-c5f5-4b03-94ed-bcf34f769394",
   "metadata": {},
   "source": [
    "#### Use Case - Loan Repayment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26f1fb-7c3b-42ac-b65e-3fbf2a7fd3d7",
   "metadata": {},
   "source": [
    "> I need to find out if my customers are going to return the loan they took from my bank or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74f3fd-3047-4529-933e-dc7aa3847e6f",
   "metadata": {},
   "source": [
    "From this we will generate a problem statement  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb50616-8df9-4c33-8579-9057bd4a2f2d",
   "metadata": {},
   "source": [
    "+ Problem Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ea011-b31f-49f9-8fc4-d2a67a2e8f33",
   "metadata": {},
   "source": [
    "To predict if a customer will repay loan amount or not using decision tree algorithm in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff545fb-9b3d-4864-8aeb-273dc1924cec",
   "metadata": {},
   "source": [
    "###### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5fc9eda-a628-4a6d-a135-4b7dc01b540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split # no longer in use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee79984-2b93-466a-a608-71d197dec630",
   "metadata": {},
   "source": [
    "The numpy helps with complicated mathematics in machine learning.\n",
    "\n",
    "We will use our pandas, it is a dataframe set up and it helps you to store your data in excel sheet form.\n",
    "We will need to split the data using train test split from sklearn.model selection import train test split\n",
    "\n",
    "We use the decision tree classifier to do all the mathematics you need to do via the entropy and information gain.\n",
    "\n",
    "We will use the sklearn .accuracy score to score our dataset\n",
    "\n",
    "Then we need the sklearn import tree for the decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30480dde-f51b-4ad5-9c54-a8235ba9ae2c",
   "metadata": {},
   "source": [
    "###### loading data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "110c9c55-b6aa-4cee-ae40-65af1d2ea076",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\Decision+Tree+Dataset.csv\", \n",
    "sep= ',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06a1e2c7-5ee9-486f-be09-393dbc6e2a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial payment</th>\n",
       "      <th>Last payment</th>\n",
       "      <th>Credit Score</th>\n",
       "      <th>House Number</th>\n",
       "      <th>sum</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>10018</td>\n",
       "      <td>250</td>\n",
       "      <td>3046</td>\n",
       "      <td>13515</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205</td>\n",
       "      <td>10016</td>\n",
       "      <td>395</td>\n",
       "      <td>3044</td>\n",
       "      <td>13660</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>257</td>\n",
       "      <td>10129</td>\n",
       "      <td>109</td>\n",
       "      <td>3251</td>\n",
       "      <td>13746</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>246</td>\n",
       "      <td>10064</td>\n",
       "      <td>324</td>\n",
       "      <td>3137</td>\n",
       "      <td>13771</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117</td>\n",
       "      <td>10115</td>\n",
       "      <td>496</td>\n",
       "      <td>3094</td>\n",
       "      <td>13822</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>413</td>\n",
       "      <td>14914</td>\n",
       "      <td>523</td>\n",
       "      <td>4683</td>\n",
       "      <td>20533</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>359</td>\n",
       "      <td>14423</td>\n",
       "      <td>927</td>\n",
       "      <td>4838</td>\n",
       "      <td>20547</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>316</td>\n",
       "      <td>14872</td>\n",
       "      <td>613</td>\n",
       "      <td>4760</td>\n",
       "      <td>20561</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>305</td>\n",
       "      <td>14926</td>\n",
       "      <td>897</td>\n",
       "      <td>4572</td>\n",
       "      <td>20700</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>168</td>\n",
       "      <td>14798</td>\n",
       "      <td>834</td>\n",
       "      <td>4937</td>\n",
       "      <td>20737</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Initial payment  Last payment  Credit Score  House Number    sum Result\n",
       "0                201         10018           250          3046  13515    yes\n",
       "1                205         10016           395          3044  13660    yes\n",
       "2                257         10129           109          3251  13746    yes\n",
       "3                246         10064           324          3137  13771    yes\n",
       "4                117         10115           496          3094  13822    yes\n",
       "..               ...           ...           ...           ...    ...    ...\n",
       "995              413         14914           523          4683  20533     No\n",
       "996              359         14423           927          4838  20547     No\n",
       "997              316         14872           613          4760  20561     No\n",
       "998              305         14926           897          4572  20700     No\n",
       "999              168         14798           834          4937  20737     No\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d58ab234-a4a3-421b-b208-a0d7a74e958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length::  1000\n",
      "Dataset Shape::  (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset length:: \", len(balance_data))\n",
    "print(\"Dataset Shape:: \", (balance_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c07e30-70c2-4c45-abda-ac057f133821",
   "metadata": {},
   "outputs": [],
   "source": [
    "My result for SHAPE IS DIFFERENT !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9801c6cf-6fbb-4579-b752-f8998b51d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial payment</th>\n",
       "      <th>Last payment</th>\n",
       "      <th>Credit Score</th>\n",
       "      <th>House Number</th>\n",
       "      <th>sum</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>10018</td>\n",
       "      <td>250</td>\n",
       "      <td>3046</td>\n",
       "      <td>13515</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205</td>\n",
       "      <td>10016</td>\n",
       "      <td>395</td>\n",
       "      <td>3044</td>\n",
       "      <td>13660</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>257</td>\n",
       "      <td>10129</td>\n",
       "      <td>109</td>\n",
       "      <td>3251</td>\n",
       "      <td>13746</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>246</td>\n",
       "      <td>10064</td>\n",
       "      <td>324</td>\n",
       "      <td>3137</td>\n",
       "      <td>13771</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117</td>\n",
       "      <td>10115</td>\n",
       "      <td>496</td>\n",
       "      <td>3094</td>\n",
       "      <td>13822</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Initial payment  Last payment  Credit Score  House Number    sum Result\n",
       "0              201         10018           250          3046  13515    yes\n",
       "1              205         10016           395          3044  13660    yes\n",
       "2              257         10129           109          3251  13746    yes\n",
       "3              246         10064           324          3137  13771    yes\n",
       "4              117         10115           496          3094  13822    yes"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pandas model for head print statement to explore the data\n",
    "print(\"Dataset:: \")\n",
    "balance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b87d8336-81d4-40fb-8f3e-7e6e9ec1d07b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15932/1657121841.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state=100,\n\u001b[0;32m      9\u001b[0m   max_depth=3, min_samples_leaf=5)\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mclf_entropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \"\"\"\n\u001b[0;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    904\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    181\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    182\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "#train and build our data tree\n",
    "#seperating the Target variable\n",
    "X = balance_data.values[:, 1:5]\n",
    "Y = balance_data.values[:,0]\n",
    "# Splittiing dataset into Test and Train \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size= 0.3, random_state = 100)  \n",
    "#funcion to perform training with Entropy\n",
    "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state=100,\n",
    "  max_depth=3, min_samples_leaf=5)\n",
    "clf_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3df29-3d3c-4ee4-a3d3-738f32d14510",
   "metadata": {},
   "source": [
    "##### max depth means it will run  only tree layers before it stops.\n",
    "Minimum sample of leaf is 5, we will have 3 splits ,no more than 3 layers and 5 leafs , we have created our decision tree classifier above and trained it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a0de15d-6ac6-494f-b308-18898e83c3ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeClassifier' object has no attribute 'tree_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15932/536003722.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# function to make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_entropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_en\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecisionTreeClassifier' object has no attribute 'tree_'"
     ]
    }
   ],
   "source": [
    "# function to make predictions\n",
    "y_pred_en = clf_entropy.predict(X_test)\n",
    "print(y_pred_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cff832-b7fa-48cd-84fa-2d8a6ae6cd17",
   "metadata": {},
   "source": [
    "X_test represents the new loan you want to find out if the person will pay back or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c1e4b-0fe4-496b-b8bb-91e5e7754f67",
   "metadata": {},
   "source": [
    "Since sklearn does all the maths for you, you will use a simple line of code to find out the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "046b23d4-a638-4df0-bc8e-29854e1b5112",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15932/1687160606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Checking the Accuracy score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy is \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_en\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_en' is not defined"
     ]
    }
   ],
   "source": [
    "#Checking the Accuracy score\n",
    "print(\"Accuracy is \", accuracy_score(y_test,y_pred_en)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57570e-dc7a-4e79-af72-94525c651238",
   "metadata": {},
   "source": [
    "#### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2f195-abe8-4515-96a7-79e51dfb2505",
   "metadata": {},
   "source": [
    "So, we have create a model that uses decision tree algorithm to predict whether a customer will repay the loan or not.\n",
    "\n",
    "The accuracy of the model is about 94.6% the bank can now use this model to decide whether it should approve loan request from a paticular customer or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d3ae4-0e12-4269-8166-0f80a4ef5b23",
   "metadata": {},
   "source": [
    "### Required Reading: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151ee00-2099-4b2c-b060-fcb1360b5d5f",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d583a0d-acce-4134-a687-250e4fbfdc83",
   "metadata": {},
   "source": [
    "##### The Entropy of a Partition\n",
    "\n",
    "\n",
    "Entropy can be defined as the measure of uncertainty in a sequence of\n",
    "random events. It is the rate of disorderliness in a sample space and is\n",
    "directly opposed to knowledge. When the entropy of a system is high, the\n",
    "knowledge that can be derived from the system is low and vice versa. An\n",
    "intuitive understanding of entropy is thinking of it as the amount of\n",
    "questions required to ask to arrive at some knowledge. For example, if I\n",
    "picked a random number and you were trying to guess what number it is.\n",
    "Asking a question like, “Is it an odd number”, reduces the possibilities\n",
    "space by half. This means that the entropy or the degree of uncertainty in\n",
    "trying to determine which number I choose is reduced. In the same vein, the\n",
    "amount of information gain is large because the question moved you closer\n",
    "to the answer by dividing the sample space. Entropy usually ranges from 0\n",
    "to 1. A system with an entropy of 0 is highly stable and the knowledge that\n",
    "can be derived from such a system is high. In general terms, low entropy in\n",
    "a system indicates high knowledge while high entropy indicates low\n",
    "knowledge or instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371cce42-a30e-421f-bc76-7f479204ef48",
   "metadata": {},
   "source": [
    "Entropy can be represented mathematically as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4ce8a-93a0-4ae1-969c-aa27a1360f90",
   "metadata": {},
   "source": [
    "E(S) = Σ^c i=1 - Pi log2 Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055e9f1-0d93-43c0-bc5a-2fe4aa99c9f9",
   "metadata": {},
   "source": [
    "The formula above is the negative sum of log probabilities of an event\n",
    "happening. Remember that probability indicates the confidence we have in\n",
    "an event occurring, therefore entropy is how surprising it would be, for a\n",
    "sequence of events to occur together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c152e5b-f37e-4e1c-87f9-9fd940fed476",
   "metadata": {},
   "source": [
    "In machine learning as we would see later with decision trees, s, the entropy\n",
    "of two or more attributes of a classifier is defined by\n",
    "\n",
    "\n",
    "E(T,X) = Σ cEx P (c) E (c)\n",
    "\n",
    "\n",
    "Decision trees are a machine learning algorithm that rely heavily on the\n",
    "entropy of an attribute and the information gain to determine how to\n",
    "classify samples in a classification problem. Let us look at decision trees in\n",
    "depth in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf229f5-cdd4-4fed-a7b2-5c34ee0db95c",
   "metadata": {},
   "source": [
    "#### Creating a Decision Tree\n",
    "\n",
    "\n",
    "A decision tree is a machine learning algorithm which is mainly used for\n",
    "classification that constructs a tree of possibilities where the branches in the\n",
    "tree represents decisions and the leaves represents label classification. The\n",
    "purpose of a decision tree is to create a structure where samples in each\n",
    "branch are homogenous or of the same type. It does this by splitting\n",
    "samples in the training data according to specific attributes that increase\n",
    "homogeneity in branches. These attributes form the decision node along\n",
    "which samples are separated. The process continues until all sample are\n",
    "correctly predicted as represented by the leaves of the tree.\n",
    "To explain the concept of a decision tree further, let us look at a toy\n",
    "example below that demonstrates its capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11662fde-280e-403b-9018-934b925efac6",
   "metadata": {},
   "source": [
    "Let us assume that we are a laptop manufacturer and we want to predict\n",
    "which customers from an online store are likely to buy our new top of the\n",
    "range laptop, so that we can focus our marketing efforts accordingly. This\n",
    "problem can be modelled using a decision tree with two classes (yes or no),\n",
    "for whether a person is likely to purchase or not.\n",
    "\n",
    "\n",
    "At the root of the tree, we want to choose an attribute about customers that\n",
    "reduces entropy the most. As we saw in the last section, by reducing the\n",
    "entropy, we increase the amount of knowledge that is contained in the\n",
    "system. We choose the appropriate attribute by calculating the entropy of\n",
    "each branch and the entropy of the targets (yes or no). The information gain\n",
    "is closely related to the entropy and is defined as the difference in entropy\n",
    "of the targets (final entropy) and the entropy given a particular attribute was\n",
    "chosen as the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff674c-9ce4-47a1-9ef8-20125e059ed4",
   "metadata": {},
   "source": [
    "Gain(T,X) = Entropy(T) - Entropy (T,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28816798-051f-4560-987f-b163aee76853",
   "metadata": {},
   "source": [
    "The formula above is used to calculate the decrease in entropy. The attribute\n",
    "with the largest information gain or decrease in entropy is chosen as the\n",
    "root node. This means that the attribute reduces the decision space the most\n",
    "when compared to other attributes. The process is repeated to find other\n",
    "decision nodes via attributes until all samples are correctly classified\n",
    "through the leaves of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6befc-4dfd-4a2b-b626-7fd2151ceebe",
   "metadata": {},
   "source": [
    "In the example above, age is the attribute that offers the most information\n",
    "gain so samples are split on that decision node. If the customer is middle\n",
    "aged, then they are likely to purchase a new laptop as they are probably\n",
    "working and have higher spending power. If the customer is a youth this\n",
    "brings us to another decision node. The attribute used is whether the youth\n",
    "is a student or not. If the youth is a student, they are likely to buy else they\n",
    "are not. That brings us to the leaves (classes) of the node following the\n",
    "youth branch of the tree. For the senior branch, we again split samples on\n",
    "an informative attribute, in this case credit rating. If the senior has an\n",
    "excellent credit rating that means they are likely to buy, else the leaf or\n",
    "classification for that sample along this branch of the tree is no.\n",
    "\n",
    "\n",
    "Let us now work on an example using Python, Scikit-Learn and decision\n",
    "trees. We would tackle a multi-class classification problem where the the\n",
    "challenge is to classify wine into three types using features such as alcohol,\n",
    "color intensity, hue etc. The data we would use comes from the wine\n",
    "recognition dataset by UC Irvine. It can be downloaded at\n",
    "https://gist.github.com/tijptjik/9408623/archive/b237fa5848349a14a14e5d4\n",
    "107dc7897c21951f5.zip\n",
    "First, lets load the dataset and use Pandas head method to have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69ab373-45fe-4861-bd06-715db340b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# comment the magic command below if not running in Jupyter notebook\n",
    "%matplotlib inline\n",
    "#dataset = pd.read_csv('wine.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3770ef-0752-4e9d-9d95-cad607059092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\winequality-red (2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5925d0-6741-4821-9291-ae4292573291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n",
       "0   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n",
       "1   7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5                                                                                                                     \n",
       "2  7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...                                                                                                                     \n",
       "3  11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...                                                                                                                     \n",
       "4   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33b1af-0386-4b45-91a0-732a560aaf39",
   "metadata": {},
   "source": [
    "There are 13 predictors and the first column “wine” contains the targets.\n",
    "The next thing we do is split the dataset into predictors and targets,\n",
    "sometimes referred to as features and labels respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1c668c-f1fa-4406-b8eb-6226c8240f27",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Wine'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12096/580916958.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Wine'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Wine'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4905\u001b[0m         \"\"\"\n\u001b[1;32m-> 4906\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4150\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4185\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6017\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6019\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Wine'] not found in axis\""
     ]
    }
   ],
   "source": [
    "features = dataset.drop(['Wine'], axis=1)\n",
    "labels = dataset['Wine']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a4067-3891-49a9-b4e5-067a3de22c8e",
   "metadata": {},
   "source": [
    "As is the custom to ensure good evaluation of our model, we divide the\n",
    "dataset into a train and test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1858fb-5578-4a70-852a-54ceaa5e6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels,\n",
    "test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157dc527-b854-4880-9537-6e4f8fb4f255",
   "metadata": {},
   "source": [
    "We can now evaluate the trained model on the test set and print out the\n",
    "accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad964b2-288d-4ecc-893d-34c3b5732199",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930bb9d-22a9-4c82-b599-ba3bd3f58d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(labels_test, pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfb3e1-1407-4eca-bc45-4e4d9392ffa0",
   "metadata": {},
   "source": [
    "We achieve an accuracy of 0.91 which is very impressive. It means that\n",
    "91% of samples in our test set were correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5de328-e186-4e70-9003-5d9b18d7a0b8",
   "metadata": {},
   "source": [
    "#### Decision Tree: Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce7715-d6ff-416c-97eb-f40f5cc010b3",
   "metadata": {},
   "source": [
    "1. When we remove sub-nodes of a decision node,You can say opposite process of splitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d20dc-c228-4f72-a4c1-08c894d72d96",
   "metadata": {},
   "source": [
    "Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f2b0d9-5f38-49a3-801b-f33f45119d63",
   "metadata": {},
   "source": [
    "2. It is a process of dividing a node into two or more sub-nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0375095-956e-42e4-8b60-38af1de33704",
   "metadata": {},
   "source": [
    "3. When a sub-node splits into further sub-nodes, then it is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a6727-1319-4f4f-bd42-d414666f74f0",
   "metadata": {},
   "source": [
    "Decision Node: When a sub-node splits into further sub-nodes, then it is called the decision node. Leaf / Terminal Node: Nodes do not split is called Leaf or Terminal node. Pruning: When we remove sub-nodes of a decision node, this process is called pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b502b1-38b0-4f75-bb6d-f57f96f55166",
   "metadata": {},
   "source": [
    "4. Nodes that can’t be splitted are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb8a9a-33e8-45eb-bde6-2d650d261061",
   "metadata": {},
   "source": [
    "Leaf / Terminal Node: Nodes do not split is called Leaf or Terminal node. Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d40c35-6f86-4204-9e6b-38e2fcba8548",
   "metadata": {},
   "source": [
    "5. Decision Tree Algorithm can be used for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ad9de-69b7-4d33-a01f-bbe2bc1b4e67",
   "metadata": {},
   "source": [
    "Decision trees are extremely useful for data analytics and machine learning because they break down complex data into more manageable parts. They're often used in these fields for prediction analysis, data classification, and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc28031-2fc7-4662-b10a-eae71c03dcbf",
   "metadata": {},
   "source": [
    "### Week 11 : Day 3 - Random  Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8942336-c950-4e3d-8e55-f1365c4e6b92",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d5b0a-d4cb-4b24-80cf-81d6b2c1900f",
   "metadata": {},
   "source": [
    "Random Forest is a supervised machine learning algorithmn\n",
    "Random forest Is a method that operates by constructing multiple decision trees.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "No over fitting \n",
    "\n",
    "High accuracy\n",
    "\n",
    "it estimates missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e1ef7-d289-407c-a584-aea2d8982d49",
   "metadata": {},
   "source": [
    "Random forest Is a method that operates by constructing multiple decision trees.\n",
    "\n",
    "The decision tree determines the course.\n",
    "\n",
    "It creates images\n",
    "it uses the majority decision\n",
    "Example is google map\n",
    "\n",
    "Random forest is a build up of decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace315b-9570-4aba-bf4d-bd8bb87957e6",
   "metadata": {},
   "source": [
    "#### Application of random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284ea37-d221-4be9-b780-33fe33055263",
   "metadata": {},
   "source": [
    "+  Remote sensing\n",
    "\n",
    "It is used in Remote sensing, Used in ETM devices to acquire images of the earth's surface.\n",
    "Accuracy is higher and training time is less.\n",
    "\n",
    "\n",
    "+  Object Detection\n",
    "\n",
    "Multiclass object detection is done using Random Forest algorithms.\n",
    "It provides better detection in complicated environments \n",
    "\n",
    "+ Kinect\n",
    "\n",
    "Random forest is used in a game console called Kinect.\n",
    "Tracks body movements and recreates it in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989aaf8-d0b5-4d15-991c-7bfff932e327",
   "metadata": {},
   "source": [
    "#### What is in it for you ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582c53f-e20b-477e-bbb4-db12e98c4f6f",
   "metadata": {},
   "source": [
    ">  What is machine learning >\n",
    "\n",
    "> Applications of Random forest\n",
    "\n",
    "> What is Classification?\n",
    "\n",
    "> Why Random Forest?\n",
    "\n",
    "> What is Random forest?\n",
    "\n",
    "> Random forest and Decision Tree\n",
    "\n",
    "> Use case - Iris Flower Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ddb97-9193-43bc-a98d-93426a5b1050",
   "metadata": {},
   "source": [
    "#### What is machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f89b03-a86d-427f-a968-e4f15618c98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77eb4db7-e376-462e-951d-e39be185667d",
   "metadata": {},
   "source": [
    "#### Types of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44955c-adae-4554-9233-9ff414bfd6c2",
   "metadata": {},
   "source": [
    "+ Surpervised learning : \n",
    "\n",
    "Where you have lots of data and you are able to train your models.\n",
    "\n",
    "+ Unsurpervised learning :\n",
    "\n",
    "you have to look at the data and divide it based on its own algorithm without having any training.\n",
    "\n",
    "+ Reinforcement learning:\n",
    "\n",
    "Where you have a plus or negative whether you have the answer correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6fd8d-7d4c-419f-bfdf-bebd20ac9da9",
   "metadata": {},
   "source": [
    "Random forest falls into surpervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f96cd-1dc6-4573-83e9-cec279e8fb96",
   "metadata": {},
   "source": [
    "+ Supervised learning falls into two groups, classification and regression\n",
    "\n",
    "\n",
    "random forest falls under classification\n",
    "\n",
    "Classification is a kind of problem where the output are categorical in nature like 'Yes' or 'No','True' or 'false' 'o' or 'i'\n",
    "\n",
    "Under classification we have \n",
    "\n",
    "KNN\n",
    "\n",
    "Naive Bayes\n",
    "\n",
    "Decision tree  which is the part of the random forest we are stuying today.\n",
    "\n",
    "Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a03ed5-2087-48da-9677-c26af6976ca3",
   "metadata": {},
   "source": [
    "#### Why Random Forest ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e5179-dabb-4cf4-933a-0b2099a73973",
   "metadata": {},
   "source": [
    "+ No overfitting\n",
    "\n",
    "Use of multiple trees reduces the risk of overfitting.\n",
    "Overfitting means we have fit the data so close that what we have as our sample that we pick up on all the weird part and instead of predicting the overall data you are predicting the weird stuff you dont want.\n",
    "\n",
    "+ Training time is less\n",
    "\n",
    "+ High accuracy :\n",
    "\n",
    "Runs efficiently on large database.\n",
    "For large data ,it produces highly accurate predictions.\n",
    "In today's world of big data, this is really important.This is where random forest really comes in\n",
    "\n",
    "it estimates missing data.\n",
    "Data in today's world is very messy,if you have a random forest , you can maintain the accuracy when a large proportion of the data is missing.\n",
    "\n",
    "For example, if you have data that comes into five or six different areas, maybe they took one kind of statictics in one area and another type of statistics that is slightly different in another area,but they have the same shape and shared data but one is missing, like the number of children in the house when you are doing something on the demoraphics and the other one is missing the size of the hoouse\n",
    "\n",
    "It will look at both of them seperately  and build two different trees and then do a very good job of guessing which one fits better even though it is missing in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738550da-b30f-4088-a50b-e46a4a977285",
   "metadata": {},
   "source": [
    "#### What is Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720b8b6-5fb0-43b1-bc9a-8d245a726d00",
   "metadata": {},
   "source": [
    "+ Random forest or Random Decision Forest is a method thst operates by constructing multiple decision trees during training phase.\n",
    "\n",
    "The Decision of the majority of the trees is chosen by the random forest as the final decision."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88573da3-b6ec-4215-9a41-97b8361fb6d8",
   "metadata": {},
   "source": [
    "Example :\n",
    "\n",
    "We have\n",
    "\n",
    "Decision    output 1  apple \\>\n",
    "Tree 1\n",
    "\n",
    "Decision   Output 2   lemon \\> Random\n",
    "Tree 2                           Forest\n",
    "\n",
    "Decision   output 3     Apple \\>\n",
    "Tree 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13ff4f-aead-4927-9c83-eb5ffa6b5cff",
   "metadata": {},
   "source": [
    "we have decision  tree 1,2,3 and all these is what forms random forest.\n",
    "the randon forest will now say i have two vote for apple and one vote for lemon, the the majority is apples so the final decision is apple.\n",
    "\n",
    "To undersatnd how random forest works , we need to dig a little dipper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854e243-18a8-424c-b389-fe1067a93a88",
   "metadata": {},
   "source": [
    "#### Random Forest and Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692412b-a41f-498c-bb21-c212356df8e8",
   "metadata": {},
   "source": [
    "+ A Decision Tree is a tree shaded diagram used to determine a cause of action.Each branch of the tree represents a possible decision, occurance or reaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113b0a8-9ecd-41aa-972a-2c24d348834c",
   "metadata": {},
   "source": [
    "For e,g we have a bowl of fruits , the first question is  \n",
    "   Is the diameter >=3\n",
    "   /                \\\n",
    "   False             True\n",
    "                         \\\n",
    "                  Is color orange\n",
    "                  /           \\ \n",
    "                  False        True\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd462c-07b8-4635-ab4e-cf7121002bbd",
   "metadata": {},
   "source": [
    "#### Decision tree - Important Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560543a-3aa8-4d70-9904-e388e372ac10",
   "metadata": {},
   "source": [
    "A decision Tree is a very important term to note.\n",
    "\n",
    "+ Entropy :\n",
    "\n",
    "Entropy is the measure of randomness or unpredictability in the dataset.\n",
    "\n",
    "\n",
    "+ Information gain \n",
    "It is a measure of decrease in the entropy after the dataset is split. ( Decision E1 - E2)\n",
    "\n",
    "+ Leaf Node : \n",
    "\n",
    "The leaf Node carries the classification or the decision ie its going to be an apple on the leaf node  (set 1)or a lemon on the leaf node(set2). They are the final decisions or classifications\n",
    "\n",
    "+ Decision Node:\n",
    "\n",
    "A Decision Node has two or more branches>\n",
    "\n",
    "For example we have 5 apples and one lemon in set 1 and 5 lemon and 1 apple in set 2.\n",
    "You have to make a decision on which tree it goes down to based on some kind of measurement or information given to the tree.\n",
    "\n",
    "+ Root Node\n",
    "\n",
    "The top most Decisoin node is known as the Root node. This is where you have all your data and the first decisio  you have to make. the first split and the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e319eb7-d655-4dd9-8b9c-1befe76648eb",
   "metadata": {},
   "source": [
    "With Entropy we have a high amount of randomness< it means if you are going to guess based on this data ,it will not be able to tell you whether is a lemon or an apple, it will just say is a fruit.\n",
    "\n",
    "So what we need to do is to split the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01016ec0-ce83-4d08-afe5-bb3570e583b4",
   "metadata": {},
   "source": [
    "E1  Bowl of fruits (Initial Dataset)  High\n",
    "          /               \\        Entropy\n",
    "        \n",
    "            Decision split       ( Afetr)\n",
    "                                                                          (splitting)\n",
    "        \n",
    "        \n",
    " E2   Set 1       E2    Set 2  \n",
    "                                  Lower\n",
    "      /\\                  /\\      entropy\n",
    "      \n",
    "apple(5) lemon(1)    apple(1) lemon(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61976d-6d70-4078-9c12-dc1885530404",
   "metadata": {},
   "source": [
    "#### How does a Decision Tree work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7dc66-0f40-4b33-b2cf-66b51024fd98",
   "metadata": {},
   "source": [
    "We  have a bowl of fruit,\n",
    "We create a problem statement statement:\n",
    "\n",
    "+ To classify the different types of fruits in the bowl based on different features.\n",
    "\n",
    "+ The dataset in the bowl is looking quite messy and the entropy is high in this case. If the bowl is our decision maker , you will will not know the one to pick, grape, apple or lemon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1090a28-c1da-452b-baad-69c0cc584aab",
   "metadata": {},
   "source": [
    "+ Training Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eed4d9d0-40a7-4410-9dba-61fbfccf3808",
   "metadata": {},
   "source": [
    "Color        Diameter     label\n",
    "\n",
    "Red            3           Apple\n",
    "\n",
    "Yellow          3           Lemon\n",
    "\n",
    "Purple          1           Grapes\n",
    "\n",
    "Red              3          Apple\n",
    "\n",
    "Yellow           3         lemon\n",
    "\n",
    "Purple           1         Grape      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef997e6-edf0-4800-a4b2-9fed160588f0",
   "metadata": {},
   "source": [
    "+ How to split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2860df-21c3-4ca9-957c-1c3fb1d280d1",
   "metadata": {},
   "source": [
    "We have to frame the data in such a way that information gain is the highest.\n",
    "It is very key to note that we are looking at the best gain. we dont want to just start sorting out the smallest piiece in there, We want to split it the biggest way we can.\n",
    "\n",
    "Note:\n",
    "Gain is the measure of decrease in Entopy after splitting.\n",
    "\n",
    "Now, we are trying to choose a condition that gives us the highest gain.\n",
    "We wil do that by splitting the data using each condition and checking the gain that we get out of them.\n",
    "\n",
    "The condition that gives us the highest gain will be used to make the first split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23647f9d-7798-4954-a289-d835e2b30ff7",
   "metadata": {},
   "source": [
    "+ Conditions\n",
    "\n",
    "Color == purple?\n",
    "\n",
    "diameter = 3  / ( Let's say this condition\n",
    "                 gives us maximum gain)\n",
    "\n",
    "Color == Yellow?\n",
    "\n",
    "Color == Red?\n",
    "\n",
    "Diameter=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1fb5a-37bf-4652-a10b-6bda1a8f02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    A bowl of fruit\n",
    "         Is Diameter >=3 ?\n",
    "        /              \\\n",
    "        false          True\n",
    "                Is color Yellow?\n",
    "                /          \\\n",
    "        False Capple)   True (lemon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f687f2f-b315-470a-83fc-fc4d5baecf9c",
   "metadata": {},
   "source": [
    "Our first split \n",
    "\n",
    "We split the data based on the duameter, is it greater than or equal to 3 ,if it is not, is false,it goes to the grape bowl and if it is true ,it goes into the bowl full of lemon amd apples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac84e3-b1dc-499a-bd5f-befb098779b5",
   "metadata": {},
   "source": [
    "The entropy after the split have decreased considerably, now we can make 2 decisions .\n",
    "This node has already attained an entropy value of zero as you can see . There is only one kind of label left for this branch ( The grape), so no further splitting is require for this node.\n",
    "\n",
    "Howevr, this node on the right containing apples and lemon still requires a split to decrease the entropy futher.\n",
    "\n",
    "So ,we split the right node further based on the color. If it is yellow it goes to the true side and if it is red ,it goes to the false side.\n",
    "\n",
    "So the entropy in this case is now zero.\n",
    "Because there is only one type of data in each od these bowl.\n",
    "So we can predict lemon with a 100% accuracy, we can preict apple with a 100% accuracy along with the grape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e11130-90ce-46aa-9c02-7b082e736c3d",
   "metadata": {},
   "source": [
    "+ How does a Random Forest work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68720342-5a2d-4a84-a5fc-5e1692efa1a8",
   "metadata": {},
   "source": [
    "Lets say we have already built three trees\n",
    "\n",
    "Let this be tree 1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b807374-4b3e-4004-9095-affd95b2a7f3",
   "metadata": {},
   "source": [
    "           Is Diameter >=3\n",
    "           /             \\\n",
    "        False            True\n",
    "                           |\n",
    "                        Is color Orange?\n",
    "                \n",
    "                        /          \\\n",
    "                    False         True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300e695-eff6-4231-a87f-39402e71a711",
   "metadata": {},
   "source": [
    "The above tree looks at the diameter if it is greater than or eqaul to 3 , is true otherwise is false,\n",
    "\n",
    "Second if the color is orange is True ,otherwise false"
   ]
  },
  {
   "cell_type": "raw",
   "id": "113548ae-7ee9-4cf3-99fd-b37610417b7d",
   "metadata": {},
   "source": [
    "Second Tree:\n",
    "    \n",
    "              Is color red?\n",
    "              /           \\\n",
    "            True          False\n",
    "            |\n",
    "    Is shape==Circle?\n",
    "    /             \\\n",
    "    False         True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd48572-e0f9-4195-9003-c81143442afd",
   "metadata": {},
   "source": [
    "Is color red , this is split differntly, brcause we have alot of red, we say is the color red, that will bring our entropy down, if is true , it goes to the right and if false it goes to the left.\n",
    "Thn we look at the shape , false and true and so on,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ac1c4-afe3-462d-b28d-59d02a65585f",
   "metadata": {},
   "source": [
    "Tree 3:\n",
    "    \n",
    "        Is Diameter=1\n",
    "        \n",
    "        /           \\\n",
    "        True         False\n",
    "        |             |\n",
    "                    Grows in summer?\n",
    "                     /          \\\n",
    "                False           True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f96160-f39a-410f-92d0-8d271897525d",
   "metadata": {},
   "source": [
    "Is diameter = 1 , we came up with this because there are alot of cherries in the bowl.and this will dropt the entropy the quickest.\n",
    "Another category, does it grow in the summer?\n",
    "If it is false it goes to the left and if true , to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d644a-9175-4867-867c-46bfe4e58515",
   "metadata": {},
   "source": [
    "We have 3 diffrent images categorizing a  fruit. Let's to classify this fruit,\n",
    "\n",
    "Let's look at a fruit to explain further,  in this fruit is blackened so you may not know exacly what it is. Randpm forest does well with missing image.\n",
    "\n",
    "Tree 1 classifies it as an orange\n",
    "\n",
    "Tree 2  classifiers it as a red\n",
    "\n",
    "Tree 3 as the qestion is the diameter =1 ,this is false\n",
    "\n",
    "Diameter = 3\n",
    "color = Orange\n",
    "Growing in summer = Yes\n",
    "Shape = Circle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac67993-58b8-4990-b15a-016e6fa8e1de",
   "metadata": {},
   "source": [
    "How does the ramdom forest work?\n",
    "\n",
    "The first 1 says it is an orange, the seconf says it is a cherry and the 3rd one says it is an orange. \n",
    "\n",
    "The majority of the vote says it isa an orange 2 against 1 so we go for the majority.\n",
    "\n",
    "So the fruit is classified as an orange althogh we didint know the colur because that was a missing data but we were able to find out the actual fruit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229dcd7e-661d-45f0-83dd-bd4f789a6fca",
   "metadata": {},
   "source": [
    "#### Use Case - Iris Flower Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd493a-00e6-4492-ba04-eee3bad4ed06",
   "metadata": {},
   "source": [
    "+ Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c170a-c877-4400-a254-9e10f5f36ecb",
   "metadata": {},
   "source": [
    "Wonder which species of iris do this flower belong to?\n",
    "\n",
    "Let's try to predict this species of the flowers using machine learning in python.\n",
    "\n",
    "Let's see how it can be done.\n",
    "\n",
    "You can also use pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e545fba-bc4c-426f-a756-3692d3f30324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the library with iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "# loading scikit's random forest classifier libary\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# loading pandas\n",
    "import pandas as pd\n",
    "# loading numpy\n",
    "import numpy as np\n",
    "# Setting random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "800f8d47-ab82-4c85-9b62-9c4a26b0e509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the data using the following codes\n",
    "# Creating an object called iris with the iris data\n",
    "iris = load_iris()\n",
    "# print(iris)\n",
    "# Creating a dataframe with the four feature variables\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "# Viewing the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb3c3423-cdb6-4843-bc8e-bb3f3924acf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(iris)\n",
    "\n",
    "# Adding a new column for the species name\n",
    "df['species'] = pd.Categorical.from_codes(iris.target,\n",
    "iris.target_names)\n",
    "# you can use species or target both means same thing\n",
    "# viewing the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37705c17-9ed1-4ef6-9ba4-01362939f310",
   "metadata": {},
   "source": [
    "#### Note the last column specie is what we are trying to predict or the independent variable.\n",
    "\n",
    "We are exploring data and one of the challanges is knowing how good your model is. Did your model work. To do this we need to train our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62351a4c-fffe-496b-99df-d0049825903d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "  species  is_train  \n",
       "0  setosa      True  \n",
       "1  setosa      True  \n",
       "2  setosa      True  \n",
       "3  setosa      True  \n",
       "4  setosa      True  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Test and Train Data\n",
    "df['is_train'] = np.random.uniform(0,1, len(df)) <= .75\n",
    "# we a generating a random number between 0 snd 1 and we will\n",
    "# do it for each of the rows ie the row df comes from.\n",
    "# If it is lesss than .75 ,then it's true but if it is more than\n",
    "# .75 is false\n",
    "# View the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8543865-a8e1-4d64-a1c1-cc866cd11565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation in the training data: 118\n",
      "Number of observation in the test data: 32\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframes with test rows and training rows\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "# If df = train is true ,it goes for training and if df=train is false ,\n",
    "# it goes to the test\n",
    "# Show the number of observations for the test and training dataframes\n",
    "print('Number of observation in the training data:', len(train))\n",
    "print('Number of observation in the test data:', len(test))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df959693-179e-4388-8902-a5d5ca6e2235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
       "       'petal width (cm)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of the feature column's names\n",
    "features = df.columns[:4]\n",
    "# The first 4 (:4)ie 0,1,2,3 will be the features\n",
    "# Viewing features or titles of those columns\n",
    "features\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429802fc-df4f-4c25-ae20-c4e2748d6a00",
   "metadata": {},
   "source": [
    "So when we run it we will have the above result, sepal length and width and peta length an width of the iris flower .\n",
    "\n",
    "Take note, when you are in a command line whether jupiter note book or terminal window, if you just put the name of it, it will show the result, this is same as doing print features.\n",
    "\n",
    "But if you are writing a code and saving its script and running it by remote, you need to put the print in there.\n",
    "\n",
    "We will now create label for the other part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec849c47-9d22-4c16-9484-c8d4fd65a675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting each species name into digits\n",
    "y = pd.factorize(train['species'])[0]\n",
    "# o is to convert to 0,s and 1,s which the computer understands\n",
    "# Specie is just column,factorize looks at the fact and we have just 3 of them\n",
    "# Viewing target \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49333615-f539-4a84-9001-20e8be5be26f",
   "metadata": {},
   "source": [
    "From the result, we can see that y generates an array that is equal to training set, of zeros, ones and twos representing the three different kinds of flowers we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0317e-0654-450d-9c12-85b6ad26052c",
   "metadata": {},
   "source": [
    "Next thing to do is to start predicting. \n",
    "We have two lines of code\n",
    "first is the randomforestclassifier, we will pass two variables in there n_jobs  is used to prioritize the code, if you are doing it on your computer you can say n_jobs =2 but if you are working in a larger or  abig data , you may need to prirotize it diffently. The work of the n_job is to help you change your priority and random state =0\n",
    "\n",
    "Then clf.fit(train[features] ,y ) this helps you to fit your data, train it and create your randomforest classifier, this is the code that does everything, we wil take our training set with the features, then our target the y , note the y is 9,1,2 that we just created and the features is the actual data going in that we put into the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94a99392-6561-49e1-912e-8a6210e5b925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=2, random_state=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a random forest Classifier.\n",
    "clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "#Training the Classifier\n",
    "clf.fit(train[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16f287-9db3-4227-a8ea-2baccaf53af1",
   "metadata": {},
   "source": [
    "Next,we need to test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fefd8d35-37ea-4cd4-b731-cd5ac4168136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the trained Classifier to the test\n",
    "clf.predict(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ae9b72a-40fc-48a4-b4e4-45d975c61bf1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "31                 5.4               3.4                1.5               0.4\n",
       "38                 4.4               3.0                1.3               0.2\n",
       "52                 6.9               3.1                4.9               1.5\n",
       "66                 5.6               3.0                4.5               1.5\n",
       "68                 6.2               2.2                4.5               1.5\n",
       "70                 5.9               3.2                4.8               1.8\n",
       "72                 6.3               2.5                4.9               1.5\n",
       "89                 5.5               2.5                4.0               1.3\n",
       "98                 5.1               2.5                3.0               1.1\n",
       "103                6.3               2.9                5.6               1.8\n",
       "109                7.2               3.6                6.1               2.5\n",
       "111                6.4               2.7                5.3               1.9\n",
       "114                5.8               2.8                5.1               2.4\n",
       "116                6.5               3.0                5.5               1.8\n",
       "118                7.7               2.6                6.9               2.3\n",
       "122                7.7               2.8                6.7               2.0\n",
       "140                6.7               3.1                5.6               2.4\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "147                6.5               3.0                5.2               2.0\n",
       "149                5.9               3.0                5.1               1.8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also do test featuers  ,remember feaures is in an array\n",
    "# It puts in all the different columns we loaded in the features\n",
    "test[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4bb31-0089-4bb2-9906-3e56d551b3c5",
   "metadata": {},
   "source": [
    "We will be viewing our test fatures differrently with the code below, notice we have proba ie probabibilyt, this generates 3 numbers ie 4 leaf nodes,\n",
    "\n",
    "If you remember from all the theorem we did , thi is our predictors, the first one is predicting  a 1 for Setosa , a 2 for verginica and a 0 for versicoloretc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e17ba8b4-aa7b-4ac9-a5bd-3bd2bbb04847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.95, 0.05, 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.99, 0.01, 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the predicted probabilities of the first 10 observations\n",
    "clf.predict_proba(test[features])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b2cd6-6d78-40b0-b61a-24a66ac37551",
   "metadata": {},
   "source": [
    "Lets change it a little using 10:20 ie 10 to 20, from the result, we can see in a line 0, 0.5, 0.5, so if we are to vote, it means setosa gets 0 votes , verginica gets 0.5 votes and  Versicolor gets 0.5 votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d209eba-bd11-48e8-824f-8ab7f2d43acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  ],\n",
       "       [0.99, 0.01, 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.67, 0.33],\n",
       "       [0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.82, 0.18],\n",
       "       [0.  , 0.03, 0.97],\n",
       "       [0.  , 0.42, 0.58],\n",
       "       [0.  , 0.99, 0.01],\n",
       "       [0.  , 0.96, 0.04]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the predicted probabilities of the first 10 observations\n",
    "clf.predict_proba(test[features])[10:20]\n",
    "#clf.predict_proba(test[features])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde96d41-236c-4de4-af6b-70387dc2e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "We wiil then set up our predictors and map them out to the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5e094e6-2ede-4866-b2ae-8190b5e3db2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa'], dtype='<U10')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping names for the plants for each predicted plant class\n",
    "# we will run our predict test features again\n",
    "preds = iris.target_names[clf.predict(test[features])]\n",
    "# Viewing the PREDICTED species for the first five observations [0:5]\n",
    "preds[0:5]\n",
    "# preds[0:25] # to view it more \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "363462c0-2830-4464-8cb4-5584a200d6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     setosa\n",
       "8     setosa\n",
       "10    setosa\n",
       "13    setosa\n",
       "17    setosa\n",
       "Name: species, dtype: category\n",
       "Categories (3, object): ['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the ACTUAL species for the first five observations\n",
    "test['species'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6e959-24a2-4dc6-b4ef-813d8900bbdd",
   "metadata": {},
   "source": [
    "From the result, the first one is what our forest is doing , the secon one is the actual data, so we need to combine this so as to understand what that means, we need to know how good our data is ,how good it is in predicting the feature.\n",
    "This takes us to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4e012e90-2291-415f-8fe8-c8786f67a601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Species</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Species</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Species  setosa  versicolor  virginica\n",
       "Actual Species                                  \n",
       "setosa                 13           0          0\n",
       "versicolor              0           5          2\n",
       "virginica               0           0         12"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating confusuion matrix\n",
    "pd.crosstab(test['species'], preds, rownames=['Actual Species'], colnames=['Predicted Species'])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e888bf-fb6e-48af-96b8-6c7b3bf920ce",
   "metadata": {},
   "source": [
    "We create a function in pandas using the Crosstab function, the crosstab  takes two sets of data and creates a chart out of them.\n",
    "\n",
    "We have the versicolor, setosa and verginica.\n",
    "the way to read this, you consider where the point where the 3 flowers meet, from the result we have setosa 13, versicolor 5  and virginica 12 \n",
    "when you add it up,\n",
    "The number of accurate predictions = 30Also observe, versicolor have 2 in accurate preiction.\n",
    "\n",
    "Model accuracy\n",
    "\n",
    "30 / 92 X 100 = 93\n",
    "\n",
    "We can say that the model accuracy is 93\n",
    "\n",
    "So the model accuracy is 93%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5fb559d9-e4c0-4b34-8135-bc6bdadcbee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa'], dtype='<U10')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets deploy our clf.predict to preict 2 flowers\n",
    "preds = iris.target_names[clf.predict( [[5.0, 3.6, 1.4,2.0],[5.0, 3.6, 1.4, 2.0]] )]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e984f-b673-4fb1-89aa-d98faff541b1",
   "metadata": {},
   "source": [
    "The cross tab helps to create confususion matrix or correlation for our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ded38-7e54-4e81-82a6-35ed03bdd689",
   "metadata": {},
   "source": [
    "Feature scaling is a way of bring two different data to a comparable state. E.g Age and salary. eg you want to determine if a student should get a loan or not. the dat you have is age and monthly alolwance.\n",
    "\n",
    "age 17, 50,000 you pass in your data so that two of them can be in a comparable state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da57e0-a54b-4994-be55-16f1aca45c96",
   "metadata": {},
   "source": [
    "+ Confusion Matrix is a useful machine learning method which allows you to measure Recall, Precision, Accuracy, and AUC-ROC curve. Below given is an example to know the terms True Positive, True Negative, False Negative, and True Negative. True Positive: You projected positive and its turn out to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2b279-584f-47e2-9e09-80ff894c02fe",
   "metadata": {},
   "source": [
    "### Random Forest Explained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560e16c-7f64-4c85-ae0a-4c349a2019b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8974a5-0aa7-4f15-a10b-ffbdb7b71043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa4d47-2739-4fce-bbcd-1675ccbbae89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df3274fc-0b0a-4d57-9d72-228673586506",
   "metadata": {},
   "source": [
    "##### Random Forest: Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c319-8f8f-4d00-af2c-a6ea90d91f5d",
   "metadata": {},
   "source": [
    "1. What are random forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7e1e6-8fd9-48c2-ad87-51357afe1929",
   "metadata": {},
   "source": [
    "An ensemble learning method for classification, regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051db8c9-5cdd-4d91-add5-a0079d445a5b",
   "metadata": {},
   "source": [
    "Random Forest is a powerful and versatile supervised machine learning algorithm that grows and combines multiple decision trees to create a “forest.” It can be used for both classification and regression problems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b152fa-0712-478d-a017-a9e5a5f9bea2",
   "metadata": {},
   "source": [
    "2. How does random forest calculate probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb0969-c074-47c9-ab14-d64ad3e1543b",
   "metadata": {},
   "source": [
    "In Random Forest package by passing parameter “type = prob” then instead of giving us the predicted class of the data point we get the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7220d8-27f9-4dde-892a-6699e5fc406e",
   "metadata": {},
   "source": [
    "A random forest is a popular tool for estimating probabilities in machine learning classification tasks. However, the means by which this is accomplished is unprincipled: one simply counts the fraction of trees in a forest that vote for a certain class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627848c-0513-4073-bc7c-1c4fab1cfdf8",
   "metadata": {},
   "source": [
    "3. How do I stop Overfitting random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b618b-a0fd-46de-b765-1c344ce4d101",
   "metadata": {},
   "source": [
    "optimize a tuning parameter that governs the number of features that are randomly chosen to grow each tree from the bootstrapped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76905882-f8d1-42c9-9817-55bc8c8bde08",
   "metadata": {},
   "source": [
    "To avoid over-fitting in random forest, the main thing you need to do is optimize a tuning parameter that governs the number of features that are randomly chosen to grow each tree from the bootstrapped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8c1c4-9dc4-4f15-afe7-82a04e7428a3",
   "metadata": {},
   "source": [
    "4. How many decision trees are there in a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8123dc-ae3c-45fd-9b66-6bf3c3dbe811",
   "metadata": {},
   "source": [
    "They suggest that a random forest should have a number of trees between 64 - 128 trees. With that, you should have a good balance between ROC AUC and processing time.\n",
    "\n",
    "There are two main types of decision trees that are based on the target variable, i.e., categorical variable decision trees and continuous variable decision tree\n",
    "\n",
    "\n",
    "A random forest is simply a collection of decision trees whose results are aggregated into one final result. Their ability to limit overfitting without substantially increasing error due to bias is why they are such powerful models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c80bf-6ce8-4949-b938-bdf7dd8ab42e",
   "metadata": {},
   "source": [
    "5. Random forest is supervised or unsupervised ________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb40712-5fbe-4cd8-b8ef-0746e54ea072",
   "metadata": {},
   "source": [
    "A Random Forest Algorithm is a supervised machine learning algorithm which is extremely popular and is used for Classification and Regression problems in Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241f98b-05d5-43d6-a1b7-2dcb6fc62c80",
   "metadata": {},
   "source": [
    "### Week 11 Day 4 : Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64504639-426c-4387-a507-d00ac88f7469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524252a-c6f2-4360-a4fb-59c26928e388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bef5b9a4-f208-45c8-9cfb-441ba8e0935c",
   "metadata": {},
   "source": [
    "### Boosting Machine Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8326724-048f-467f-ad41-56f9b34862f4",
   "metadata": {},
   "source": [
    "Boosting technique helps you to increase the accuracy of your model.\n",
    "\n",
    "helps to filter data and make accurate model.\n",
    "\n",
    "emsable method are bosting and bagging\n",
    "\n",
    "boosting training samples are taken randomly\n",
    "\n",
    "bagging sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0301b-82b8-4c48-a385-2dcd4d046c1e",
   "metadata": {},
   "source": [
    "Random Forest are, as their name suggests, a\n",
    "group of individual Decision trees that make up a\n",
    "forest. These individual trees are quite prone to overfitting.\n",
    "\n",
    "\n",
    "With boosting, this works in the same way, but while\n",
    "in bagging each model is trained independently, in\n",
    "boosting the N models are trained sequentially, taking into\n",
    "account the success of the previous model and increasing\n",
    "the weights of the data that this previous model has had the\n",
    "highest error on, which makes the subsequent models\n",
    "focus on the most difficult data observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c7a72-d9a8-4c63-a011-ec7930ecf732",
   "metadata": {},
   "source": [
    "How boosting is done.\n",
    "\n",
    "1. All the data samples start with the same\n",
    "weights. These samples are used to train an\n",
    "individual model (a Decision Tree lets say)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a1de3-3b2b-4d6a-8cd9-13cf9af01e29",
   "metadata": {},
   "source": [
    "How does a Boosting Model Make\n",
    "Predictions?\n",
    "They way a boosting model makes predictions on new data\n",
    "is very simple. When we get a new observation with its\n",
    "features, it is passed through every one of the individual\n",
    "models, having each model make its own prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5e4ca-c618-4c47-84ba-3339f68f24a6",
   "metadata": {},
   "source": [
    "Then, taking into account the weight of each one of these\n",
    "models, all these predictions are scaled and\n",
    "combined, and a final global prediction is give To end, let's explore the characteristics of the most\n",
    "common Boosting models out there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a492d9-3a1f-4d4b-8b46-36080bcb1260",
   "metadata": {},
   "source": [
    "ADABOOst\n",
    "\n",
    "Short for Adaptive Boosting, AdaBoost works by the\n",
    "exact process described before of training sequentially,\n",
    "predicting, and updating the weights of the miss-classified\n",
    "samples and of the corresponding weak models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b2eb7-6099-460d-8ee3-d7a534b497ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient boosting machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54593f71-484e-4fc8-8f5f-7e0930ee0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "Very similar to AdaBoost, Gradient Boosting\n",
    "Machines train weak learners sequentially, adding more\n",
    "and more estimators, but instead of adapting the weights of\n",
    "the data, it tries to predict the residual errors made\n",
    "by the previous estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26155ac3-2629-4e8f-a253-51ab4928e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e60a971-90ee-40b2-9b1c-3bf09b390d76",
   "metadata": {},
   "source": [
    "Short for eXtreme gradient boosting, like in Gradient\n",
    "boosting, we fit our trees to the residuals of the previous\n",
    "trees predictions, however, instead of using conventional,\n",
    "fixed size decision trees, XGBoost uses a different kind of\n",
    "trees: XGBoost trees we could call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81175027-70f5-42d2-9308-445c18e510fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "It builds these trees by calculating similarity scores\n",
    "between the observations that end up in a leaf node. Also,\n",
    "XGBoost allows for regularisation, reducing the possible\n",
    "overfitting of our individual trees and therefore of the\n",
    "overall ensemble model.\n",
    "Lastly, XGBoost is optimised to push the limit of the\n",
    "computational resources of boosted tree algorithms,\n",
    "making it a very high performance and fast algorithm\n",
    "in terms of time and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf09c6-1ef1-401b-8241-bac1db645ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e6276-ea58-486a-bd96-e6e998e38374",
   "metadata": {},
   "outputs": [],
   "source": [
    "Light Gradient Boosting Machines, known by the\n",
    "short name of LightGBM, are yet another turnaround of\n",
    "improvements for Gradient Boosting algorithms. Instead of\n",
    "using a level-wise growing strategy for the decision trees\n",
    "like in XGBoost, it uses a leaf-wise growth strategy,\n",
    "giving it the chance to achieve a higher error reduction per\n",
    "jump than other tree based algorithms. Also, compared to\n",
    "XGBoost, LightGBM is generally faster, especially on large\n",
    "data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3152e-93a4-47d9-8c6e-45039b9fe3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion\n",
    "That is it! As always, I hope you enjoyed the post, and that I\n",
    "managed to help you understand what boosting is, how it works,\n",
    "and why it is so powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19558f-5f43-4725-9406-2325be4b5106",
   "metadata": {},
   "source": [
    "### Boosting Machine Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73460e55-f0c2-4704-8e75-a40b123ea7de",
   "metadata": {},
   "source": [
    "Boosting machine learning is used for more advanced acquired tecniques that combines weak learners to increase the efficiency of your model.\n",
    "\n",
    "it combines the output we got from weak learners to get a strong model\n",
    "\n",
    "in boosting you pay more attention to all your misclassified  .\n",
    "\n",
    "The basic principle is to generate multiple waek leaners and conbine their predictions to form one strong rule\n",
    "\n",
    "weak learners are outcomes that are giving you errors. Adding more weight is what we call boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e4404-b679-4e37-8950-1fa7edec77bf",
   "metadata": {},
   "source": [
    "Ensamble learning means combination of several models together to get a good outcome e.g boosting a nd bagging a re types of emsamble model\n",
    "\n",
    "we use ensamble model cos differnt model have their own strength andweakness and we use ensamble model to get the best result  for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde6110-e388-4425-ace9-d8ddb7ad2cd9",
   "metadata": {},
   "source": [
    "esamble learner is boosting is used in Sequential.\n",
    "\n",
    "Bagging is parallel eg.randomized forest\n",
    "\n",
    "Boosting is not a model,it is just a boost used in your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6f9b4-6924-42d9-81b4-87d252cee938",
   "metadata": {},
   "source": [
    "#### Bagging is a method of merging the same type of predictions. Boosting is a method of merging different types of predictions. Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995090c3-8c16-4653-bd4b-3d5a9c21345c",
   "metadata": {},
   "source": [
    "Bagging eg is random forest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510aca6-f0c1-4a6e-85da-acba5bdc9385",
   "metadata": {},
   "source": [
    "In Bagging, each model receives an equal weight. In Boosting, models are weighed based on their performance.\n",
    "\n",
    "Models are built independently in Bagging. New models are affected by a previously built model’s performance in Boosting.\n",
    "\n",
    "In Bagging, training data subsets are drawn randomly with a replacement for the training dataset. In Boosting, every new subset comprises the elements that were misclassified by previous models.\n",
    "\n",
    "Bagging is usually applied where the classifier is unstable and has a high variance. Boosting is usually applied where the classifier is stable and simple and has high bias.\n",
    "\n",
    "\n",
    "Popular Machine Learning and Artificial Intelligence Blogs\n",
    "IoT: History, Present & Future\tMachine Learning Tutorial: Learn ML\tWhat is Algorithm? Simple & Easy\n",
    "Robotics Engineer Salary in India : All Roles\tA Day in the Life of a Machine Learning Engineer: What do they do?\tWhat is IoT (Internet of Things)\n",
    "Permutation vs Combination: Difference between Permutation and Combination\tTop 7 Trends in Artificial Intelligence & Machine Learning\tMachine Learning with R: Everything You Need to Know\n",
    "How does Bagging and Boosting obtain N learners?\n",
    "\n",
    "Bagging and Boosting obtain N learners by creating additional data in the training stage. The random sampling and substitution from the original set produce N new training data sets. The replacement and sampling mean that certain observations may be iterated in every new training data set.\n",
    "\n",
    "Any element has the same probability to exist in a new data set in bagging in ML. The observations are weighted in Boosting. Thus, some of them will frequently take part in the new sets.\n",
    "\n",
    " Which one to use -Bagging or Boosting?\n",
    "\n",
    "Both of them are useful for data science enthusiasts to solve any classification problem. The choice among these two depends on the data, the circumstances, and the simulation. Moreover, the choice of the ensemble technique is simplified as you gain more experience working with them.\n",
    "\n",
    "Boosting and Bagging techniques reduce the variance of your single estimate. This is because they merge several estimates from various models. Hence, the result might show a model with improved stability.\n",
    "\n",
    "Bagging is preferable when the classifier is not robust and shows high variance. You can understand bagging is the example of which type of learning when you start implementing it. But if the classifier features a high bias, then Boosting will provide the desired results.\n",
    "\n",
    "The bagging in ML will seldom provide a better bias if using a single model shows a low performance. On the other hand, Boosting can create a combined model with a lower error rate because it corrects the weights of incorrectly predicted data points.\n",
    "\n",
    "\n",
    "Bagging must be considered if a single model’s downfall overfits the training data. The reason is boosting doesn’t prevent overfitting data. Therefore, Bagging is more effective and the preferred choice for most data scientists.\n",
    "\n",
    "You should choose a base learner algorithm to use Boosting or Bagging. For instance, if you select a classification tree, Bagging and Boosting will comprise a pool of trees as large as you want.\n",
    "\n",
    "Bagging and Boosting: A Conclusive Summary\n",
    "Now that we have thoroughly described the concepts of Bagging and Boosting, we have arrived at the end of the article and can conclude how both are equally important in Data Science and where to be applied in a model depends on the sets of data given, their simulation and the given circumstances. Thus, on the one hand, in a Random Forest model, Bagging is used, and the AdaBoost model implies the Boosting algorithm.\n",
    "\n",
    "A machine learning model’s performance is calculated by comparing its training accuracy with validation accuracy, which is achieved by splitting the data into two sets: the training set and validation set. The training set is used to train the model, and the validation set is used for evaluation. \n",
    "\n",
    "You can check IIT Delhi’s Executive PG Programme in Machine Learning  in association with upGrad. IIT Delhi is one of the most prestigious institutions in India. With more the 500+ In-house faculty members which are the best in the subject matters.\n",
    "\n",
    "Why is bagging better than boosting?\n",
    "From the dataset, bagging creates extra data for training. Random sampling and substitution from the original dataset is used to achieve this. In each new training data set, sampling with replacement may repeat certain observations. Every Bagging element has the same chance of emerging in a fresh dataset. Multiple models are trained in parallel using these multi datasets. It is the average of all the forecasts from several ensemble models. When determining classification, the majority vote obtained through the voting process is taken into account. Bagging reduces variation and fine-tunes the prediction to a desired result.\n",
    "\n",
    "How are the main differences bagging and boosting?\n",
    "Bagging is a technique for reducing prediction variance by producing additional data for training from a dataset by combining repetitions with combinations to create multi-sets of the original data. Boosting is an iterative strategy for adjusting an observation's weight based on the previous classification. It attempts to increase the weight of an observation if it was erroneously categorized. Boosting creates good predictive models in general.\n",
    "\n",
    "What are the similarities between bagging and boosting?\n",
    "Bagging and boosting are ensemble strategies that aim to produce N learners from a single learner. They sample at random and create many training data sets. They arrive at their final decision by averaging N learners' votes or selecting the voting rank of the majority of them. They reduce variance and increase stability while reducing errors.\n",
    "\n",
    "Want to share this article?\n",
    "\n",
    "Lead the AI Driven Technological Revolution\n",
    "APPLY FOR EXECUTIVE PG PROGRAMME IN MACHINE LEARNING & AI FROM IIIT-B\n",
    "Leave a comment\n",
    "Your email address will not be published. Required fields are marked *\n",
    "\n",
    " Machine Learning Courses\n",
    "Advanced Certificate Programme in Machine Learning and NLP from IIIT Bangalore - Duration 8 Months\n",
    "Master of Science in Machine Learning & AI from LJMU - Duration 18 Months\n",
    "Executive PG Program in Machine Learning and AI from IIIT-B - Duration 12 Months\n",
    "Our Popular Machine Learning Course\n",
    "Machine Learning Course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fe9f1-1ba7-4af5-84d1-a9d4b538e2d9",
   "metadata": {},
   "source": [
    "Cross-validation is a statistical method used to estimate the skill of machine learning models.\n",
    "\n",
    "It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n",
    "\n",
    "In this tutorial, you will discover a gentle introduction to the k-fold cross-validation procedure for estimating the skill of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb1ad1-2486-4361-b3d0-ee8cad321709",
   "metadata": {},
   "source": [
    "+ Cross validation is used to remove all the bias in your data . so that it will be well randomized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6eb578-904d-470e-8d9f-7c572326fd1a",
   "metadata": {},
   "source": [
    "#### Boosting: Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726feda1-6040-482b-abd1-af7078e53870",
   "metadata": {},
   "source": [
    "1. __Boosting:_______________ consists of the idea of filtering or weighting the data that is used to train our team of weak learners, so that each new learner gives more weight or is only trained with observations that have been poorly classified by the previous learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38b2ed-a919-49ae-975c-629ee5dffaaa",
   "metadata": {},
   "source": [
    "2. Boosting models fall inside this family of _Ensemble methods___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21a49e-6a63-4ff7-99b7-39a77668368a",
   "metadata": {},
   "source": [
    "3. What is the key difference between Boosting and Bagging?\n",
    "\n",
    "\n",
    "Boosting involves sequential training while bagging involves parallel training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee65ad1-da66-452d-9369-d38782b29e78",
   "metadata": {},
   "source": [
    "4. Why is boosting effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29959fb3-720c-430d-9a35-476cce80a565",
   "metadata": {},
   "source": [
    "Ans :It's able to focus on the most difficult data observation sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85134a56-43b6-48f2-9aad-a7a384d05454",
   "metadata": {},
   "source": [
    "5. What plays a vital role in the boosting training process?\n",
    "\n",
    "Algorithmn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f047b9-6fbe-4f38-b36b-427353bc8643",
   "metadata": {},
   "source": [
    "6. Which of these is a type of boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f5f28-b6f8-4c01-9d41-2a164d16d315",
   "metadata": {},
   "source": [
    "Adaboost\n",
    "LightGBM\n",
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52fd57-e6c7-4211-88c8-9779fb3e109a",
   "metadata": {},
   "source": [
    "7.Gradient Boosting Machines ___________________ train weak learners sequentially, adding more and more estimators, but instead of adapting the weights of the data, it tries to predict the residual errors made by the previous estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949d367-53f6-4f37-82f1-9febe9ff38c3",
   "metadata": {},
   "source": [
    "Very similar to AdaBoost, Gradient Boosting Machines train weak learners sequentially, adding more and more estimators, but instead of adapting the weights of the data, it tries to predict the residual errors made by the previous estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72858c00-f81f-4dfd-b119-812b93070e26",
   "metadata": {},
   "source": [
    "8. ____Adaptive Boosting________________________ works by the exact process described before of training sequentially, predicting and updating the weights of the miss-classified samples and of the corresponding weak models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ba3be-b3ed-47cb-a1e2-9e06daf3cca1",
   "metadata": {},
   "source": [
    "Adaptive Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc552663-d0b5-43ea-8dab-f5bd57d99fbe",
   "metadata": {},
   "source": [
    "9. EXtreme Gradient Boosting_______________ uses XGBoost trees rather than fixed size decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157973c-07ef-4e4c-8c62-04c0fd16ea4c",
   "metadata": {},
   "source": [
    "10. One of these is NOT a type of boosting models. \n",
    "\n",
    "tensorflow is not  type of boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d6d7d-b556-45f3-a228-974ad1cbb17a",
   "metadata": {},
   "source": [
    "Gradient Boosting Machines\n",
    "eXtreme Gradient Boosting\n",
    "Light Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130d0f7-4ddd-4bcd-bc74-2fd7e19c4399",
   "metadata": {},
   "source": [
    "### Ensemble Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274eb17a-8078-40ff-a316-89fba372f717",
   "metadata": {},
   "source": [
    "bagging or bootstrap\n",
    " create random\n",
    "    \n",
    "    average \n",
    "    \n",
    "    random forest is a good example of ensambee\n",
    "    \n",
    "    it reduces overfitting createed by boosting\n",
    "    \n",
    "bosting arranges weak learners sequentially.\n",
    "\n",
    " each new tre is used on a modified \n",
    " \n",
    " adaboost helps you to \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0369e24-8d38-4aeb-8d7c-617a59c4cec0",
   "metadata": {},
   "source": [
    "##### Ensemble Learning Quiz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db20a8-ed5d-4198-b5f6-fc521cc08282",
   "metadata": {},
   "source": [
    "1. Which of the following algorithm is not an example of an ensemble method?\n",
    "ans:\n",
    "Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018db8b2-9392-4adb-b3bc-dee4e0fa7de0",
   "metadata": {},
   "source": [
    "Examples of ensemble method\n",
    "\n",
    "A. Extra Tree Regressor\n",
    "\n",
    "B. Random Forest\n",
    "\n",
    "C. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef090c-c768-427e-9465-9792171f70fc",
   "metadata": {},
   "source": [
    "2. What is true about an ensembled classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bfd4f-1763-41d9-b02b-68381ec36afc",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. Classifiers that are more “sure” can vote with more conviction\n",
    "2. Classifiers can be more “sure” about a particular part of the space\n",
    "3. Most of the times, it performs better than a single classifier\n",
    "\n",
    "All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcbc9c-c6e1-4374-a50e-ff00c1b47be4",
   "metadata": {},
   "source": [
    "3. Which of the following option is / are correct regarding benefits of ensemble model?\n",
    "\n",
    "﻿1. Better performance\n",
    "2. Generalized models\n",
    "\n",
    " one and 2 is correct\n",
    "\n",
    "\n",
    "3. Better interpretability\n",
    "\n",
    "incorrect no 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f752c-881f-44ec-8b1d-065a4190958b",
   "metadata": {},
   "source": [
    "4. Which of the following is / are true about weak learners used in ensemble model?\n",
    "\n",
    "\n",
    "1 and  2\n",
    "﻿1. They have low variance and they don’t usually over-fit\n",
    "2. They have high bias, so they can not solve hard learning problems\n",
    "\n",
    "no 3 is not correct\n",
    "3. They have high variance and they don’t usually over-fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e4eed-e15d-4537-a093-3e9b0b1b1fd9",
   "metadata": {},
   "source": [
    "5. If we didn’t assign a base estimators to the bagging classifier it will use by default:\n",
    "\n",
    "Answer\n",
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5e7c8-b11e-4f07-8905-3dcb550513e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Week 12 machine learning \n",
    "\n",
    "SVD single value decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b6e5a8-d154-4100-9e65-2b1949b95010",
   "metadata": {},
   "source": [
    "Big data involves larger quantities of information while small data is, not surprisingly, smaller. Here's another way to think about it: big data is often used to describe massive chunks of unstructured information. Small data, on the other hand, involves more precise, bite-sized metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27357aea-8139-4ae4-b527-1964ec8b524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  What is Dimensionality Reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be0ff5-5931-4568-a0d4-fa16eddc030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Big data involves larger quantities of information while small data is, not surprisingly, smaller. Here's another way to think about it: big data is often used to describe massive chunks of unstructured information. Small data, on the other hand, involves more precise, bite-sized metrics.19 Dec 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1632510-547c-44c9-b9e6-872fd8d19754",
   "metadata": {},
   "source": [
    "Dimensionality reduction is simply the process of reducing the dimension of\n",
    "your feature set. \n",
    "\n",
    "Your feature set could be a dataset with a hundred columns\n",
    "\n",
    "(i.e features) or it could be an array of points that make up a large sphere in\n",
    "the three-dimensional space.\n",
    "\n",
    "Dimensionality reduction is bringing the\n",
    "number of columns down to say, twenty or converting the sphere to a circle\n",
    "in the two-dimensional space.\n",
    "\n",
    "That is all well and good but why should we care? Why would we drop 80\n",
    "columns off our dataset when we could straight up feed it to our machine\n",
    "learning algorithm and let it do the rest?\n",
    "\n",
    "The Curse of Dimensionality\n",
    "\n",
    "We care because the curse of dimensionality demands that we do. The curse\n",
    "of dimensionality refers to all the problems that arise when working with\n",
    "data in the higher dimensions that did not exist in the lower dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f86ec-c13c-4fa8-b1d5-33e453e424c2",
   "metadata": {},
   "source": [
    "As the number of features increase, the number of samples also increases\n",
    "proportionally. The more features we have, the more samples we will need to\n",
    "have all combinations of feature values well represented in our sample.\n",
    "\n",
    "As the number of features increases, the model becomes more complex. The\n",
    "more the number of features, the more the chances of overfitting. A machine\n",
    "learning model that is trained on a large number of features, gets\n",
    "increasingly dependent on the data it was trained on and in turn overfitted,\n",
    "resulting in poor performance on real data, beating the purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdf736-d487-4899-967d-5dae49fec17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Avoiding overfitting is a major motivation for performing dimensionality\n",
    "reduction. The fewer features our training data has, the lesser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc59b6-a397-46ca-b053-283ca6d0e430",
   "metadata": {},
   "source": [
    "assumptions our model makes and the simpler it will be. But that is not all\n",
    "and dimensionality reduction has a lot more advantages to offer, like\n",
    "1. Less misleading data means model accuracy improves.\n",
    "2. Less dimensions mean less computing. Less data means that algorithms\n",
    "train faster.\n",
    "3. Less data means less storage space required.\n",
    "4. Less dimensions allow usage of algorithms unfit for a large number of\n",
    "dimensions\n",
    "5. Removes redundant features and noise.\n",
    "Feature Selection and Feature Engineering for dimensionality\n",
    "reduction\n",
    "Dimensionality reduction could be done by both feature selection methods as\n",
    "well as feature engineering methods.\n",
    "Feature selection is the process of identifying and selecting relevant features\n",
    "for your sample. Feature engineering is manually generating new features\n",
    "from existing features, by applying some transformation or performing some\n",
    "operation on them.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d12e7ca4-02fa-4215-b2bf-8af7789a49af",
   "metadata": {},
   "source": [
    "Feature selection can be done either manually or programmatically. For\n",
    "example, consider you are trying to build a model which predicts people’s\n",
    "weights and you have collected a large corpus of data which describes each\n",
    "person quite thoroughly. If you had a column that described the color of each\n",
    "person’s clothing, would that be much help in predicting their weight? I person’s clothing, would that be much help in predicting their weight? I\n",
    "think we can safely agree it won’t be. This is something we can drop without\n",
    "further ado. What about a column that described their heights? That’s a\n",
    "definite yes. We can make these simple manual feature selections and reduce\n",
    "the dimensionality when the relevance or irrelevance of certain features are\n",
    "obvious or common knowledge. And when it's not glaringly obvious, there\n",
    "are a lot of tools we could employ to aid our feature selection.\n",
    "1. Heatmaps that show the correlation between features is a good idea.\n",
    "2. So is just visualising the relationship between the features and the\n",
    "target variable by plotting each feature against the target variable.\n",
    "Now let us look at a few programmatic methods for feature selection from\n",
    "the popular machine learning library sci-kit learn, namely,\n",
    "1. Variance Threshold and\n",
    "2. Univariate selection.\n",
    "Variance Threshold is a baseline approach to feature selection. As the name\n",
    "suggests, it drops all features where the variance along the column does not\n",
    "exceed a threshold value. The premise is that a feature which doesn’t vary\n",
    "much within itself, has very little predictive power.\n",
    ">>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6686e-e805-4e47-9c1f-7b546d01e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
    ">>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
    ">>> selector = VarianceThreshold()\n",
    ">>> selector.fit_transform(X)\n",
    "array([[2, 0],\n",
    "[1, 4],\n",
    "[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd0919-4fe1-41d5-a9d0-154dd33480c5",
   "metadata": {},
   "source": [
    "Univariate Feature Selection uses statistical tests to select features.\n",
    "Univariate describes a type of data which consists of observations on only a\n",
    "single characteristic or attribute. Univariate feature selection examines each\n",
    "feature individually to determine the strength of the relationship of the\n",
    "feature with the response variable. Some examples of statistical tests that can\n",
    "be used to evaluate feature relevance are Pearson Correlation, Maximal\n",
    "information coefficient, Distance correlation, ANOVA and Chi-square.\n",
    "Chi-square is used to find the relationship between categorical variables and\n",
    "Anova is preferred when the variables are continuous.\n",
    "Scikit-learn exposes feature selection routines likes SelectKBest,\n",
    "SelectPercentile or GenericUnivariateSelect as objects that implement a\n",
    "transform method based on the score of anova or chi2 or mutual\n",
    "information. Sklearn offers f_regression and mutual_info_regression as the\n",
    "scoring functions for regression and f_classif and mutual_info_classif for\n",
    "classification.\n",
    "F-Test checks for and only captures linear relationships between features\n",
    "and labels. A highly correlated feature is given higher score and less\n",
    "correlated features are given lower score. Correlation is highly deceptive as it\n",
    "doesn’t capture strong non-linear relationships. On the other hand, mutual\n",
    "information methods can capture any kind of statistical dependency, but\n",
    "being nonparametric, they require more samples for accurate estimation.\n",
    "Feature selection is the simplest of dimensionality reduction methods. We\n",
    "will look at a few feature engineering methods for dimensionality reduction\n",
    "later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012b00e-b687-416a-b96a-9110db235ad8",
   "metadata": {},
   "source": [
    "#### Linear Dimensionality Reduction Methods\n",
    "\n",
    "\n",
    "\n",
    "The most common and well known dimensionality reduction methods are\n",
    "the ones that apply linear transformations, like\n",
    "\n",
    "1. PCA (Principal Component Analysis) : Popularly used for\n",
    "dimensionality reduction in continuous data, PCA rotates and projects data\n",
    "along the direction of increasing variance. The features with the maximum\n",
    "variance are the principal components.\n",
    "2. Factor Analysis : a technique that is used to reduce a large number of\n",
    "variables into fewer numbers of factors. The values of observed data are\n",
    "expressed as functions of a number of possible causes in order to find which\n",
    "are the most important. The observations are assumed to be caused by a\n",
    "linear transformation of lower dimensional latent factors and added\n",
    "Gaussian noise.\n",
    "3. LDA (Linear Discriminant Analysis): projects data in a way that the\n",
    "class separability is maximised. Examples from the same class are put closely\n",
    "together by the projection. Examples from different classes are placed far\n",
    "apart by the projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076badd-b17b-4d7f-bd94-bdba6d773558",
   "metadata": {},
   "source": [
    "Non-linear Dimensionality Reduction Methods\n",
    "Non-linear transformation methods or manifold learning methods are used\n",
    "when the data doesn’t lie on a linear subspace. It is based on the manifold\n",
    "hypothesis which says that in a high dimensional structure, most relevant\n",
    "information is concentrated in a small number of low dimensional\n",
    "manifolds. If a linear subspace is a flat sheet of paper, then a rolled up sheet\n",
    "of paper is a simple example of a nonlinear manifold. Informally, this is\n",
    "called a Swiss roll, a canonical problem in the field of non-lilinear\n",
    "dimensionality reduction. Some popular manifold learning methods are,\n",
    "\n",
    "\n",
    "1. Multidimensional scaling (MDS) : A technique used for analyzing\n",
    "similarity or dissimilarity of data as distances in geometric spaces. Projects\n",
    "data to a lower dimension such that data points that are close to each other\n",
    "(in terms of Euclidean distance) in the higher dimension are close in the\n",
    "lower dimension as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f343fd-ed0f-4658-900d-56ca99fc1458",
   "metadata": {},
   "source": [
    "2. Isometric Feature Mapping (Isomap) : Projects data to a lower\n",
    "dimension while preserving the geodesic distance (rather than Euclidean\n",
    "distance as in MDS). Geodesic distance is the shortest distance between two\n",
    "points on a curve.\n",
    "\n",
    "3. Locally Linear Embedding (LLE): Recovers global non-linear structure\n",
    "from linear fits. Each local patch of the manifold can be written as a linear,\n",
    "weighted sum of its neighbours given enough data.\n",
    "\n",
    "4. Hessian Eigen Mapping (HLLE): Projects data to a lower dimension\n",
    "while preserving the local neighbourhood like LLE but uses the Hessian\n",
    "operator to better achieve this result and hence the name.\n",
    "\n",
    "5. Spectral Embedding (Laplacian Eigenmaps): Uses spectral techniques\n",
    "to perform dimensionality reduction by mapping nearby inputs to nearby\n",
    "outputs. It preserves locality rather than local linearity\n",
    "\n",
    "6. t-distributed Stochastic Neighbor Embedding (t-SNE): Computes the\n",
    "probability that pairs of data points in the high-dimensional space are\n",
    "related and then chooses a low-dimensional embedding which produces a\n",
    "similar distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c77f63-dc54-4c04-8619-30f350729d1e",
   "metadata": {},
   "source": [
    "#### Auto-encoders\n",
    "\n",
    "\n",
    "Another popular dimensionality reduction method that gives spectacular\n",
    "results are auto-encoders, a type of artificial neural network that aims to\n",
    "copy their inputs to their outputs. They compress the input into a\n",
    "latent-space representation, and then reconstruct the output from this\n",
    "representation. An autoencoder is composed of two parts :\n",
    "1. Encoder: compresses the input into a latent-space represent\n",
    "\n",
    "1. Encoder: compresses the input into a latent-space representation.\n",
    "2. Decoder: reconstruct the input from the latent space representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c33b7-9677-405e-800a-d25281515d68",
   "metadata": {},
   "source": [
    "#### Feature Selection using Fisher Score and Chi2 (χ2) Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c4ca5-c0f4-48e7-9a15-32d14f91bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Removes irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b97fd6-6a35-4f91-b7e0-a8054e000f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Σ ( Xi – μ )2'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
