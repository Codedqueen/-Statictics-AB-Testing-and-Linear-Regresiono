{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29cf884-ce31-4fff-a9f6-9416e24d21da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Week 9 day 1- Linear regression with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465a28d-f9af-45fd-8f9d-67f202edd07f",
   "metadata": {},
   "source": [
    "### What is sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3e47a-94dc-4ef6-8fc2-047b93655c71",
   "metadata": {},
   "source": [
    "Scikit -learn(sklearn) is a machine learning package for python and\n",
    "\n",
    "it agueably comes from SciPy Toolkit meaning an addition on top of the package scipy.\n",
    "\n",
    "Sklearn is built on Numpy, SciPy and matplotlib.\n",
    "\n",
    "This last statement has two major implication.\n",
    "\n",
    "First, sklearn is very fast and efficient .\n",
    "Second,it often prefers working with arrays.\n",
    "\n",
    "So far, we have worked mainly with pandas DataFrames because of statsmodels.\n",
    "\n",
    "Now, however even if we pre-process data in pandas we may need to \n",
    "\n",
    "transform it into an Ndarray before feeding it to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd671b0c-9d91-4815-a4fe-16e1e00cb704",
   "metadata": {},
   "source": [
    "Major Advantages of Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7f222-671d-4f3c-a1c0-a215a56468a7",
   "metadata": {},
   "source": [
    ">1. It boost incredible documentation\n",
    "\n",
    "Whatever doubt you may has on how it works, Sklearn have it on its website,usually with the example applications.\n",
    "\n",
    "This will be a great supplementary report for you when you are learning to work with this package.\n",
    "\n",
    ">2. Variety\n",
    "\n",
    "In terms of machine learning ,sklearn is the leading package right now.\n",
    "\n",
    ".Regression\n",
    ".Classification\n",
    ".Clustering\n",
    ".Support vector machines\n",
    ".Dimensionality reduction\n",
    "\n",
    "Everything is there.\n",
    "\n",
    "The only weak spot in this range is deep learning,\n",
    ".Tensoflow \n",
    ".Keras\n",
    ".pytorch \n",
    "are much better options in this area.\n",
    "\n",
    ">3. Numerical stability:\n",
    "\n",
    "Sklearn is famously numerically stable.\n",
    "\n",
    "The basic idea is that training an algorithm is about performing complicated mathematical operations in the backround.\n",
    "\n",
    "When the numbers you are dealing with are too small or too big,your code may break.\n",
    "\n",
    "Not because of your programming skills but because the library you are using have some minor inefficiencies which may turn into big problems.\n",
    "\n",
    "Sklearn rarely have those and the community actively contribute to the package when ever an issue arises ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e24a09-9a53-4020-bbe9-a801fb3563df",
   "metadata": {},
   "source": [
    "To help you choose, here are the best Python libraries for machine learning and deep learning.\n",
    "\n",
    "NumPy. NumPy is an open-source numerical and popular Python library. ...\n",
    "SciPy. SciPy is a free and open-source \n",
    "\n",
    "library that's based on NumPy. ...\n",
    "\n",
    "Scikit-Learn. ...\n",
    "\n",
    "Theano. ...\n",
    "\n",
    "TensorFlow. ...\n",
    "\n",
    "Keras. ...\n",
    "\n",
    "PyTorch. ...\n",
    "\n",
    "Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09456db1-6c8b-4061-ba52-ea2a9b6a0fb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Game plan for sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6153e6bb-903d-43cc-81bb-9b6d22b5a545",
   "metadata": {},
   "source": [
    "In the beginning of data science training , we were introduced to the traditional statistical method such as linear regression, Logistic Regression and clustering.\n",
    "\n",
    "We have methods like neural networks which are a whole lot more sophisticated ,which we call machine learning.(Deep learning)\n",
    "\n",
    "Both groups can be called machine learning method. To distinguish between them ,we can seperate them into machine learning and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a038f-2e94-4a17-9019-b9fa4737afa5",
   "metadata": {},
   "source": [
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are ...\n",
    "\n",
    "â€ŽAI vs. Machine Learning vs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8966fda8-eb9f-4dee-bcf0-001ad1076991",
   "metadata": {},
   "source": [
    "Hence, the area we are covering in this lecture, linear and logistics regressions often fall under the term machine learning according and you may call them ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4e3c7-e974-4e11-8281-97fb4c29781f",
   "metadata": {},
   "source": [
    "We said earlier that statsmodel is a great library for learning purposes buy in practice, Sklearn is the preferred choice of most professionals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c55b8e-2dbd-49f0-9378-7b8b8c97be45",
   "metadata": {},
   "source": [
    "We will learn the theory in statsmodels and transfer to sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a37ea-508b-4b6b-9476-e9123544145d",
   "metadata": {},
   "source": [
    "Game plan:\n",
    "    \n",
    "We will use thesame example as before.\n",
    "\n",
    "But this time employing sklearn.\n",
    "\n",
    "We will not repeat any old theorem, hence need to go through the lessons up until now.\n",
    "\n",
    "Stasmodels feeds quite well with DataFrame however sklearn models work with arrays .\n",
    "\n",
    "Therefore,we will be compeled to work with Numpy sligthly more\n",
    "\n",
    "Finally, we will learn not only syntax but also new concepts made easy by this extra ordinary package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2066aaa-9238-4907-b3d9-cd0049303212",
   "metadata": {},
   "source": [
    "The line of best fit is used to express a relationship in a scatter plot of different data points. It is an output of regression analysis and can be used as a prediction tool for indicators and price movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff4079-2b7c-4314-9069-f576f1db1ca4",
   "metadata": {},
   "source": [
    "Goodness of fit for the regression is indicated by the mean square weighted deviates (MSWD), which is a measure of data point displacement from the regression line beyond each point's analytical uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cb90e-f392-4d6c-82c3-b005b591d103",
   "metadata": {},
   "source": [
    "As you increase in elevation, there is less air above you thus the pressure decreases. As the pressure decreases, air molecules spread out further (i.e. air expands), and the temperature decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d7fd8-ff1c-4119-9bf3-d7810fb5c9f6",
   "metadata": {},
   "source": [
    "Least Square Error - In linear regression, we try to minimize the least square errors of the model to identify the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409016e2-68fa-4541-8623-509bbd3e18d3",
   "metadata": {},
   "source": [
    "What does log-loss conceptually mean? Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value.17 Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef91a54-fcfd-4f50-9820-8a4e31c3bd09",
   "metadata": {},
   "source": [
    "IN statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cab80-a6a8-4ebb-8779-5f0354535e0c",
   "metadata": {},
   "source": [
    "### Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b62287-6bdb-4512-9999-86fab3ffdb8e",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc495f5f-803d-40f0-9720-4aac6f134ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d46d4d-4729-4c87-b368-d7392dd5f77e",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "This is the model needed for creating all methods needed for creating a llinear regression with sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637605b-4da5-465b-8fd8-1346c30a438d",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58106c6c-3ae1-4fd5-9664-ba2a3998a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\python files - Copy\\\\1.01. Simple linear regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d51ca25-f294-4e34-b970-6d247ec52ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA\n",
       "0  1714  2.40\n",
       "1  1664  2.52\n",
       "2  1760  2.54\n",
       "3  1685  2.74\n",
       "4  1693  2.83"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b340ed-4d3c-4835-8f90-fcf02594a55b",
   "metadata": {},
   "source": [
    "Declare the dependent variable GPA (output target) and independent variable SAT (input target or feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8e10a-bf01-49e6-a07d-d617b55c79d1",
   "metadata": {},
   "source": [
    "CREATE THE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c312b6-76b0-4cf7-9424-02e1e54f9b39",
   "metadata": {},
   "source": [
    "When we fit the model we need to convert the to nd array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc6aea5-2e43-4b9e-bb9c-a90cf78efb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['SAT']# x is called input or feature\n",
    "y = data['GPA'] # y is called output or taget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ced20-dad3-443f-b34f-719f3e665990",
   "metadata": {},
   "source": [
    "Generally,there are three types of machine learning:\n",
    "\n",
    "1. Supervised \n",
    "\n",
    "2. Unsupervised\n",
    "\n",
    "3. Reinforcement\n",
    "\n",
    "So far we have only seen supervised learning.\n",
    "\n",
    "In supervised learning ,we have inputs and targets.\n",
    "\n",
    "Inputs are the features we use to predict an outcome, in this case the SAT score.\n",
    "\n",
    "The target are the correct values we are aiming for and in this case the GPA.\n",
    "\n",
    "Usually the targets are historical values that corressponds with given SAT scores.\n",
    "\n",
    "Infact, our machine learning algorithm will find the utmost coefficient of a linear regression model .\n",
    "\n",
    "Thats when given an SAT score that predicts the GPA of a student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27065be-c5f2-46c4-aca5-90a3c7e5dd95",
   "metadata": {},
   "source": [
    "Check the shapes of our input and targets.\n",
    "They are both vectors of link 84.\n",
    "\n",
    "Let's proceed to feeding them to our algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ee14d4-f63c-4128-bf51-84291145259c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01dbc013-3baf-4c7d-bfd8-569dbdb4afcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53382a42-7d1e-4870-a216-89e13bb77250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_matrix = x.values.reshape(-1,1)\n",
    "x_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d0ee3-efa8-4f82-9a66-1aa9a4ae832d",
   "metadata": {},
   "source": [
    "sklearn takes full advantage of the object oriented capabilities of python and works alot with very well written classes.\n",
    "\n",
    "More often than not,we will need to create an object or an instance of the given class.\n",
    "\n",
    "In this case, let reg = LinearRegression()\n",
    "\n",
    "After we execute this line reg  we will have an instance of the linearRegression class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99c044-505c-4740-8820-0b5cfd75a532",
   "metadata": {},
   "source": [
    "REGRESSION ITSELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f82d7cf-30a3-4d55-adf2-1fbd38e4b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02886abb-d183-4ce4-bde8-9efc1cb4f62c",
   "metadata": {},
   "source": [
    "Next thing to do is to fit the regression.\n",
    "\n",
    "reg.fit , then indicate the two arguement(x,y), konwn as the target in the correct order, order is very important , because sklearn has a different order than StatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504d8169-8ff3-4c1b-975d-8f67431d78d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(x_matrix,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8f06f-7aef-491e-8b67-3813c4751499",
   "metadata": {},
   "source": [
    "After running this code ,we got an error\" Expected 2D array got 1D array instead.\n",
    "\n",
    "This is because our input is one dimentional object (84) which sklearn does not fancy.\n",
    "\n",
    "If we check our code above, we will see that x is a vector and has single dimension.\n",
    "\n",
    "What we must do is to reshape it into a matrix.\n",
    "\n",
    "This can be achieved with the reshape method .\n",
    "\n",
    "Let x_matrix = x.values.reshape(84,1)\n",
    "\n",
    "Because we only need the values from that DataFrame.\n",
    "the desired shape is (84,1)\n",
    "\n",
    "Note we are not changing anything , we are only changing the dimenionality from 1 dimension bject to 2 dimension object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e61f3-c7db-49fc-8ea0-3411ac92fc16",
   "metadata": {},
   "source": [
    "Alternaively, we can use te sklearn suggestion,reshape(-1,1)\n",
    "\n",
    "Syntatially ,it has the same meaning, but is more generalized as we need not specify the number of observations we got.\n",
    "\n",
    "x_matrix = x.values.reshape(-1,1)\n",
    "\n",
    "Checking the shape we got the desird result,\n",
    "\n",
    "x_matrix.shape (84,1)\n",
    "\n",
    "After the correction we can now rerun the regression.\n",
    "\n",
    "Note that this is an issue only for a simple linear regression model.\n",
    "\n",
    "If we have more than one feature will will not have this complication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369331e-3045-4da7-91a5-9d9e466cc59b",
   "metadata": {},
   "source": [
    "The assumption the developers had is that normally sklearn inspects dozens of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d03978-8319-422d-a74b-ee66d84f1482",
   "metadata": {},
   "source": [
    "### A Note on Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3a496-e9e7-4851-b864-a30473e7a41e",
   "metadata": {},
   "source": [
    "As explained in the lesson, the concept 'normalization' has different meanings in different contexts.\n",
    "There are two materials which we find particularly useful:\n",
    "\n",
    "\n",
    "\n",
    "The Wikipedia article on Feature scaling: \n",
    "https://en.wikipedia.org/wiki/Feature_scaling\n",
    "\n",
    "This article on L1-norm and L2-norm: \n",
    "http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd9a2b-9a8a-4c2b-88d7-07d4180fb556",
   "metadata": {},
   "source": [
    "### Simple linear regression - summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2dd93-1bee-420f-a6cb-c0ef17ef033d",
   "metadata": {},
   "source": [
    "EXPLANATION OF THE PARAMETERS ON SIMPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef1232-3323-47a6-9dc6-b57e1a99f406",
   "metadata": {},
   "source": [
    "1. STANDARDIZATION: This is the process of subtracting the mean and dividing by the standard deviation(a type of normalization) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ee579-b135-4767-9d32-7695384232fe",
   "metadata": {},
   "source": [
    "NORMALIZATION: It has different meaning depending on the case;here- we subtract the mean but divide by the L2-norm of the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874988e-7d48-44b4-a7cc-0da9a93d4e5b",
   "metadata": {},
   "source": [
    "Sometimes the two prameters are used interchangeably\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca293d-86ac-423d-90a3-5cd08c0e9f8f",
   "metadata": {},
   "source": [
    "2. Copy_X_True\n",
    "\n",
    "This copies the input before fitting them, this is a safety net gainst normalization and other transformation that can be done by sklearn while creating an algorithm.\n",
    "\n",
    "If you remember in our StatsModels lectures,we will create copies of DataFrame often , but sklearn takes care of that automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e2323-842c-4719-8e17-101e0843d498",
   "metadata": {},
   "source": [
    "3. Fit_intercept=True\n",
    "\n",
    "In StasModel we have to manually add a constant but the fit_intercept takes care of that precisely. '\n",
    "\n",
    "If you dont want an intercept, you can just set it to false\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8f9bd-8f38-4b21-a304-3d95634d0e94",
   "metadata": {},
   "source": [
    "4. N-jobs\n",
    "\n",
    "This is a parameter used when we want to pararalize routines .\n",
    "\n",
    "By default, only one cpu is used.\n",
    "n_jobs = 1\n",
    "For this simple example, we won't see a difference no matter the number of jobs we set.\n",
    "\n",
    "However, if you work on problems with lot and lots of data and have more than one cpu available, you can take advantage of this parameter by seting it to \n",
    "n_jobs = 2\n",
    "n_jobs = 3\n",
    "n_jobs = 4\n",
    "n_jobs = 5  and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f05882-b43d-4855-b324-772525647521",
   "metadata": {},
   "source": [
    "R-SQUARED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea95dc-14d8-431f-a746-4c117306f433",
   "metadata": {},
   "source": [
    "To get the R-squared of a linear regression with sklearn,\n",
    "\n",
    "We will use the method reg.score(x_matrix,y)\n",
    "\n",
    "The result is 0.406, \n",
    "\n",
    "exactly the one we found with stats\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b4d15c-fb05-4008-a4b7-6190459a8169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40600391479679765"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(x_matrix,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d403f-a7c0-46d4-b69d-35a521e87eb3",
   "metadata": {},
   "source": [
    "COEFFICIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8815fab7-172e-43ee-826d-83392d7313fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00165569])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_  \n",
    "# we us this command to find the coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89716ca-dc2a-4f0c-b925-30a7ce903800",
   "metadata": {},
   "source": [
    "The result is an nd array containing all coefficients in this case, there is a single value. 0.001655 ~ 0.0017\n",
    "\n",
    "When we get to the multiple regression lectures, this array will be filled with each of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21784e38-3727-4df6-9073-abaa5d4670cd",
   "metadata": {},
   "source": [
    "INTERCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4fc2098-7891-4338-9cb9-b35070f05c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2750402996602803"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_ \n",
    "# code needed for the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76f14c-7655-4aef-bbe8-75baadb115e5",
   "metadata": {},
   "source": [
    "Here ,we got a float instead of an array.\n",
    "\n",
    "This is because this type of regresion always have a single intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee192a1-ee33-429c-b996-3e14228d9d4a",
   "metadata": {},
   "source": [
    "MAKING PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dad055-a5dd-45de-be2b-3f687e920f9d",
   "metadata": {},
   "source": [
    "Maing predictions using the SAT score.\n",
    "\n",
    "There is a dedicated method called predict, which we can take advantage of.\n",
    "\n",
    "Predict takes up arguement which we want to predict, and outputs the prediction according to the model\n",
    "\n",
    "reg_predict(new_inputs) Returns the prediction of the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed6c64c-c429-4e11-9f4a-2ef44f11a22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.15593751])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict([[1740]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b79180-c726-4d2d-a7b4-dbb93da8a6cc",
   "metadata": {},
   "source": [
    "For example ,reg.predict(1740) will give\n",
    "\n",
    "us a predicted GPA for an SAT score of\n",
    "\n",
    "1740.\n",
    "\n",
    "The result is 3.1559 ~ 3.16\n",
    "\n",
    "This result if you bserve is also an array and not a float.\n",
    "\n",
    "This is because the predict method can take more than a single value.\n",
    "\n",
    "Infact , it can take a data frame or an array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93becbbf-2b03-4235-ba57-52d7ec5dbfd1",
   "metadata": {},
   "source": [
    "Let's try anothe feature out and predict some more values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963c134-3ef2-4d90-975f-b14bb77afa65",
   "metadata": {},
   "source": [
    "Lets create a new DataFrame called new_data \n",
    "\n",
    "see result below with column name 'SAT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042646ef-4763-4305-87e0-db65abb68338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT\n",
       "0  1740\n",
       "1  1760"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame(data=[1740,1760],columns=['SAT'])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f863b99-6c76-4269-84a5-1e2bd4aaf289",
   "metadata": {},
   "source": [
    "Using the reg.predict(new_data) method we can feed the whole DataFrame.\n",
    "\n",
    "Te result is an array with two predictions: 3.1559 ~3.16 and 3.1890 ~ 3.19.\n",
    "\n",
    "If you want you can add this information directly into the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d6c4ab-99ac-4c90-81b5-90bdb3b480ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.15593751, 3.18905127])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d9aaa-5243-46f1-99a4-f6788d743256",
   "metadata": {},
   "source": [
    "To add the new prediction into the original DataFrame.\n",
    "\n",
    "use the feature below.\n",
    "\n",
    "This will create a new series in the DataFrame containing the predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30404fb7-8cfd-4e3f-8a59-00daf53b668b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>Predicted_GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1740</td>\n",
       "      <td>3.155938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1760</td>\n",
       "      <td>3.189051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT  Predicted_GPA\n",
       "0  1740       3.155938\n",
       "1  1760       3.189051"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['Predicted_GPA'] = reg.predict(new_data)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98e200-c329-4ed4-83d1-d71d477db1d7",
   "metadata": {},
   "source": [
    "Finally , we can plot the regression using thesame code as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9de03d86-95e7-4331-bbbc-469b34006988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)\n",
    "yhat = reg.coef_*x_matrix + reg.intercept_\n",
    "#yhat + 0.0017* X + 0.275\n",
    "fig = plt.plot(x,yhat, lw=4, c='orange', label = 'regression line')\n",
    "plt.xlabel('SAT' , fontsize = 20)\n",
    "plt.ylabel('GPA' , fontsize = 20)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff783f8-3539-4435-90c5-162e21699169",
   "metadata": {},
   "source": [
    "Above is our first regression plot with sklearn.\n",
    "\n",
    "The default method gives us much less information but much more capabilities in the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83115444-74eb-4a48-b3f5-862fc66a3a25",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40b98cd0-df6f-4111-bf44-a12ce9f9b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f3293a-fba1-4ae8-89a8-d3a9256f2558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>1</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>3</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>3</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>3</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1936</td>\n",
       "      <td>3</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1810</td>\n",
       "      <td>1</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1987</td>\n",
       "      <td>3</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1962</td>\n",
       "      <td>1</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2050</td>\n",
       "      <td>2</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SAT  Rand 1,2,3   GPA\n",
       "0   1714           1  2.40\n",
       "1   1664           3  2.52\n",
       "2   1760           3  2.54\n",
       "3   1685           3  2.74\n",
       "4   1693           2  2.83\n",
       "..   ...         ...   ...\n",
       "79  1936           3  3.71\n",
       "80  1810           1  3.71\n",
       "81  1987           3  3.73\n",
       "82  1962           1  3.76\n",
       "83  2050           2  3.81\n",
       "\n",
       "[84 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"1.02. Multiple linear regression.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd6ee4f1-5717-4b3f-bd0e-5ac24b68e988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>1</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>3</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>3</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>3</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT  Rand 1,2,3   GPA\n",
       "0  1714           1  2.40\n",
       "1  1664           3  2.52\n",
       "2  1760           3  2.54\n",
       "3  1685           3  2.74\n",
       "4  1693           2  2.83"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81c043d6-c7ba-44b1-a621-03fa2929350f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>Rand 1,2,3</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1845.273810</td>\n",
       "      <td>2.059524</td>\n",
       "      <td>3.330238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.530661</td>\n",
       "      <td>0.855192</td>\n",
       "      <td>0.271617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1634.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1772.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1846.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2050.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SAT  Rand 1,2,3        GPA\n",
       "count    84.000000   84.000000  84.000000\n",
       "mean   1845.273810    2.059524   3.330238\n",
       "std     104.530661    0.855192   0.271617\n",
       "min    1634.000000    1.000000   2.400000\n",
       "25%    1772.000000    1.000000   3.190000\n",
       "50%    1846.000000    2.000000   3.380000\n",
       "75%    1934.000000    3.000000   3.502500\n",
       "max    2050.000000    3.000000   3.810000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f168e-3091-4996-ab9b-abf98bbf368d",
   "metadata": {},
   "source": [
    "Sample is the machine learning word for observation.\n",
    "\n",
    "In machine learning ,you may often hear practitioners refering to dataset with a sample size of 84 as having observations or 84 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449c8d5-abbc-4c66-b8ac-03571737dc50",
   "metadata": {},
   "source": [
    "Create the multiple linear rgression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a194488-6161-4ac7-9345-fa193ea2762b",
   "metadata": {},
   "source": [
    "You must declare the dependent and independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878aa6d-cdd9-4891-94c2-6f874ea900ba",
   "metadata": {},
   "source": [
    "Declare the dependent variable GPA (output target) and independent variable SAT and Random1,2,3(input target or feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5962709-effb-4ab9-892f-6fdf03b7a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[[\"SAT\", 'Rand 1,2,3']]\n",
    "y = data['GPA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf682987-f286-412f-95da-89769797caa9",
   "metadata": {},
   "source": [
    "REGRESSION ITSELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b88a1435-d10e-49e3-8a0c-edcb8551ca76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0e2b9-c2cd-479a-b1bf-c475037b45f8",
   "metadata": {},
   "source": [
    "Notice that the cell executed without an error and we do not need t reshape the input.\n",
    "\n",
    "This is because sklearn is optimised for multiple linear regression.\n",
    "\n",
    "We only need 2 line of code to create a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ccbb0-50aa-498b-ae78-6e161eac7f72",
   "metadata": {},
   "source": [
    "To interprete the result we will explain the coefficient  and the intercept of our model.\n",
    "\n",
    "ref_coef_ explains an array of two elements, the coeficient of the SAT score and the coefficient of the Rand 1,2,3 variable.\n",
    "\n",
    "they are ordered in the way we feed them.\n",
    "\n",
    "scrolling up abit we realised that 0.00165 ~ 0.017 corresponds with SAT score and -0.00826982 ~ -0.0083 corresponds with Rand 1,2,3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c317146-9d29-4c2e-90ea-142c8338bd44",
   "metadata": {},
   "source": [
    "INTERCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cc41630-a0b0-4f4a-a4f6-300213501647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29603261264909486"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e9459-ff99-424c-9d8b-0221f1fb8b46",
   "metadata": {},
   "source": [
    "The result shows us that the intercept of this model is 0.296. \n",
    "\n",
    "By comparing this result with the stats Model summary, we confirm that the two packages yields identical result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252036c0-b148-45ad-b330-e8bc08f4cd42",
   "metadata": {},
   "source": [
    "### Adjusted R-sqaured "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74ffa3-65dc-47d7-9c14-76a6fdc3febc",
   "metadata": {},
   "source": [
    "This is one of the most common measures of goodness of fit.\n",
    "\n",
    "It is a universal measure to evaluate how well linear regression fair and compare.\n",
    "\n",
    "Moreover, there is a very intuitive method called score which can help us with that.\n",
    "\n",
    "Depending on the type of regression you are performing, a regression score will have a different meaning.\n",
    "\n",
    "We can use the same score function for both the simple linear regression funstion and multiple linear regression function.\n",
    "\n",
    "What is inconvinient though is that when we are workong with sklearn we are compelled to call on a bunch of different method in other to reach the amazing summary which statsmodels provides us with.\n",
    "\n",
    "With this knowledge we will be able to choose our preferred package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e781394-1d4c-4458-b451-fa3ab06e59ce",
   "metadata": {},
   "source": [
    "CALCULATING THE R-SQUARED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "098bb196-43a4-4ddd-b790-40e514b90f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40668119528142843"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To find r-squared\n",
    "reg.score(x,y) # it takes two arguement , the input and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f1676-c6b3-4522-8ec3-daa6a3ef7135",
   "metadata": {},
   "source": [
    "AFter running the cell we got 0.4066 ~ 0.407.\n",
    "\n",
    "If we compare this result with the statsmodel regression summary from last session.\n",
    "\n",
    "The R-squared value achieved is pricely thesame.\n",
    "\n",
    "The good news is that the two packages are thesame in terms of result.\n",
    "\n",
    "The bad new is tha this the R-sqare value and not the adjusted R-squared.\n",
    "\n",
    "The adjusted R-squared is a much more appropriate measure for multiple linear Regression.\n",
    "\n",
    "It steps on the R-squared and adjust for the number of variable included in the model.\n",
    "\n",
    "If we are using features with low or no explanatory power, the R-squared will increase.\n",
    "\n",
    "Hence, we need to penelazie the success of usage through the adjusted R-squared.\n",
    "\n",
    "Unfortunately, there is no ready to use method included in sklearn.\n",
    "\n",
    "In such cases, there are two options:\n",
    "\n",
    "1. We can Google to check if someone had thesame prblem and solved it .\n",
    "\n",
    "2. Use our  mad -math skills to create our own measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11968d-4c15-42eb-974a-2e453ba14147",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Formula for Adjusted R^2\n",
    "\n",
    "$R^2_{adj.} = 1 - (1-R^2)*\\frac{n-1} {n-p-1}$"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee7e300b-5b9c-417d-9fa3-3deb2152b12d",
   "metadata": {},
   "source": [
    "$R^2_{adj.} = 1 - (1-R^2)*\\frac{n-1} {n-p-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace11ad-2df8-4dd1-8b71-362b4091f1bb",
   "metadata": {},
   "source": [
    "When you use mark down the code will compile and we will get a need math  formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23129d44-8c31-4a8f-a3f5-efcb67a7c598",
   "metadata": {},
   "source": [
    "In this formula,\n",
    "\n",
    "n = 84 (number of observations)\n",
    "\n",
    "p = 2 (the number of predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5fcd2-98e7-4e5d-9058-50016c330aee",
   "metadata": {},
   "source": [
    "Since it is based in our browser ,it has some unconventional capabilities  that most of the other id's lack .\n",
    "\n",
    "Above is the lateck code,\n",
    "by running the cell, the code will compile and we will get a need math formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f25fa2-72de-4c12-a293-87d66501a0e3",
   "metadata": {},
   "source": [
    "We will now try to create the R-squared measure on our own.\n",
    "\n",
    "Actually the expression is far from complicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d5855-99f6-42fe-a773-7f38f39a79df",
   "metadata": {},
   "source": [
    "First, we need to declare a variable called R2 where we will store the R2 we found earlier.\n",
    "\n",
    "next, we must find n and p (84 and 2), the n predictors stored in x\n",
    "\n",
    "x has a shape of 84,2 ie 84 observations and 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a174b6-d38f-4968-bf94-7a15b98cfcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3483dcb-a1a2-462e-befd-3872cc2efdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = reg.score(x,y)\n",
    "\n",
    "#n = 84\n",
    "n = x.shape[0]\n",
    "p = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a75e66-efe8-46fc-8da9-65a0f699c85a",
   "metadata": {},
   "source": [
    "To build the parameterised version of the code, we can use the shape function to extract the number of observations and features.\n",
    "\n",
    "The sample n are equal to the first \n",
    "\n",
    "dimension value of that shape, \n",
    "\n",
    "so n = 84, will now be changed to n = x.shape[0].\n",
    "\n",
    "logical the number of features \n",
    "\n",
    "p = 2  is represented as p = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0edd2-a907-43c9-a582-52cd97483ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = reg.score(x,y)\n",
    "\n",
    "n = x.shape[0]\n",
    "\n",
    "p = x.shape[1]\n",
    "\n",
    "adjusted_r2 = 1- (1-r2)*(n-1)/(n-p-1)\n",
    "adjusted_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a2d53-ef3f-4162-a107-3bef965b2b84",
   "metadata": {},
   "source": [
    "Finally ,we can calculate the adjusted R-squared,\n",
    "\n",
    "let ,  \n",
    "adjusted R-squared = 1- {1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "Next is to define your own python function that calculates the adjusted R-squared giving inputs and targets.\n",
    "\n",
    "But lets leave that for home work \n",
    "\n",
    "use this to execute the cell \n",
    "adjusted_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f91a6b-5a73-4c45-850a-948e3fd45a44",
   "metadata": {},
   "source": [
    "The value we get is 0.392,\n",
    "\n",
    "Usually you will not be able to validate if this is the correct answer.\n",
    "\n",
    "But we have the statsmodel regression summary to look at.\n",
    "\n",
    "Comfirming the two values,we confirm that we have worked correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d39e1-413c-4d3d-99b1-0ff7026ded61",
   "metadata": {},
   "source": [
    "Our conclusion is that the adjuste R-squared is considerably lower than the R^2\n",
    "Adj.R2<R*2, therefore one more of the predictors have little or no explanatory power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5f321-7ddd-434f-8795-0f63029a0738",
   "metadata": {},
   "source": [
    "### Feature Selection through p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560bf44-feb4-4771-8ff0-e829c993bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b696e-cc1d-4a65-b970-2c64c99962a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"1.02. Multiple linear regression.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545b299-86d8-471d-ae28-fa653d195655",
   "metadata": {},
   "source": [
    "One of the two variables gotten from the adjustedR-square does not explain \n",
    "\n",
    "the explanatary power of the model, hence , \n",
    "\n",
    "if we can identify which one and remove it, we can further simplify our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791d0c7-c54e-4856-82eb-b3237c0543a6",
   "metadata": {},
   "source": [
    "How do we detect the variables that are unneeded in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655c680-abff-463d-a331-f3b7eab26103",
   "metadata": {},
   "source": [
    "It is called Feature selction:\n",
    "    \n",
    "It is a very important procedure in machine learning as\n",
    "\n",
    "it simplifies models which are much easier to interprete by data scientist.\n",
    "\n",
    "Through this process we gained improved speed and often prevent the series of \n",
    "\n",
    "other unwanted issues arising from having too many issues.\n",
    "\n",
    "Feature selection is already an idea we are already familiar with.\n",
    "\n",
    "When we were dealing with statsmodels, we were given the p-values of the \n",
    "\n",
    "features, which we later used to determine whether the independent \n",
    "\n",
    "variables were relevant for the model.\n",
    "\n",
    "Recall that if a variable have a p-value of above 0.05 we can disregard it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4202a-6b81-41a3-b2f9-fd791d7a8080",
   "metadata": {},
   "source": [
    "How can we find the p-values in sklearn ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca820595-f1bf-4d73-ad97-91e89c4b7b3d",
   "metadata": {},
   "source": [
    "One of the cases is where sklearn packages manifests\n",
    "\n",
    "itself as a machine learning package rather than a statistiscal one.\n",
    "\n",
    "There is no built in method we can call to get them.\n",
    "\n",
    "Therefore,we must find the work around it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea28e9-6cc6-4d2f-9b80-31f06103be9d",
   "metadata": {},
   "source": [
    "A very close concept is available with \n",
    "\n",
    "the feature_selection.f_regression from sklearn, it is called f-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b4b19-a943-4e25-a251-2533fe0f0534",
   "metadata": {},
   "source": [
    "f-regression creates simple linear \n",
    "\n",
    "regressions of each feature and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e0627-9c8a-4b4e-9288-d3de7cb3f293",
   "metadata": {},
   "source": [
    "For our GPA and SAT Score example,\n",
    "\n",
    "it will translate into 2 regressions.\n",
    "\n",
    "1. GPA <- SAT\n",
    "\n",
    "2. GPA <- Rand 1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5a2f8-425e-4140-9c50-e457b5afd8da",
   "metadata": {},
   "source": [
    "Then the method will calculate the  f-statistic \n",
    "\n",
    "for each of those regressions and return the respective p-values.\n",
    "\n",
    "If there were 50 features,50 simple regressions will be created.\n",
    "\n",
    "Note that for a simple linear regression, the p-value of the f-satistics \n",
    "\n",
    "concides with the p-value of the only independent variable.\n",
    "\n",
    "Hence ,this method is pricely what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821b709-7c47-4585-a813-06660532a45a",
   "metadata": {},
   "source": [
    "Hands on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b50e2c-99b1-4608-bf3d-eb324a59d610",
   "metadata": {},
   "source": [
    "From sklearn feature selection we must import f-regression\n",
    "\n",
    "Then we can call the method f_regression with arguement the \n",
    "\n",
    "features contained in x and the target contained in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb26e67-8ae0-4f0c-b3ce-5c637c770bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faae44-dcc1-4e34-b339-6d12976571cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_regression(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd20b9-fec1-43d5-a768-07fcff63276a",
   "metadata": {},
   "source": [
    "Running the code,we have 2 output arrays, the first one is the f-statistic for each of\n",
    "\n",
    "the regressions,\n",
    "(56.048, 7.1995~) and the second one contains the corresponding p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ef4cd-f92a-4d5e-a077-07976313eb43",
   "metadata": {},
   "source": [
    "Generally ,we are interested only in the p-values,\n",
    "\n",
    "Hence, we will declare a new variable called p-values which will be equal to \n",
    "\n",
    "the element at position 1 of the f-regression output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b95555-0bda-4c83-96ed-e0878e8a0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = f_regression(x,y)[1]\n",
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f598bc7-ca75-4c8b-8f36-b169fbab4e2f",
   "metadata": {},
   "source": [
    "You will notice that from the result, the p-values is written in a strange way,\n",
    "\n",
    "this is  ascientific notation that not everyone is aware of.\n",
    "\n",
    "Here is how to  interprete them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44369d-4f14-4bdb-ade3-e2e4c8d68d5a",
   "metadata": {},
   "source": [
    "e-11 means * 10^-11 means / 10^11\n",
    "\n",
    "Similarly :\n",
    "\n",
    "e-1 means * 10^-1 or simply  means \n",
    "\n",
    "/10(divided by 10)\n",
    "\n",
    "Similarly,even for experience users ,it is not always intuitive to look at the scientific notation,\n",
    "\n",
    "Hence, we will round the numbers to 3 digits after the dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36aece0-685c-4e0c-804b-585cdf50be20",
   "metadata": {},
   "source": [
    "lets type p-values .round parenthesis()\n",
    "\n",
    "In the parenthesis we can choose the number of digits after the dot we want to round to .\n",
    "\n",
    "Three is completetly enough for p-value interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3d505-6c2c-4693-80d9-e0a8aeaf57b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c84fcb-e255-4719-8447-e72b593d0e9d",
   "metadata": {},
   "source": [
    "Our new results are (0, 0,6760) the p-values of the two features\n",
    "\n",
    "The first one refers to the first \n",
    "column of x SAT (0.000) \n",
    "\n",
    "and the second one refers to the second column of x (rand 1,2,3) and the p-value is 0.676."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97138ce-8dbe-455c-bea0-d72c6eb03df7",
   "metadata": {},
   "source": [
    "With your prior regression analysis experience you can easily determine that SAT is a useful variable while rand 1,2,3 is useless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b63e26-4cf1-4ea1-aa02-bfc2f6b71580",
   "metadata": {},
   "source": [
    "We have found the p-values  of the regression to this simple work around, but what should be noted though is that these are the uni variable p-values or p-values reached from simple linear models.\n",
    "\n",
    "They did not reflect the ineter connection of the features in out multiple linear regression.\n",
    "\n",
    "Hence, f-regression should be used with caution as it is one bit too simplicit for more complicated problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b73b2f-771d-4836-bb83-1771b41de09d",
   "metadata": {},
   "source": [
    "### Creating a summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154f2f0-76e9-4b60-ae0f-41477790d159",
   "metadata": {},
   "source": [
    "We will start by creating DataFrame called reg_summary. \n",
    "\n",
    "It will contain 2 rows, one for each variable,so for now ,\n",
    "\n",
    "let reg_summary be pd.DataFrame(data=['SAT', Rand 1,2,3)] and a simple column called 'features' and this gives us the names of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ceda20-2511-43ed-a711-f25bb2524e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary = pd.DataFrame(data=['SAT','Rand 1,2,3'], columns=['features'])\n",
    "reg_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37df5ce-29b6-4b4e-af53-e2aecf7a3178",
   "metadata": {},
   "source": [
    "If we want to parameterise our code, we do not have to write the name of the 2 features but rather refer to the column value of the DataFrame x,lets replace this with x.column values, after executing we find out is the result is identical but much more preferable for models with dozens and hundreds of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc0dd7-7069-45f9-889a-6ca6f09b9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary = pd.DataFrame(data=x.columns.values, columns=['features'])\n",
    "reg_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561219a-c9cb-4014-98a0-d391aa76aeff",
   "metadata": {},
   "source": [
    "Next we will create a new series called reg_summary [coefficients] and it will contain the regression coefficients obtained true. reg_coref_\n",
    "\n",
    "we will also add a column called p-values conprising of the p-values we got from the feature selection in our last lesson.\n",
    "\n",
    "What we get is a nice and simple summary table with the features ,coefficients and the p-values.\n",
    "\n",
    "Once more , we conclude that Rand 1,2,3 does not contribute to our model and should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44d2b3-1da9-4472-a22f-bb41b9ab3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary ['coefficients'] = reg.coef_\n",
    "\n",
    "reg_summary ['p-values'] = p_values.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c3ebe-e5c3-4d4a-a55c-9222939c7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077021d-1375-4ac8-8e8a-27c19c80d2be",
   "metadata": {},
   "source": [
    "p-values is one of the best ways to determine if a variable is redundant \n",
    "\n",
    "but they provide no information whatsoever about how useful a variable is.\n",
    "\n",
    "This means that 2 variables may both have a p-value of 0.000 making them \n",
    "\n",
    "relevant but this does not mean that they are equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b16bf0-b579-4e88-861f-f09de0bf1912",
   "metadata": {},
   "source": [
    "### A Note on Calculation of P-Values with sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbebca5-92df-4b1f-bb1e-a720532b55ac",
   "metadata": {},
   "source": [
    "As suggested in the previous lecture, the F-regression does not take into account the interrelation of the features.\n",
    "\n",
    "A not so simple fix for that is to ammend the LinearRegression() class.\n",
    "\n",
    "You can find the relevant code with comments below. \n",
    "\n",
    "Note that the results will be identical to those of StatsModels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1720b98-5f8c-4e9d-909b-860ca2f80ec8",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94670a-d0f7-4a61-9aab-13804f48d8ff",
   "metadata": {},
   "source": [
    "Most commom problem with working with numerical data is the difference in magnitude.\n",
    "\n",
    "An easy fix for this situation is\n",
    "\n",
    "Standardization\n",
    "\n",
    "Feature scaling  or \n",
    "\n",
    "Normalization\n",
    "\n",
    "Normalisation can also refer to a few additional concept with the machine learning and this is why we stick with\n",
    "\n",
    "the terms : Standardization and feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56746f7c-e2aa-4947-86ad-da1e685aefb6",
   "metadata": {},
   "source": [
    "Standardization or feature scaling is the process of transforming the data we are working with into a standard scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aad4cd-7ca0-4ccb-8c08-a307889e082a",
   "metadata": {},
   "source": [
    "This translate to subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac05b0-4125-47b3-ae4c-41c54ab1e696",
   "metadata": {},
   "source": [
    "Standard variable = \n",
    "\n",
    "x - Î¼ / Ïƒ\n",
    "\n",
    "x = original variable\n",
    "\n",
    "Î¼  = mean of original variable\n",
    "\n",
    "Ïƒ  = Standard deviation of original variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21904f6-37a0-40d6-beb4-8735cce870e6",
   "metadata": {},
   "source": [
    "In this way ,regardless of the dataset we will always obtain a distribution with the mean of zero and a standard deviation of 1 which could easily be proven."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae99cdc-1f4e-4d57-a756-972e3749251c",
   "metadata": {},
   "source": [
    "Lets show that with our fx example, say our algorithm have two input variables.\n",
    "\n",
    "Euro/Dollar Exchange rate   \n",
    "\n",
    "and the Daily trading volume\n",
    "\n",
    "We have 3 days work of observation\n",
    "\n",
    "\n",
    "DaY    Exchange rate   Daily trading                              volume\n",
    "\n",
    "1          1.3              110,000\n",
    "\n",
    "2          1.34              98,700\n",
    "\n",
    "3          1.25             135,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30051cef-8900-4d75-abb9-d2fd60e014b7",
   "metadata": {},
   "source": [
    "Lets standardize this figures.\n",
    "\n",
    "we standardize the euro/dollars exchange rate regarding the other euro/dollars exchange rate.\n",
    "\n",
    "The mean is 1.3 while the standard deviation is 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23118c-d139-492c-8d12-b668d085b7c4",
   "metadata": {},
   "source": [
    "Going through the above mentioned transformation, using this formula:\n",
    "\n",
    "x - Î¼ / Ïƒ\n",
    "\n",
    "these values becomes\n",
    "\n",
    "0.07 (1.3)\n",
    "\n",
    "0.96 (1.34)\n",
    "\n",
    "-1.03(1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c4178-3e87-4d8e-bfe4-12c11509f326",
   "metadata": {},
   "source": [
    "If we use same formula to standardize trading volume we obtain the \n",
    "\n",
    "following values:\n",
    "\n",
    "-0.25  (110,000)\n",
    "\n",
    "-0.85  (98,700)\n",
    "\n",
    "1.1  (135,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d12fce-085f-4164-999a-121ddc2e770f",
   "metadata": {},
   "source": [
    "Thus,we have forced figures of very different scale to appear similar.\n",
    "\n",
    "That why another name for standardization is called feature scaling.\n",
    "\n",
    "This will not show our linear combination treat the two variables equally and also is much easier to make sense of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a18e7-7f1f-4776-a811-0d017f30ba03",
   "metadata": {},
   "source": [
    "The transformation of trading volumes \n",
    "\n",
    "allowed us to transform the volumes \n",
    "\n",
    "from 110,000, 98,700,and 135,000 to \n",
    "\n",
    "-0.25, -0.85 and 1.1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42870a0d-334b-4e92-b894-8d1387c6944a",
   "metadata": {},
   "source": [
    "This makes the third term considerable higher than the average while the first one is around the average.\n",
    "\n",
    "We can confidently say that 135,000 trades per day is a high figure while 98,700 is low.\n",
    "\n",
    "Pls disregard the simplification of having just 3 observations as it is  just an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813a71e-7913-47a4-af76-bb329a89f603",
   "metadata": {},
   "source": [
    "OTHER METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe21a0-985c-4d61-8072-b5a6242ae8ad",
   "metadata": {},
   "source": [
    "There are different strategies to deal with differnt magnitudes,\n",
    "\n",
    "but Standardization is probably the most common one.\n",
    "\n",
    "Standardization is the one we will emloy in the pratical example we will face in this course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040dca47-c23e-4e34-8302-1b262fd3f383",
   "metadata": {},
   "source": [
    "##### Feature selection through Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e7df6-5956-45c6-86af-3ec620aa4a8d",
   "metadata": {},
   "source": [
    "##### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305d8f4-df5b-4714-a59f-c4f8ca1e0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f8fb7-2931-409f-9a65-79aa06192de7",
   "metadata": {},
   "source": [
    "##### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be08e9-80c2-4108-a66d-c83c10101b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"1.02. Multiple linear regression.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c71e68-85d2-4622-bb9d-96682c473fd4",
   "metadata": {},
   "source": [
    "Lets pick up our code from the point where we declared the dependent and independent variable.\n",
    "\n",
    "Our plan for this lecture is to scale them using the standard scaler module from sklearn and then create a new regression with scaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bd86e-f4e3-43ae-a265-7e4c959e24ba",
   "metadata": {},
   "source": [
    "Standard scaler() is a processing module used to standardize (or scale) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280925b-6f11-4aec-a357-e7533f407f88",
   "metadata": {},
   "source": [
    "Lets import the relevant module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47498ff5-fb90-4078-81d3-6b7e08eaeee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa7d02-8227-4e46-916b-7e76bb97699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8d4e2-2bf6-4ce6-8327-094038240df5",
   "metadata": {},
   "source": [
    "Please take not of capital letters while importing object.\n",
    "\n",
    "Then declare a standardscaler object scaler = StandardScaler()\n",
    "\n",
    "Note that we have created an empty standard scaler project.\n",
    "\n",
    "\n",
    "The object we just created will be used to scale our data.\n",
    "\n",
    "In other words, it will subtract the mean and be divided by the standard deviation from each point feature wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0600a-13de-43f5-87c0-bcc206322c1f",
   "metadata": {},
   "source": [
    "The next step is to fit our imput data as follows scaler.fit(x)\n",
    "\n",
    "This line will calculate the mean and standard deviation of each feature.\n",
    "\n",
    "Fit calculates the mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbfebef-6da3-4078-b564-bfcb12c3c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde0dc1-ecd5-460f-bf06-573c6b29f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION !!!!!! MY RESULT IS DIFFENT !!!!??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e34e3-61ae-4a62-ba54-221d1e2fbb15",
   "metadata": {},
   "source": [
    "This information :\n",
    "\n",
    "With_mean=True, \n",
    "\n",
    "with_std=True \n",
    "\n",
    "will be stored in the scaler object \n",
    "\n",
    "hence it willl not be an empty object any more ,\n",
    "\n",
    "IT will contain information about the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e927d39-5637-4ae9-8770-e91f6d571c8a",
   "metadata": {},
   "source": [
    "This is very important, whenever you get a new data,\n",
    "\n",
    "you will know that the Standarddization information is contained in the scaler.\n",
    "\n",
    "Hence, you will be able to standardize the new data same way.\n",
    "\n",
    "Remember this as it will be very useful later on.\n",
    "\n",
    "We have the information but the input are still unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ab0e3-bd03-43db-9570-1e6a093a3d79",
   "metadata": {},
   "source": [
    "new_data = pd.read_csv('new_dat.csv')\n",
    "\n",
    "Scaler contains all standardization info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58f1f8-04f9-484e-8642-2633d32835f2",
   "metadata": {},
   "source": [
    "We have just prepared the scaling mechanism scaler.fit(x)\n",
    "\n",
    "In other to apply it we must use another method called transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a6b50-c05b-4b51-af4e-1c237382a90f",
   "metadata": {},
   "source": [
    "StandardScaler.transforn(x) transforms the unscaled inputs using the information contained in the scaler object(feature-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10625651-52bb-44a0-b23f-1cbcaea20305",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6183f30-5e9f-4d0e-ab4d-520399a72ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588571a6-2d53-412d-bf88-d7b95afaea4e",
   "metadata": {},
   "source": [
    "We subtract the mean and divide by the standard deviation for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b63070-9928-41a5-ae6b-4dfd57d97aae",
   "metadata": {},
   "source": [
    "When ever we get a new data we will just apply\n",
    "scaler.transform (new_data)\n",
    "\n",
    "To reach he same transformation as we just did.\n",
    "\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "This the most common and useful way to transform  new data when you employ a module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1941909-0093-4b4c-ae66-eb5552f9db77",
   "metadata": {},
   "source": [
    "From the result above, we can see that all the new dta has been standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92c4e5-5126-4058-a7d0-1f6d4b4a8a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Selection through Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa9a37-cf23-4486-bdc5-785c73441794",
   "metadata": {},
   "source": [
    "in our last lesson we discussed what standardization is but we could not see how it relates to a particular problem.\n",
    "\n",
    "When we calculated the coefficient of this regression without standardization, we couls not immediately see the effect of each variable on the output.\n",
    "\n",
    "The reason is that SAT is ranging between 600 and 2400, while Rand 1,2,3 between 1, 2 and 3.\n",
    "\n",
    "The respective coefficients were 0.0017 and -0.0083.\n",
    "\n",
    "Hence, it seems that 'Rand 1,2,3' has a bigger coefficient , hence a a greater impact.\n",
    "\n",
    "However, this conclusion can be wrong.\n",
    "\n",
    "Since SAT scores has a much greater magnitude,it is much more important!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200eda1-4f9e-4c55-bb5c-b9d40ce6a006",
   "metadata": {},
   "source": [
    "For instance, if Jane has 18000 on the SAT and the randon number assihned to her was 2, then her expected GPA will be 18000 times 0.0017 plus 2 times -0.0083 plus the intercept (0.2960"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e61c2b-f3ae-4c97-bf30-dc2e50dfb668",
   "metadata": {},
   "source": [
    "Linear Model: SAT * 0.0017 + Rand 1,2,3*(-0.0083) + 0.296\n",
    "\n",
    "Jane: 1800*0.0017 + 2*(0.0083) + 0.296\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ce599-b972-40b0-a2a6-7914f2d6a7c4",
   "metadata": {},
   "source": [
    "Obviously, the SAT expression is contributing alot to the final expression overall:\n",
    "\n",
    "1800^0.0017 = 3.06 \n",
    "2 ^*-0.0083 = -0.0166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0b161-cd9a-4bdb-88e2-21fbb3a72fe3",
   "metadata": {},
   "source": [
    "However, by looking at the coeficient alone, one might be deceived.\n",
    "\n",
    "This issue is overcome through feature scaling.\n",
    "\n",
    "Having all inputs with theame magnitude allows us to compare their impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b48cf-b01c-44eb-8632-7bea4ba3a8cb",
   "metadata": {},
   "source": [
    "##### Regression with the scaled features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba816bf-7407-4a78-b9c8-4ec305747694",
   "metadata": {},
   "source": [
    "We will be creating new regression with the scaled input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320321a4-600c-402f-90f5-0fcaf1ad0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x_scaled,y) #To fit scaled input and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd31ae-76cb-4665-9d85-a32a74b08f37",
   "metadata": {},
   "source": [
    "We will get nothing more than a line comfrrming that regression has been suceessfully peformed.\n",
    "\n",
    "To access the actual diference we can look at the coefficients and the intercept using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66df43-b9c5-4610-85a6-92c74fdb4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639e23d-0f3c-41d0-91a8-a947bd901b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010fdee-135e-40c7-9f0d-afd9d23904c2",
   "metadata": {},
   "source": [
    "Before analyzing the result,lets arrange them in a summary table. and this time let us include the intercept as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b788d-6170-43a5-9968-40d38d6af1bf",
   "metadata": {},
   "source": [
    "##### Creating a summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a36efd-433c-44b8-947b-4b00a592383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary = pd.DataFrame([['Intercept'],['SAT'],['Rand 1,2,3']], columns=['features'])\n",
    "reg_summary['weights'] = reg.intercept_, reg.coef_[0], reg.coef_[1]                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7f326-a42b-45ed-9399-269bd8043600",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694445ec-3f63-47ab-8920-0d68e571b639",
   "metadata": {},
   "source": [
    "The term 'weight' is the machine learning word for the term 'coefficient'\n",
    "\n",
    "We have staaaandardized the features which resulted in standardized coefficients or weight.\n",
    "\n",
    "The name comes from the fact that the bigger the weight the bigger the impact of the feature on the regreesion.\n",
    "\n",
    "It carries weight on the result.\n",
    "In thesame line of thought, in the machine learning context, the intercept is called buyers.\n",
    "\n",
    "The idea is that the intercept is nothing but a number that has some  regression wuth some constant.\n",
    "\n",
    "But if we need to  adjust our regression with some constant, regression is biased by that number.\n",
    "\n",
    "Machine Learning Practisioners simlpy call it bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0a934-bcb7-4b47-b76f-48a121d47c9b",
   "metadata": {},
   "source": [
    "Lets change the word intercept in the regression summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed4016-332f-4db6-a5d9-ce98eecbf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary = pd.DataFrame([['Bias'],['SAT'],['Rand 1,2,3']], columns=['features'])\n",
    "reg_summary['weights'] = reg.intercept_, reg.coef_[0], reg.coef_[1]                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50848e20-0893-4fbc-a1ad-827ac8e9f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57757f69-469e-40c6-8260-ae90eefdec24",
   "metadata": {},
   "source": [
    "We have achieved the regression summary which contains the \n",
    "\n",
    "features,weight and the Bias of our regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59bc1e-a94e-4287-a73c-ed620eed361e",
   "metadata": {},
   "source": [
    "Using this new names in  conventions we can conveniently distinguish between a regular summary table and \n",
    "\n",
    "this new one we just created with standardized coefficients and weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3870ecde-90ee-4c91-8f56-2712f32d2844",
   "metadata": {},
   "source": [
    "The interpretation of this weight, \n",
    "\n",
    "the closer our weight is to zero, the smaller is its impact.\n",
    "\n",
    "The bigger the weight the bigger its impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a224bc-1bae-4172-93a3-bff630918f49",
   "metadata": {},
   "source": [
    "In our example , the weight of  SAT is 0.17 \n",
    "\n",
    "which is much much bigger than that of 'Rand 1,2,3'which is -0.007."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7d01e-e633-480f-9050-5b0a3e4cd08a",
   "metadata": {},
   "source": [
    "This brings us to feature selection through standardization.\n",
    "\n",
    "We can clearly see that 'Rand 1,2,3' barely contributes to our output if at all. \n",
    "\n",
    "It will mae little difference if we remove it from our model or leave it there with a weight of almost zero and zero times any value is equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f2c72-6f5b-4c42-8834-8fa3954e5574",
   "metadata": {},
   "source": [
    "This concept is quite remarkeable.\n",
    "\n",
    "When we perform feature scaling, we dont really care if a useless variable is there or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53638b77-92f1-46f0-b4a1-61804e4dba53",
   "metadata": {},
   "source": [
    "this also one of the main reasons why sklearn does not support p-values since most ML practitioners perform some kind of feature scaling before feeding the models.\n",
    "\n",
    "We dont need to identify the worst performing features.\n",
    "\n",
    "They are automatically penalised by having a very small weight .\n",
    "\n",
    "In general, is better to leave out the worst performing features as they interact with the useful ones and may bias the weight even if only slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eab80e-8061-47d1-bda8-e7e2e47eb7ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Making predictions with Standardaized coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eacbebd-e68e-4d5a-bc7d-7ff3354fb516",
   "metadata": {},
   "source": [
    "##### Making predictions with the standard coefficients(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8112ca-a6a6-44a4-b643-6f32f061e172",
   "metadata": {},
   "source": [
    "We will be looking at how to predict values\n",
    "\n",
    "using our standardized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a66cf-11fe-4774-9d55-64210bb6625b",
   "metadata": {},
   "source": [
    "Imagine you got some new data, for \n",
    "\n",
    "simplicity let's create DataFrame with two \n",
    "\n",
    "observations,one is a student who got 1700\n",
    "\n",
    "on the SAT and was assigned no 2 randomly \n",
    "\n",
    "and the other scored 1800 and was assigned\n",
    "\n",
    "1 randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd678316-63fd-4d79-a14b-69ae7a78a8b0",
   "metadata": {},
   "source": [
    "It is crucial that our new data, DataFrame is arranged in the exact same way as our input data.\n",
    "After we comfirm this, we will proceed to predicting the new value.\n",
    "\n",
    "We can simply call the predict method on our regression and specify the new input as an arguement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d2091-512e-4ffe-a62a-d678e2bbff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(data=[[1700,2],[1800,1]],columns=['SAT','Rand 1,2,3'])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b94db8-b812-4363-af3e-cc40ad5a7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9b889-cd96-468c-87b3-4266b059619e",
   "metadata": {},
   "source": [
    "THe result we got is 295.4 and 312.6 and this is not even a valid GPA.\n",
    "\n",
    "This happened because our regression model is trained on standardized input .\n",
    "\n",
    "It expects values that are of thesame magnitude as the one used in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee56c2-62ba-4b34-a328-1507a16db625",
   "metadata": {},
   "source": [
    "We explained earlier that the new DataFrame should be arranged in thesame way as the input x, \n",
    "\n",
    "it must also be standardized in thesame way ,with thesame mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294f4ff-2650-48d7-90e0-763f18016481",
   "metadata": {},
   "source": [
    "But we already have that stored in the scaler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed295467-1512-44c0-a3e3-48b7d1138273",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a70f6-8f9b-4de8-a446-83835783c0c7",
   "metadata": {},
   "source": [
    "Lets use thesame methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526ea01-a550-4ad8-b1c4-1d6e71143bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d62ab-d32d-4014-9814-c74af2fdddca",
   "metadata": {},
   "source": [
    "using th well known method predict, we can find  what the predictions are when we feed the new data scale of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1372e73-eceb-4a0d-9c4d-a30905b0141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_scaled = scaler.transform(new_data)\n",
    "new_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e21e31-db82-40e8-9800-56c114910d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(new_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df5700-d2fa-4641-a1c7-e24b0194b183",
   "metadata": {},
   "source": [
    "##### what will happen if we decide to remove the 'Rand 1,2,3' variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ab60e-3783-4624-aedd-768ebcdd16f8",
   "metadata": {},
   "source": [
    "First,we must create a new regression, i will call it simple as it is a simple linear regression.\n",
    "\n",
    "Next ,we must declare the the input, lets create a new variable called x-simple which will contain all observations from x-scaled from last lecture, but only referring to the SAT column or column 0, \n",
    "\n",
    "If you remember sklearn will return an error if the input are not of matrix form.\n",
    "\n",
    "So lets make it a matrix with the reshape method.\n",
    "\n",
    "Next, we fit the regression with inputs x-simple matrix and output y.\n",
    "\n",
    "Once the regression is fitted, we can proceed to predicting the new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13f53c-11c6-4012-89c2-24d87122017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_simple = LinearRegression()\n",
    "x_simple_matrix = x_scaled[:,0].reshape(-1,1)\n",
    "reg_simple.fit(x_simple_matrix,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeccf55-4615-4e85-83a8-3004d979c491",
   "metadata": {},
   "source": [
    "With arguement only the first column of the scaled new data.\n",
    "\n",
    "it is crucial that we only feed the SAT score because this regression is trained only on it.\n",
    "\n",
    "Moreover ,once again it must be re-shaped so that the code will execute properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f46ac-2b0c-4a5e-afb2-18830df0bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_simple.predict(new_data_scaled[:,0].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94356f-4193-48a3-b7cd-e0e5a251cfd6",
   "metadata": {},
   "source": [
    "If we compar this result from what we got from our multiple linea regression,\n",
    "\n",
    "The predicted GPA is slightly different but if we round up to two digits after the dot, we get the exact same result.\n",
    "\n",
    "3.09 and 3.26\n",
    "\n",
    "This shows us why the develpoers of sklearn has decided that p-values are not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c250843-1dce-4f76-93fd-afac8c749665",
   "metadata": {},
   "source": [
    "When we apply feature scaling, it often does not effect the final result if we keep the real values of the significant features.\n",
    "\n",
    "The weight will be so close to zero that they will barely influence the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57811d8f-fed1-4957-b3fe-8f0c05b5362e",
   "metadata": {},
   "source": [
    "### Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c781027-f183-431b-8592-080a1dc3f329",
   "metadata": {},
   "source": [
    "One of the most commonly ased questions in data science interviews is overfitting and underfitting.\n",
    "\n",
    "A recruiter will casually bring up the topic and as you, 'What is overfitting and how do we deal with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e70f82-d927-4e26-b306-1b7268c8ed89",
   "metadata": {},
   "source": [
    "There are two concepts that are inter related.\n",
    "\n",
    "1. Underfitting\n",
    "\n",
    "2. Overfitting\n",
    "\n",
    "they go together and under standing one ,helps us to understand the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a812ab0-5367-492d-b5e4-283d41e61b2b",
   "metadata": {},
   "source": [
    ">1. Overfitting :\n",
    "\n",
    "This means that our training has focused more on a particular training set so much,it has 'missed the point'.\n",
    "\n",
    "They are so supper good at modelling the data that they fail to come very near to each observation\n",
    "\n",
    "The problem is that the random noise is captured inside an over fitting model.\n",
    "\n",
    "Overfitting is much harder though as the accuracy of the model seems outstanding.\n",
    "\n",
    "There is one popular solution to over fitting.\n",
    "\n",
    "We can split out initial data set into two.\n",
    "Training and Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a497ab8-ab2b-4b88-b141-b1e0a7d20df4",
   "metadata": {},
   "source": [
    "Spits like 90% training and 10% test or 80/20 are common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0fb6b-8533-4898-9064-37498645ca64",
   "metadata": {},
   "source": [
    "How to achieve the split of training and test.\n",
    "\n",
    "We create the regression on a training Data.\n",
    "\n",
    "After we have the coefficients, we test the model on a test data by accesing the accuracy.\n",
    "\n",
    "The whole point is that the model has never seen the test data point so it cannot over-feed on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e643b3-f5c2-4151-8303-9ef1f29ee93b",
   "metadata": {},
   "source": [
    "Pls check note book for graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f1389-2d7a-4801-9aa8-95e341b1ada1",
   "metadata": {},
   "source": [
    ">2. Underfitting:\n",
    "\n",
    "The model has not captured the underlying logic of the data.\n",
    "\n",
    "It doesn;t know what to do and therefore provides an answer thatnis far from correct.\n",
    "\n",
    "Lets explain this with graph as it is more intuitive and cooler.\n",
    "\n",
    "Under fitting models are clumsy and have a lower accuracy.\n",
    "\n",
    "You will quickly realise that either there are no relationship to be found or you need a different model.\n",
    "\n",
    "Underfitting is easy to spot because you have almost no accuracy whatsoever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cf6d8-2daf-4f4a-85c0-d7650fe19ec5",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a765b8e-9936-4c08-9d0f-9b1d5f08dbc8",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e78e5e-55c8-49f2-a30c-2a0ae75c3d21",
   "metadata": {},
   "source": [
    "We train the model on a training dataset and check how well it behaves on a testing one.\n",
    "\n",
    "Ultimately, we are trying to avoid the scenario where the model learns to predict the training data very well but fails misearable in giving the new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4f70f-8a1f-4057-866b-304c393025bc",
   "metadata": {},
   "source": [
    "Now we are going to explore,how to technically perform a split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388ab6b-8a9b-4454-a4d1-1bae98fe0c3b",
   "metadata": {},
   "source": [
    "Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f5a04-1ee4-4952-bea4-9bb3d214fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909b841-1f12-4ec3-84b2-c9085054d63a",
   "metadata": {},
   "source": [
    "GENERATE SOME DATA WE ARE GOING TO SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4507e3e-d33f-4cc8-ba75-e61044ecb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1,101) # a range is a built -in python function range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655c6aa-e639-40a0-ae75-a6a92d47c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0536e-9b13-4ddc-823b-fd4d8530108b",
   "metadata": {},
   "source": [
    "arange is a built -in python function range but places the generated values in an array rather than a list.\n",
    "\n",
    "Therefore,the result is an array with elements from 1 to 100 included"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866fd06-6b54-4500-9d81-dfc2fc200bbb",
   "metadata": {},
   "source": [
    "It is very important that the values inside are arranged .\n",
    "\n",
    "This will help us  agreat deal once the splitting process has been accomplished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0366c81-62c2-47d9-9495-d3a82dc42734",
   "metadata": {},
   "source": [
    "Apart from a we will create another array called b that will range from 501-600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e65d0-81b8-432b-b10c-535c795b8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arange(501,601)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97025d-5c6b-4262-9305-e1f7a103d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c738e5-70c0-4c20-8dfe-391b889f8f08",
   "metadata": {},
   "source": [
    "We will use the method train test split (x) to split the data.\n",
    "\n",
    "Train test split(x) splits arrays or matrices into random train and test subsets\n",
    "\n",
    "Simply put ,it takes an array and splits it into two array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d35a2f-1ef7-4928-b35f-44f3fd03a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc92053-1eb7-463c-bbda-63e20165b6b7",
   "metadata": {},
   "source": [
    "In general, we will prefer storing the results in dedicated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9470be-e2ae-4d34-99ad-2a3fe89b6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train, a_test = train_test_split(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ef7cd-8a53-459d-8e6e-46523fd5fd90",
   "metadata": {},
   "source": [
    "The first array is always considered to be the training array while the second is the testing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b7afd-2d03-40ce-8ef7-021671621c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_train, a_test = train_test_split(a, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc277c-2fa3-4906-aa89-2bbfc7c3c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_train, a_test = train_test_split(a, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7f171-236d-4b9b-8862-1e56469eb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_train, a_test = train_test_split(a, test_size=0.2, random_state=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74f3b7-bc89-4118-9744-84cfa46c0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe88b4-4170-4070-be92-b1cf1defef18",
   "metadata": {},
   "source": [
    "EXPLORE THE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41434e3-295a-4c45-903f-b76341ebf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train.shape, a_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60a63d-e10e-457a-8058-5721bd91b1fd",
   "metadata": {},
   "source": [
    "It makes sense to check the shape of the two variable. \n",
    "\n",
    "A train have a length of 75 while a-test have a length of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29355684-0afb-45dc-9789-8d7fc6bf4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the respective arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a729291-2cd7-407a-85fa-984377d89a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a6e0b-f70c-4e33-a3e8-3da0b78b6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59adec3-f783-4a41-8f22-a548f0aa2e26",
   "metadata": {},
   "source": [
    "There are two important observations that we can spot.\n",
    "\n",
    "First the default split is 75 and 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7ef65-6bb7-46c6-b2f8-b082d1a3ba2f",
   "metadata": {},
   "source": [
    "Second both arrays are shuffled and they use to consist of numbers ordered from 1 to 100 but now are completely randomised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfeb0c8-25d3-492e-a5ce-d4c6109f2e8a",
   "metadata": {},
   "source": [
    "Infact these are the default setting of train-test-split.\n",
    "\n",
    "75/ 25 is not an utmost split for a work, if you dont like dedicating so much of your data to testing.\n",
    "\n",
    "\n",
    "Hence, to amend this ,we can use an arguement called test size and set it to a float between 0 and 1.\n",
    "\n",
    "Therefore to achieve an 80/20 split we need to simply include test size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5839c9-cb29-4042-b378-568d28442a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train, a_test = train_test_split(a, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7a5a5-cefa-4382-b031-66e9255064f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train.shape, a_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87235d67-b577-45e2-8fec-125bfe920577",
   "metadata": {},
   "source": [
    "Going through the cells below we can see that we have successfully created an 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc884f-2558-40de-8183-20ae96082aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9edf4-3636-4eaa-9a65-3d6ea31407bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc74108-fe62-448f-82cf-5870671a1135",
   "metadata": {},
   "source": [
    "Sometimes when working with time series data for instance the order of the array is of utmost important.\n",
    "\n",
    "In this case we prefer our dataset to be split without being shuffled.\n",
    "\n",
    "There is an arguement called shuffle which is set to True by default.\n",
    "\n",
    "Changing that to False will yield a very intuitive result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315f825-6f80-4833-bf1d-89482948fe2b",
   "metadata": {},
   "source": [
    "The first 80 observation of a will be placed in a-train while the last 20 in a-test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61c870-b7e8-4b96-8502-3f2a279ef888",
   "metadata": {},
   "source": [
    "In practice, most of the time we prefer to shuffle the data, this removes time dependency, day of the week effect etc\n",
    "\n",
    "Usually these are not crucial to causaul relationship we are looking for.\n",
    "we are looking for.\n",
    "\n",
    "We can only detoriate our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62db8c-7d96-4a31-926c-2fe4bfaa18c1",
   "metadata": {},
   "source": [
    "Shuffle Arguement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007d056-c781-4045-bec6-ee54f2c76ad4",
   "metadata": {},
   "source": [
    "Now our result are shuffled again.\n",
    "\n",
    "You may have noticed that each time we run the code we get a different shuffle.\n",
    "\n",
    "So data is re-arranged in a random manner.\n",
    "\n",
    "This is another issue for modelling.\n",
    "\n",
    "The main problem is that each time we split the data, we get  different training and testing data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f925f2-64dc-4fa4-a62c-bcc2bc9ea3f6",
   "metadata": {},
   "source": [
    "So if you are working on your models at home, you will be creating a different regression on different data each time you run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159af9d3-0d8b-4243-a96d-4cef371f57b7",
   "metadata": {},
   "source": [
    "Training a model on different data will generally have little impact but it will have one nonetheless.\n",
    "\n",
    "For instance,your R-squared is likely to change with one or two percentage point just because of the split.\n",
    "\n",
    "If you are trying to improve your model with many tidy twist each of which are bringing one or two percent of additional explanatory power, a different shuffle every time will present an objective assessment of the changes.\n",
    "\n",
    "In the best case scenario, you will like to have shuffle data but shuffled in thesame way every time.\n",
    "\n",
    "Fortunately,sklearn has a random state arguement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4abaa-19c4-4ef1-9ecc-d9384b67bd1d",
   "metadata": {},
   "source": [
    "Lets see an example in action,\n",
    "\n",
    "We will add a random state arguement and set it to 42.\n",
    "\n",
    "42 is the conventionally used number by the community.\n",
    "\n",
    "A kind of an internety joke\n",
    "\n",
    "If we try re-running the code again and again , we will always get the exact same shuffled split.\n",
    "\n",
    "If we want to get a diffeent shuffle split ,we will simply change the random  state to a different number.\n",
    "\n",
    "Lets try that out with 365, we got a different shuffle split and no matter hoew many times we re=run the cells, the split doesnt change while the numbers are still randomized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf9da1-b359-49ef-bb7f-bd3d400504da",
   "metadata": {},
   "source": [
    "Finally, we must know that this method have another convenience. \n",
    "\n",
    "train_test_split(x,y) splits array$ or matrice$ into random,train and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714371f6-985d-47da-be6c-8717ea0a01c7",
   "metadata": {},
   "source": [
    "We can split more than one array at thesame time. \n",
    "\n",
    "Here is where our b-array comes into play\n",
    "We can simply add b as an arguement to include two new variables to store the returned arrays.\n",
    "\n",
    "We call them b-train and b-test.\n",
    "\n",
    "By exploring the shapes and the arrays and themselves we reach consistence result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448971f-7bb2-4802-9ef0-97577616960d",
   "metadata": {},
   "source": [
    "a consists of the order sequence of the numbers from 1 to 100 while b  is from 501 to 600.\n",
    "\n",
    "Therefore we can say that the number 1 from a matches with 501 from b, 25 from a matches with 525 from b and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb70be1-7c89-4b8c-bf34-987d878c9f4b",
   "metadata": {},
   "source": [
    "When we split a and b using the train test split, the elements are shuffled in thesame way. \n",
    "\n",
    "Exploring a-train and b-train shows that they are indeed matching.\n",
    "\n",
    "This is extremely important for regression because we want a certain observation input to match with this target even after shuffling.\n",
    "\n",
    "In our practical example, we will use this knowledge in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23579623-f1b7-4b77-a9e1-ca0f7fc461c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train.shape, b_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cec019-3ebe-4fa0-a364-065ab6d4c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703449a4-a51d-46d8-910b-06f4c9f51384",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c7a89-60c4-4558-902c-0ef9d2acea04",
   "metadata": {},
   "source": [
    "### Linear Regression Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8fa81-be10-44e2-97d8-88ff1e3ab192",
   "metadata": {},
   "source": [
    "### Linear Regression Practical Example (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f156b-a65d-47c0-b06a-38824308b403",
   "metadata": {},
   "source": [
    "We will deal with real life examples about car sales.\n",
    "\n",
    "It is so real life that it is very messy and we need to first\n",
    "\n",
    ". clean our dataset then go through sevsral assumptions.\n",
    "\n",
    ". Relax some assumptions \n",
    "\n",
    ". Practice the concept of the log transformation we mentioned earlier.\n",
    "\n",
    ". Create a model\n",
    "\n",
    ".Then finish up with some dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67fe42-e232-4f01-bea8-0dbdf08a805f",
   "metadata": {},
   "source": [
    "Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2f995-baee-46e2-86f3-ce0a7b7590d5",
   "metadata": {},
   "source": [
    "Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3765fd-89ba-4471-9d2a-76c81cdf6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce7a68-da59-4059-8f7e-4cb9caafebc3",
   "metadata": {},
   "source": [
    "Our dataset is in the file 1.04 real life example.\n",
    "This is a list of second hand cars with their respective price ,type of body,mileage, engine volume,engine type ,year of production,model and information about the time when they were registered.\n",
    "\n",
    "What we would like to do is to predict the price of the used car depending on its specifications.\n",
    "\n",
    "The first potential regression is brand as it is well known that a BMW is generally more expensive than a toyota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447fc3f-9445-4bbc-8cf5-f66195851805",
   "metadata": {},
   "source": [
    "The second relevant variable is mileage.\n",
    "\n",
    "since the more a car is driven, the cheaper it should be.\n",
    "\n",
    "Third is the engine volume,sports cars has larger engines and economy cars has smaller engines.\n",
    "\n",
    "The final variable is the year of production.\n",
    "\n",
    "The older the car the cheaper it is .\n",
    "\n",
    "With the exemption of vintage vehicles.\n",
    "\n",
    "The rest are categorical variables which we will deal with on a case by case bases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9e23a-6621-436f-a60c-fb08dab56a80",
   "metadata": {},
   "source": [
    "We haven't spoken about data cleaning so far in this course because all datasets were cleaned before it was presented .\n",
    "\n",
    "This time ,it will be different and we will get a feeling about how raw data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb1017-3be0-46bc-a86d-cfdff945901c",
   "metadata": {},
   "source": [
    "##### loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92cecd-1a8f-4d30-b2b5-ad0607aa791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\python files - Copy\\\\1.04. Real-life example.csv\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc8562-525a-4544-b510-35612de1a650",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83887a01-1daf-4996-8536-26d118d803f6",
   "metadata": {},
   "source": [
    "##### Exploring the descriptive statistics of the variationsÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f4f3b-e83f-42f8-be10-2dedaafa33ee",
   "metadata": {},
   "source": [
    "An easy way to check your data and spot problems is to use a discriptive statistics as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cba20c-84d7-47cf-93e0-9c059c23dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7bb86a-31c0-4bd7-9f6c-23875925e114",
   "metadata": {},
   "source": [
    "If you observe, we only got descriptive for numerical variables.\n",
    "\n",
    "This is what we obtained by default.\n",
    "\n",
    "Hence, to get data describing all the dataset, we need to add the aurguement ,include equals all. with this all the categorical variables will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c73a2-462b-436b-93a7-a20428c84c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845fa7f-453e-4166-80b3-c973e3aa8a31",
   "metadata": {},
   "source": [
    "The first thing we noticed is that each variable have a different number of observations.\n",
    "\n",
    "This implies that there are some missing values.\n",
    "\n",
    "Next we can see unique enteries for categorical variables.\n",
    "\n",
    "There are 312 unique models, that is someting that is hard to implement in a model. that will mean more than 300 dummies.\n",
    "\n",
    "Another piece of information we obtained is the most common category.\n",
    "\n",
    "We can see the frequency in the dataset reg for instance has 3947 'yes' enteries.\n",
    "\n",
    "Almost all of them,it looks like this variable will not be very useful.\n",
    "\n",
    "As we mentioned reliably implementing model into a regression will be a very hard task, so we drop the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1103d-572f-4f59-9c77-cd76a6e99f39",
   "metadata": {},
   "source": [
    "Alot of information from model could be engineered 'Brand','Year' and 'Engine volume' so we will not be loosing too much variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b6ffd-810c-4579-be7b-3ddfa003d67f",
   "metadata": {},
   "source": [
    "Once we are done with this practical example, we can try running a regression with model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41f82e-716c-48d6-84ff-f6e8769e9a73",
   "metadata": {},
   "source": [
    "#### Determining the variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e9a95-27ed-493c-94ef-f9fa3d860da0",
   "metadata": {},
   "source": [
    "To remove the variable, we can use the pandas method drop, \n",
    "\n",
    "It has 2 arguments,the columns we want to drop and the axis.\n",
    "\n",
    "Remember that axis zero means rows and axis one stands for columns.\n",
    "\n",
    "We are dropping columns so axis = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81336584-da0a-48d9-afe1-6c24285264dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.drop(['Model'],axis=1)\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c912d-8474-4602-8617-ac7a3965dfb2",
   "metadata": {},
   "source": [
    "##### Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41663dc-eb19-4733-b6e4-1c6fbef91add",
   "metadata": {},
   "source": [
    "Lets examine what is left in terms of variables.\n",
    "\n",
    "We still haven't addressed the missing values problem.\n",
    "\n",
    "Brand,Body,Mileage, Engine type and Year seems to have no missing values given that the total number of observations is 4345.\n",
    "\n",
    "We cant say the same thing for Price and Engine volume though.\n",
    "\n",
    "An easy way to check for missing value is to write data.isnull , it shows us the dataframe with that information\n",
    "\n",
    "True stands for missing value while false stands for available one.\n",
    "\n",
    "Since true is basically 1 while false is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b16024-43bc-4e71-9f9e-7dcd7b003b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0135c23-29b9-40dc-8a32-ec6136f79cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can write data.isnull().sum()\n",
    "This method will sum all the missing value and give us the number of null observations.\n",
    "\n",
    "Price and engine volumes are the only features with missing values but only several of them .\n",
    "\n",
    "Therefore , we will simply delete those observations.\n",
    "\n",
    "A rule of thumb is that if you are removing less than 5% of the observations, you are free to remove all observations that have missing values.\n",
    "\n",
    "In this case, they are less than that.\n",
    "So we safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb73f6c-5000-463c-a4d6-9d41f51475df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740040eb-301c-4bad-a4ec-6bbdb20fe1da",
   "metadata": {},
   "source": [
    "The easiest way to remove them is be using the ad-hoc method\n",
    "\n",
    "data_no_mv = data.dropna(axis=0)\n",
    " data_no _missing value, mv means missing value) \n",
    " \n",
    "Here we will be droping observations and not columns.\n",
    "\n",
    "After that we print the descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18018838-41cf-4bee-a95b-23f11e1e2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_mv = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba154c97-a5db-445e-97b3-7cf1601a0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_mv.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3521427-7af2-4a8e-8498-a88fa5e297eb",
   "metadata": {},
   "source": [
    "##### Exploring the PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c673a-8f25-45de-aa36-67d1c514b34c",
   "metadata": {},
   "source": [
    "Notice the minimum and maximum value with respect to mean and quartile for each variable.\n",
    "\n",
    "We will print the probability function for each feature as we discuss them.\n",
    "\n",
    "It is fairly easy to do that with seaborn.\n",
    "\n",
    "We can write sns.displot() feature of interest .\n",
    "to plot price(data_no_mv['Price']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28331b9d-3274-43ce-ac1f-3a481ee6d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_no_mv['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbedd5f-96fb-477d-a41e-3a54e921d060",
   "metadata": {},
   "source": [
    "For the optimal result ,we will be looking for a normal distribution.\n",
    "\n",
    "Price however has an exponential one, and this will surely be a problem for our regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b75309-fd7f-4896-bfe6-87429948dc09",
   "metadata": {},
   "source": [
    "Looking at the descriptive dataset ,price has a mean of about $19,000,the minimum price is about $600,000 , 25% of prices around $7000, 50% of price is below $11500 and 75% are lower than $21900000 and the maximum price is $300,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60762020-c2ed-496d-93b4-189f4c3c9050",
   "metadata": {},
   "source": [
    "Something strange is going on, obviously we have a few outliers in the price variable.\n",
    "\n",
    "Outliers are observations that lies at abnormal distance from other observations in the data.\n",
    "\n",
    "They will affect the regression dramatically and cause coefficients to be inflated as the regression will try to place the line closer to those values.\n",
    "\n",
    "One way to deal with that seemlessly is to remove the top 1% of observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997331-31ae-4d25-9666-0e07b64e1bac",
   "metadata": {},
   "source": [
    "##### Dealing with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d1988-046d-46ba-981f-6ceffbea29cb",
   "metadata": {},
   "source": [
    "The simplest way to do that with code is through the quartile method.\n",
    "DataFrame.quartile(th quartile) returns the value at the given quartile(= np.percentile) pandas \n",
    "\n",
    "It takes one arguement, the quartile, you can input values from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ceabd-f4b7-4ba8-9696-8b04e93a423e",
   "metadata": {},
   "source": [
    "If you input 0.25 you will get the 25% percentile.\n",
    "\n",
    "We want to get the 99 percentile nd keep the data below the 99 percentile.\n",
    "\n",
    "This is one way to eliminate outliers.\n",
    "\n",
    "Lets declare a new varible called \n",
    "q = data_no_mv['Price'].quartile(0.99)\n",
    "\n",
    "Note that this is actually a value around $130,000\n",
    "\n",
    "Then we will create a new DataFrame ,where we will keep those enteries of price which are lower than the 99 percentile value\n",
    "\n",
    "Notice that this code is extremely re- useable.\n",
    "\n",
    "lets see the discriptives now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8b260-ce84-4d40-bc1a-fef33a694c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = data_no_mv['Price'].quantile(0.99)\n",
    "data_1 = data_no_mv[data_no_mv['Price']<q]\n",
    "data_1.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97ca29-9012-43e0-8eda-3f8b3c08996c",
   "metadata": {},
   "source": [
    "Notice that the maximum vale is around $130,000 or the 99 percentile.\n",
    "\n",
    "While still far away from the mean, it is acceptably closer.\n",
    "\n",
    "Note that in general, we dont  print descriptives and DataFrames all the time.\n",
    "\n",
    "you do it once and just manipulate your data with code until you are done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fa790-6b2c-47f3-a604-e70c43881390",
   "metadata": {},
   "source": [
    "Every now and then ,you can check what is going on.\n",
    "\n",
    "But you rarely do that later when you become more experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115afc0d-d0b2-453c-8d96-4b7473bc5e03",
   "metadata": {},
   "source": [
    "What about the PDF ?\n",
    "\n",
    "It is comfirming what we observed from the descriptive statistics alone.\n",
    "\n",
    "The data is still distributed in the same way with less outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa17aa-86c8-48e8-adfe-f0a61e330fa1",
   "metadata": {},
   "source": [
    "We have a similar issue with mileage, engine volume and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e1b50-1669-4bef-9ffe-10c465f521ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_no_mv['Mileage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf446ae0-7585-432c-8b6d-e1d2bdcb1398",
   "metadata": {},
   "source": [
    "WE can see that the situation with \n",
    "\n",
    "mileage is almost identical from its PDF.\n",
    "\n",
    "Therefore, we can deal with it in \n",
    "\n",
    "thesame way by keeping only the data\n",
    "\n",
    "lower than the 99 percentile.\n",
    "\n",
    "See below the improved result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb379c-407e-4e59-bab0-c2f76607863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = data_1['Mileage'].quantile(0.99)\n",
    "data_2 = data_1[data_1['Mileage']<q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc7c15-32a3-4db2-a96c-69123863884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_2['Mileage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb366f7-aa6a-42e1-82fd-ce6ece38b1d9",
   "metadata": {},
   "source": [
    "PDF for engine volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f89c0e-4653-4ff4-950f-487866aad2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_no_mv['EngineV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24fef05-a1f2-469c-9ce8-82b9fb718fdf",
   "metadata": {},
   "source": [
    "It is rather strange, if we are to \n",
    "\n",
    "examine engine volume manually we will\n",
    "\n",
    "also observe that there are engine \n",
    "\n",
    "volumes values of 99.99,90,75 etc\n",
    "\n",
    "These are not usually values,\n",
    "\n",
    "Moreover,looking at the discriptives it \n",
    "\n",
    "seem as if most values are really low.\n",
    "\n",
    "A quick search on google will confirm \n",
    "\n",
    "that the interval on engine volume  \n",
    "\n",
    "should falls is between 0.6 and 6.5.\n",
    "\n",
    "Therefore,those 99.99,90 ,75 etc are \n",
    "\n",
    "incorrect entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad89c9-da56-4e95-9796-742bb64e5b87",
   "metadata": {},
   "source": [
    "Actually, a common way to label missing values is by assigning 99.99.\n",
    "\n",
    "This startes from the earliy ages of computer.\n",
    "\n",
    "It is a bad idea to label values in such ways as it is very hard for other users of the data to distinguish them from the true values.\n",
    "\n",
    "However, some people still do it.\n",
    "\n",
    "We have a bench mark 6.5,\n",
    "\n",
    "hence , we will remove all values that\n",
    "\n",
    "are lower than 6.5 since we know the \n",
    "\n",
    "natural domian of the engine volume values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c04b8c-3b77-4774-9874-a7bdfe7d9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = data_2[data_2['EngineV']<6.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1046341-8233-4b30-9647-1e7305fe3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_3['EngineV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c41563-871a-4227-a356-7a878fd90432",
   "metadata": {},
   "source": [
    "PDF of year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7d970-8b91-4380-b139-94bf40a76e16",
   "metadata": {},
   "source": [
    "The problem with  year is that it is on the low end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa89ac-6744-47fe-91c2-344c7f77f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_no_mv['Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445b1f9-6f0b-4562-934b-cf1ff3e4a87b",
   "metadata": {},
   "source": [
    "Most cars are newer but there seems to be a few vintage cars.\n",
    "\n",
    "Hence,we will take the first percentile and keep all observations that are higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ea388-465c-4b99-b53f-a36a7e9b2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = data_3['Year'].quantile(0.01)\n",
    "data_4 = data_3[data_3['Year']>q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a7adf-5549-44c9-9b0e-a1a96f7e1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_4['Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1503d25-cd1f-4e7f-825f-fc0686953669",
   "metadata": {},
   "source": [
    "Creating a data variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d40d3d-1822-41e1-8999-5889dbb546df",
   "metadata": {},
   "source": [
    "We will create a data variable and reset the index.\n",
    "\n",
    "We will call the variable data_clean as we have successfully cleaned it.\n",
    "\n",
    "Currently the indices of the dataframe refers to all the data but we want them to describe only the data useful for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934475b-aff9-42b1-8e21-9c729521c84b",
   "metadata": {},
   "source": [
    "The mothod we will use for reseting the index is dataframe.reset_index. we will include an aurgueent through out equals True to completely forget the old index\n",
    "\n",
    "Then we will print the discriptive statistics again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72959607-e680-48ff-85b8-ce2f579e35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2e606-3c94-4f46-ab0f-682be7a4f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89b10d-ab60-4960-87c3-5ec713cdc09a",
   "metadata": {},
   "source": [
    "Now all the minimum and maximum seems about right.\n",
    "\n",
    "Overall, we have deleted about 250 observations and they were the problematic ones,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4562e7-e069-4a9e-a438-b9cc2bae3e41",
   "metadata": {},
   "source": [
    "##### Checking the OLS assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f70ca7-433b-48f4-b229-a6e2938325c9",
   "metadata": {},
   "source": [
    "We will be exploring the OLS assumptions.\n",
    "We have already identified some variables as potential  regressors\n",
    "\n",
    "The categorical ones will be included as dummies.\n",
    "We dont need to worry about them when checking the assumptions.\n",
    "\n",
    "The continuous variables, Price,Year, Engine volume and mileage are those that are likely to be more challenging and causes more problems\n",
    "\n",
    "Lets check for  linearity using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4390988-a03d-4528-998b-f4108e202825",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))\n",
    "ax1.scatter(data_cleaned['Year'],data_cleaned['Price'])\n",
    "ax1.set_title('Price and Year')\n",
    "ax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])\n",
    "ax2.set_title('Price and EngineV')\n",
    "ax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])\n",
    "ax3.set_title('Price and Mileage')  \n",
    "                                                  \n",
    "plt.show()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61025939-870f-4b3f-917a-f8365d89f4de",
   "metadata": {},
   "source": [
    "These are the three scatter plot that we obtained.\n",
    "\n",
    "One for price and mileage, one for price and year and one for price and Engine Volume.\n",
    "\n",
    "We can spot patterns but definetely not  a linear one. We know we can not run a linear regression in this case.\n",
    "\n",
    "We should first transform one or more variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067fa8f-0422-4278-aab5-685ec936ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_cleaned['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c9fc4-b394-4224-b01b-f179401ab82c",
   "metadata": {},
   "source": [
    "We call the distribution plot, Price as well. \n",
    "\n",
    "All these plots are pointing us towards the idea that price is not normally distributed and from there its \n",
    "\n",
    "relationship with the other rather not normally distributed feature is not linear.\n",
    "\n",
    "The patterns are quite exponential.\n",
    "Like we learnt earlier, log transformation is a common way to deal with this issue.\n",
    "\n",
    "It is especially useful whe facing exponential plot like we do now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92eb5a-6afb-4c5f-9421-2c3eca8a7877",
   "metadata": {},
   "source": [
    "Exponential growth is a function that shows an increase within a population that occurs at the same rate over time. When populations experience doubling or tripling in numbers, you can assume the data increases exponentially. The opposite of exponential growth is exponential decay, where data shrinks rather than grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afd338-9533-4df7-a031-5ee166042537",
   "metadata": {},
   "source": [
    "Lets  take the log of price and create the plot once again.\n",
    "\n",
    "Numpy has a method that calculates the natural log.\n",
    "\n",
    "It is np.log(x) returns the natural logarithm of a number of array of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cb51d-b467-4722-890f-6bf2acb1fdec",
   "metadata": {},
   "source": [
    "##### Relaxing the assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0de25a-eaab-4b3c-ba5e-fe5fdc87ba5d",
   "metadata": {},
   "source": [
    "Lets declare a  variable called \n",
    " \n",
    "log_price = np.log of data_cleanen['Price']\n",
    "\n",
    "We will then add it as a new column at the end of the dataframe,\n",
    "\n",
    "which for us will be log _price = the log_price variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d86b12-3419-4b17-a16c-55c6b7dbb566",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_price = np.log(data_cleaned['Price'])\n",
    "data_cleaned['log_price'] = log_price\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000c53e-81b1-472e-8834-fe5335c4ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "We will now plot the graph again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b75147-8f1f-4108-b47c-5d522fe92436",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))\n",
    "ax1.scatter(data_cleaned['Year'],data_cleaned['log_price'])\n",
    "ax1.set_title('Log Price and Year')\n",
    "ax2.scatter(data_cleaned['EngineV'],data_cleaned['log_price'])\n",
    "ax2.set_title('Log Price and EngineV')\n",
    "ax3.scatter(data_cleaned['Mileage'],data_cleaned['log_price'])\n",
    "ax3.set_title('Log Price and Mileage')  \n",
    "                                                  \n",
    "plt.show()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d65aa4-c6f2-4e11-84e3-6a56953b8d58",
   "metadata": {},
   "source": [
    "We can see a linear pattern in all plots now.\n",
    "\n",
    "The lines fit the data so much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c6388-7cfb-4066-9f38-9877df959600",
   "metadata": {},
   "source": [
    "Now lets drop the original price variable from the dataframe since it is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6578508-3773-4031-9e29-07825ba363a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned.drop(['Price'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c11b0-99e5-40a8-aa93-15a661803e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5fda7-2fb0-4bb0-a6b4-5c4165f4c99e",
   "metadata": {},
   "source": [
    "We usually deal with the' NO ENDOGENEITY' assumptions.\n",
    "\n",
    "There are text to check if it is violated or one can take the residuals and find the correlation with each independent x.\n",
    "\n",
    "The assumption is not violated so if of no interest.\n",
    "\n",
    "Moreover,we will have the opportunity to discuss them after the regression is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1c01f-7898-416b-87c8-db437ad2174c",
   "metadata": {},
   "source": [
    "The third assumption have several parts.\n",
    "\n",
    "NORMALITY AND HOMOSCEDASTICITY:\n",
    "\n",
    "Normality: assummed for a big sample following the central limit theorem.\n",
    "\n",
    "Zero mean : the zero mean of the distribution of error is accomplished to the inclusion of the intercept in the  regression.\n",
    "\n",
    "HOMOSCEDASTICITY:\n",
    "\n",
    "The Homoscedasticity assumption  generally holds as we can see in the graph\n",
    "\n",
    "The reason for that is because we have already implemented a log transformation which is the most common fit for hemoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759a4c4-b959-45a9-b86e-b4c8757319f1",
   "metadata": {},
   "source": [
    "NO AUTOCORRELATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b513a-61b5-44f5-a5fe-53dd5299b727",
   "metadata": {},
   "source": [
    "The fourth assumption is no autocorrelation.\n",
    "\n",
    "We dont really need to put much effort in that.\n",
    "\n",
    "The observations we have are not coming from time series data or pandas data.\n",
    "\n",
    "They are simply a snapshot of the current situation of the second hand car sales website.\n",
    "\n",
    "Each row comes from a different customer who is willing to sell their cars with the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431fca6-2f05-4ed8-beb7-1252f3b278f7",
   "metadata": {},
   "source": [
    "Logically there is no reason for the observations to be dependent on each other.\n",
    "\n",
    "Hence, we are safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ccb6c-c33f-4cb9-a0c8-111be3a465db",
   "metadata": {},
   "source": [
    "##### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de63ffc-53f1-4425-a7ff-99f7b514aa09",
   "metadata": {},
   "source": [
    "We will need to check for Multicollinearity.\n",
    "\n",
    "It is logical that year and mileage are going to be correlated.\n",
    "\n",
    "The  newer the car the lower its Mileage, therefore we have grounds to expect some degrees of Multicollinearity in the data.\n",
    "\n",
    "Unfortunately, sklearn is a not a dedicate method to check if this asssumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d89022-5d34-4d8f-9210-982a59b0faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608d6f6-6c99-4023-8e5f-473eab94c846",
   "metadata": {},
   "source": [
    "We must turn to our old friend, statsmodels.\n",
    "\n",
    "One of the best ways to check for Multicollinearity is through VIF  or variance inflation factor.\n",
    "\n",
    "VIF produces a measure which estimates how much larger the square root of the standard error of an estimate is \n",
    "\n",
    "compared to a situation where the variable is completely uncorrelated with the other predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a940be-32f9-4d30-ba50-403f3037a7c9",
   "metadata": {},
   "source": [
    ". First,we import the VIF method from statsmodels\n",
    "\n",
    ". We define the features we want to check for Multicollinearity.\n",
    "\n",
    ". Currently we have only three continuous variable, Mileage, engine volume and year.\n",
    "\n",
    ". What is left are categorical variables that we have not dealt with yet.\n",
    "\n",
    ". Hence , we will simply take these three and evaluate their inter-correlation.\n",
    "The rest is syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a1511-3e62-4413-93b1-50a5e3bda263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "variables = data_cleaned[['Mileage','Year', 'EngineV']]\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif[\"features\"] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71085db4-0a61-49d1-b049-b099d72f0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e35c4-afd3-4e7f-9c13-756f152af9ff",
   "metadata": {},
   "source": [
    "Now lets focus on the resulting variance inflation factor.\n",
    "\n",
    "When VIF is 1 : no Multicollinearity\n",
    "This is also the minimum value oof the measuere.\n",
    "\n",
    "VIF E [ 1, +inf)\n",
    "\n",
    "Generally values between 1 and 5 are considered perfectly ok.\n",
    "\n",
    "1 < VIF < 5: perfectly okay\n",
    "\n",
    "Here, is where it becomes tricky.\n",
    "Some sources say that VIF above 5 is unacceptable\n",
    "\n",
    "5 < VIF : unacceptable.\n",
    "\n",
    "Other but the boundary at 6 while others says 10 is the cutt off line\n",
    "\n",
    "10 < VIF: unacceptable.\n",
    "\n",
    "Unfortunately, there is no firm concensus on the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980c611-8555-4470-8405-ca260673e5bf",
   "metadata": {},
   "source": [
    "From experience,is better to define data so perfect that all features have a VIF below 5.\n",
    "\n",
    "That is some how conservative.\n",
    "\n",
    "From our example ,it seems like year is definitely too correlated with the other variables .\n",
    "\n",
    "Hence , we will remove year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2388b1-0fef-484a-8293-4b1d6b2b1742",
   "metadata": {},
   "source": [
    "Lets declare data_no_multicollinearity =\n",
    "\n",
    "data_cleaned .drop year on axis =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab19c9d-1c0f-4381-a021-7cfe41746357",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_multicollinearity = data_cleaned.drop(['Year'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab23c23-ffb6-4783-8592-ed285b9d3b7a",
   "metadata": {},
   "source": [
    "### A Note on Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e4355-dbbe-40e2-9f5c-50b0242db4e7",
   "metadata": {},
   "source": [
    "What most people are wondering is not 'when do we have multicollinearity' as we usually do have some. The better question to ask is 'When can we safely ignore multicollinearity'.\n",
    "\n",
    "\n",
    "Here's a great article on the topic: \n",
    "http://statisticalhorizons.com/multicollinearity\n",
    "\n",
    "***\n",
    "Regarding the variance inflation factor method that we employed from StatsModels, you can read the full documentation here: https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a6d5d-600d-4574-8fc9-2a9d31b225d6",
   "metadata": {},
   "source": [
    "### Linear Regression Practical Example (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56f68b-1ba3-47b4-a529-740dccc4387d",
   "metadata": {},
   "source": [
    "##### Create Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4fe3d-fa64-4bb0-98c0-6a5c83642d5c",
   "metadata": {},
   "source": [
    "Here we will be discussing on how to code more than one dummy using the get dummy pplication in pandas\n",
    "\n",
    "pd.get_dummies(df[,drop_first]) spots all categorical variables and creates dummies automatically pandas.\n",
    "\n",
    "Get dummies spots all categorical variable and creates dummies automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f6f0e-3bc5-4138-88cf-dcb5b6159abd",
   "metadata": {},
   "source": [
    "An extremely important note is that if we have if we have N categories for a feature, we have to create N-1 dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb9af3-6edf-4bd8-a38c-8c44f7fb4b46",
   "metadata": {},
   "source": [
    "lets consider brands.\n",
    "Audi\n",
    "\n",
    "BMW\n",
    "\n",
    "Mercedez\n",
    "\n",
    "Mitsubishi\n",
    "\n",
    "Renault\n",
    "\n",
    "Toyota\n",
    "\n",
    "Volkswagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ac2f2-5309-4aa0-8bb1-647ab5fb13df",
   "metadata": {},
   "source": [
    "BMW dummy will be 1 if a brand is BMW and 0 if it is not.\n",
    "\n",
    "Merceddez dummy will be 1 if it is Mercedez and 0 if it is not and so on until we get to Volkwagen.\n",
    "\n",
    "For the first one(Audi),we will not creat a dummy variable.\n",
    "\n",
    "The reasoning is as follows: if all other dummy variables are zero, is clear that the car is an Audi\n",
    "\n",
    "If we include a seperate variable called Audi, we will introduce multicollinearity to the regression as the Audi dummy will be perfectly determined by the other variables.\n",
    "\n",
    "Thus if we have N categories for a feature, we have to create N-1 dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec0dd8-bf18-45e4-b68a-f49c9548c87a",
   "metadata": {},
   "source": [
    "Let's create a new variable called Data with dummies = pd.get_dummies(data_no_Multicollinearity ,then add an additional arguement drop_first= True  \n",
    "\n",
    "This is in order to make sure no dummy is created for Audi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f6b29-af08-4e89-90bf-0c064ac281ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1c3ef-e2b0-4665-b3f6-fa9d4fed2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d331ac2-88ca-42da-b3da-48e7872d0f66",
   "metadata": {},
   "source": [
    "From the newlt created feature we can see not only brand but also other categorical features namely body , Engine type and registration has been replaced with other dummies .\n",
    "\n",
    "This is very convenient for us as we needed one line of code to preprocess all categorical features.\n",
    "\n",
    "To reinforce both the VIF concept and the dummy creation see below some home works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c3287-759a-4207-935b-c2888c23d91c",
   "metadata": {},
   "source": [
    "Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24272cd-ff61-4f89-a5aa-9f2e97940284",
   "metadata": {},
   "source": [
    "1. Check the VIF of the features includin the dummies.\n",
    "\n",
    "2. Find the VIF of a set of features where you have not dropped one of the dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511a04dc-56c9-4d60-8c5f-db6c2a42593c",
   "metadata": {},
   "source": [
    "REARRANGE A BIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a24c1-1cd1-4a61-8748-a0d2524b96ff",
   "metadata": {},
   "source": [
    "We will re-order the column so that the dependent variable is the first one in the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c9a0ac-e5df-4615-9626-f1379b8bd837",
   "metadata": {},
   "source": [
    "The easiest way to do it is the manual way.\n",
    "data_with_dummies.column.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f7c84-baa5-457e-82aa-e8022b789532",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a848b6-a9d8-4fa1-ae5c-165b22463612",
   "metadata": {},
   "source": [
    "Next we create a new variable called cols , then we arrange these column name and rearrange them as we wish.\n",
    "\n",
    "We can simply place log _price in the first place an leave the rest on changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035d817-c0d0-45e2-8e41-712d7fd82d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cols=(['log_price', 'Mileage', 'EngineV', 'Brand_BMW',\n",
    "       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',\n",
    "       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',\n",
    "       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',\n",
    "       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes'])\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fef2b-dd6f-4308-aae6-ee1036da80cb",
   "metadata": {},
   "source": [
    "Finally we will store this in a new DataFrame called Data preprocessed.\n",
    "\n",
    "So data preproceed = data with dummy\n",
    "\n",
    "and we will have a re-arranged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cf8c8-baff-4f52-aade-b713661e1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = data_with_dummies[Cols]\n",
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a875d-c4bd-48a7-8068-7d682c305586",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression Practical Example (Part 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25fc4b-59c3-4498-b974-9841003057c3",
   "metadata": {},
   "source": [
    "##### Linear RegressionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9720e-51c5-431f-aa13-e07f0760b43c",
   "metadata": {},
   "source": [
    "##### Declare the inputs and the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858646f-4f20-4735-ba46-a1a7bf991553",
   "metadata": {},
   "source": [
    "In other to create our regression, we must start by declaring the input and targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30b707-87d1-430c-81e2-ec53166c81d9",
   "metadata": {},
   "source": [
    "Our target is the log_price, so we can simply write data_preprocessed [log_price]\n",
    "\n",
    "our inputs are data_preprocessed without log price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e53ff6-d0a0-4581-b9a9-f53fef38caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = data_preprocessed['log_price']\n",
    "inputs = data_preprocessed.drop(['log_price'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af0352-e765-4eef-b746-dd01f50f3405",
   "metadata": {},
   "source": [
    "##### Scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc63f8-791e-47d4-aa03-487aecc437f0",
   "metadata": {},
   "source": [
    "Next,we need to scale our data,we have already seen how to do that with standard scaler.\n",
    "\n",
    "first we need to import it from sklearn preprocessing and create a variable scaler which will be an instant of the scaler class.\n",
    "\n",
    "Fianlly ,we must fit the input using scaler.input(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915443aa-9ad8-4891-a88a-08fba3b799c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef9c46-3741-4a6d-a214-21d0a3141c4e",
   "metadata": {},
   "source": [
    "We will use the transformd method to get the standardized input.\n",
    "\n",
    "inputs_scaled = scaler.transform([inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24006097-49b8-47c8-ba64-f59d6278cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_scaled = scaler.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf49b2b-4694-4fee-bff3-3e412813265c",
   "metadata": {},
   "source": [
    "Note that it is not usually recommended to standardize dummy variables.\n",
    "\n",
    "Many practisioners will argue strongly against it .\n",
    "\n",
    "Scaling has no effect on the predictive power of dummies.\n",
    "\n",
    "Once scaled they will loose all their dummy meaning.\n",
    "\n",
    "we will learn ho wto implement a custom scaler , which standardizes only the continuous variables while leaving the dummies unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc2509-cfaa-4c5e-abfd-a3a3b047939e",
   "metadata": {},
   "source": [
    "##### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a091f7-5f1d-43e5-b1e4-9345e3e2138f",
   "metadata": {},
   "source": [
    "We import the train test split method from sklearn.model selection.\n",
    "Next let, x_train. x_test, y_test ,y_test =\n",
    "\n",
    "train_test_split(inputs_scaled, targets\n",
    "\n",
    "the first arguement refers to the axis ie input scaled and the second arguement refers to the y's ie target\n",
    "\n",
    "We set the size to 0.2 ,so we get an 80/20 split and we will also use a random  state so that we can lay a fiddle with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b1b17-bd77-4b0b-93f1-d529093ee333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs_scaled,targets, test_size=0.2, random_state=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93d79-dba3-4050-88f1-eb1ace67ce35",
   "metadata": {},
   "source": [
    "We have created 4 new variables, tose with the surface train(X_train ,y_train) will be used to train the \n",
    "model \n",
    "\n",
    "and those with the surface test (x_test, y_test ) will be used to test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188cf61-827a-4496-8b83-dc3b9cbfc2fd",
   "metadata": {},
   "source": [
    "##### Create the regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103a880-4b1f-47aa-ac75-48c1f6ae875e",
   "metadata": {},
   "source": [
    "A regression is created in the exact same way as we have done many times.\n",
    "\n",
    "let reg be an instance of the linear regression class.\n",
    "\n",
    "Next we fit thr regression with the train input and train target, containing x train and y train respectively.\n",
    "\n",
    "We know very well ,this is enough to create a linear regression model.\n",
    "\n",
    "It is not simply  a linear but a log linear regression as our dependent variable is the logarithm of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f3e60-a628-49d7-a414-e7d4861d184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ac122-6fa2-4c4c-ad5e-adfaea7670b9",
   "metadata": {},
   "source": [
    "A simple way to check the final result is to plot the predicted value of our regression against the observed values.\n",
    "\n",
    "The observed values are the targets.\n",
    "\n",
    "we can obtain the relative values \n",
    "\n",
    "using y_hat = reg.predict (x_train) \n",
    "\n",
    "and store then in a variable called y_hat.\n",
    "\n",
    "As you may remember ,the predictions \n",
    "\n",
    "of the linear model are labelled y_hat \n",
    "\n",
    "so is right to name the variable , this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ad95d-37dc-4928-ba0d-45e9285d1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc2f55-b3aa-4abb-9fab-1100068d4bf3",
   "metadata": {},
   "source": [
    "Lets create a scatter plot of y_train and y_hat.\n",
    "\n",
    "Or the target and the prediction.\n",
    "\n",
    "and also name the axis and limit them so that they are cullable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39f61b-f198-4c23-a0bc-c6ae51e2efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_hat)\n",
    "plt.xlabel('Targets (y_train)',size=18)\n",
    "plt.xlabel('Predictions (y_hat)',size=18)\n",
    "plt.xlim(6,13)\n",
    "plt.ylim(6,13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa12600-1335-45d7-b342-5083d07de59b",
   "metadata": {},
   "source": [
    "In an extreme scenario, if a target is 7 we want theprediction to be 7.\n",
    "\n",
    "if our target is 10 , we want our prediction to be also 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fe03d-fa1f-4b5f-af42-fd72b40ceca9",
   "metadata": {},
   "source": [
    "Altimately ,we can draw a five degree line which is the best possible match between target and prediction.\n",
    "\n",
    "The closer our scatter plot is to this line , the better the model.\n",
    "\n",
    "Our result is not perfect but it is definitely not random.\n",
    "\n",
    "We can clearly see that the points are situated between the 45 degree line so our model have passed this first check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00b06e-8f42-4667-b3c0-41e1ff127a59",
   "metadata": {},
   "source": [
    "RESIDUAL PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fca0fc-6309-4968-8f47-00c0d966ed70",
   "metadata": {},
   "source": [
    "The residuals are the differences \n",
    "\n",
    "between the target and the prediction.\n",
    "\n",
    "The residual plots refers to the \n",
    "\n",
    "distribution of the residuals.\n",
    "\n",
    "so we can simply use the \n",
    "\n",
    "sns.distplot and plot the difference \n",
    "\n",
    "between y-train and y_hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997f441-4a6a-437e-8273-01da909d1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_train - y_hat)\n",
    "plt.title(\"Residuals PDF\", size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a7a94-fe62-4c21-be5d-cb0230df7504",
   "metadata": {},
   "source": [
    "From the regression assumptions, we \n",
    "\n",
    "know that the errors must be normally \n",
    "\n",
    "distributed with the mean of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e5b18-f149-4f91-898f-5c33af0c6693",
   "metadata": {},
   "source": [
    "The residuals are the estimates of the\n",
    "\n",
    "errors so we will expect thesame from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f01318-74e9-4ba1-ae47-eda7aec5111d",
   "metadata": {},
   "source": [
    "The result looks quite normal in the terms of normally distributed.\n",
    "\n",
    "The mean seems to be 0 as well.\n",
    "\n",
    "The only possible issue might be \n",
    "\n",
    "because there is a much longer tail on\n",
    "\n",
    "the negative side ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ae4bc-72c7-430b-a5a7-be18405ae08a",
   "metadata": {},
   "source": [
    "Therefore, there are certain \n",
    "\n",
    "observations for which (y_train-y_hat)\n",
    "\n",
    "is much lower than the mean (a much \n",
    "\n",
    "higher price is predicted than is \n",
    "observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f18d54-4ec1-4038-a756-41a66894a244",
   "metadata": {},
   "source": [
    "This implies that those estimations tends to over estimate the target.\n",
    "\n",
    "For the fact that there are no such observations on the right side,\n",
    "\n",
    "we conclude that the predictions really underestimate the target.\n",
    "\n",
    "This is food for thought and can serve as a guidance on how to bwtter our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbd133-2403-4485-b65e-56d300eb958f",
   "metadata": {},
   "source": [
    "Finally ,lets calculate the R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2d478-da3a-4264-b959-9ed908efb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877b9da-0c6c-490e-82f1-f9a6adf8e120",
   "metadata": {},
   "source": [
    "From the result , our model is explaining 75% of the viariability of the data.\n",
    "\n",
    "This is a relatively good result, however,it could be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e795d63-e2cf-4c75-a227-5c00bcf738f2",
   "metadata": {},
   "source": [
    "Lets leave it as a home work at the end of the session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74387ba-fb28-4771-a74d-a44380624931",
   "metadata": {},
   "source": [
    "##### Finding th weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa40d6b-1ec5-46a1-9aac-bbde0f08bfc6",
   "metadata": {},
   "source": [
    "The bias is contained in\n",
    "\n",
    "reg.intercept_ while the weigth is \n",
    "\n",
    "contained in reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14b881-1236-4e01-b4ab-39bd4a204cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_  #to find the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612c480-8c89-4ac6-937b-81e38668d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_  # to find the weigth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7e57a-6469-4bf0-ae5d-cbc47ed54dd4",
   "metadata": {},
   "source": [
    "Since, the weights are hard to read in\n",
    "\n",
    "this way we create a summary table \n",
    "\n",
    "with the feature names and the \n",
    "\n",
    "corressponding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd63706-9d36-4bea-8678-51b1514c8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_summary = pd.DataFrame(inputs.columns.values, columns=['features'])\n",
    "reg_summary['Weights'] = reg.coef_\n",
    "reg_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a413ac-c33b-44bc-8bde-a5e84a3849d3",
   "metadata": {},
   "source": [
    "This model is far from interpreteable,\n",
    "\n",
    "the dependent variable is a logarithm \n",
    "\n",
    "and all features are standardized including the dummies.\n",
    "\n",
    "Lets work with what we have got"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fcb183-5ac8-487d-a726-ed1607ae0d8d",
   "metadata": {},
   "source": [
    "Weights interpretation\n",
    "\n",
    "\n",
    ">I . Continuous Variables:\n",
    "\n",
    "1. A positive weight shows that as a feature increases in value so do the log_price and 'price respectively.\n",
    "\n",
    "An example is the engine volume, the bigger the engine volume the higher the price.\n",
    "\n",
    "2. A negative weight shows that as a feature increases in value , log_price and 'price' decrease.\n",
    "\n",
    "ie the price of the car decreases. an example is mileage.\n",
    "\n",
    "The more a car is driven the lower the price get.\n",
    "\n",
    "> II. Dummy Variables:\n",
    "\n",
    "1. A positive weight shows that the respective category (Brand) is more expensive than the benchmark(Audi)\n",
    "\n",
    "For example, BMW coefficient is possitive ,therefore on average if  a car is a BMW it will be more expensive than an Audi.\n",
    "\n",
    "2. A negative weight shows that the respective category (Brand) is less expensive than the benchmark(Audi)\n",
    "\n",
    "Similarly if a car is Mishibushi, it will be cheaper than an Audi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d749052b-292b-45b2-924f-6bb6b0bee530",
   "metadata": {},
   "source": [
    "If you observe, the mercedez weight is positive but very close to 0.\n",
    "\n",
    "Since the dummies are scaled we cannot be sure if the mercedez is more \n",
    "\n",
    "expensive but it seems to be as expensive as an Audi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06771da9-b551-4484-bae8-a3cbc4d00da8",
   "metadata": {},
   "source": [
    "We can think about the other dummies in similar way.\n",
    "\n",
    "We display the possible categories and identify which one is the benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b55c77-7d32-4773-bc70-b08ee7209719",
   "metadata": {},
   "source": [
    "We can find out this as home work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d2cd6-b8ef-40b1-91a8-28c377d8dde8",
   "metadata": {},
   "source": [
    "The situation for dummies is a bit different.\n",
    "\n",
    "Since we dropped one category for each discrete variable when all the \n",
    "\n",
    "included dummies are 0 then the dropped dummy is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49871d50-8372-47db-9da6-f6d14a7f9932",
   "metadata": {},
   "source": [
    "Lets take Brand as an example, we quickly display all categories that used to exist using data_cleaned ['brand'].unique()\n",
    "\n",
    "From the result we can see we have 7 brands of car.\n",
    "\n",
    "but looking at th waste table ,we realised that Audi was the dropped one.\n",
    "\n",
    "Therefore, whenever all other dummies are zero, Audi is 1, Though Audi is the 'benchmark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d0564-4a4e-4d7e-a501-2648f3a0e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned['Brand'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28968b-da32-43a2-8c27-f3d9a9bbd8ad",
   "metadata": {},
   "source": [
    "Finally the size of the weight is also important.\n",
    "\n",
    "We say that standardized weight are cullpable and that is why we standardize them.\n",
    "\n",
    "Clearly, mileage is the most prominent feature in this regression and is more than twice as important as engine volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb5ca7-b044-48ba-870e-a95c6edad1ae",
   "metadata": {},
   "source": [
    "It is also worth noting that it doesnt\n",
    "\n",
    "make sense to compare the continuous \n",
    "\n",
    "variables with the dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff1822-83d9-4794-90bb-2ec53145ea25",
   "metadata": {},
   "source": [
    "Dummies are compared with the \n",
    "\n",
    "benchmark dummies for the respective \n",
    "\n",
    "discrete variables\n",
    "\n",
    "Like we compared the Brands with Audi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7801d-79c2-4f0c-b940-f56e7a975e77",
   "metadata": {},
   "source": [
    "###### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b4bac-81b5-4a82-9727-c21c99772b74",
   "metadata": {},
   "source": [
    "What are the reference (benchmark) categories for each categorical variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5e155-2381-4ca1-9457-2f0a3e582257",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression Practical Example (Part 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab7a32-cf32-4bf7-b1c4-f6be13583e74",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad4eb5-837c-4d39-932f-275ef3f3a3d2",
   "metadata": {},
   "source": [
    "Here, we will be finding out how our model performed.\n",
    "\n",
    "We start the testing part by finding our predictions.\n",
    "\n",
    "We can store then in y_hat_test and use reg.predict method with arguement x test to compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ed80844-8195-42b2-867b-c6c758f967e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13020/83873669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_hat_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_hat_test = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3d297-d83c-494c-b38c-7593f5341896",
   "metadata": {},
   "source": [
    "Next, we plot the test target against the predicted target and see if it resembles the 45 degree line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f0031-4016-4a35-9802-a72d74726a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_hat_test)\n",
    "plt.xlabel('Targets (y_test)',size=18)\n",
    "plt.xlabel('Predictions (y_hat_test)',size=18)\n",
    "plt.xlim(6,13)\n",
    "plt.ylim(6,13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dda0e-9894-4e8f-a7aa-7921f48e4bf9",
   "metadata": {},
   "source": [
    "From the graph, we can see that for hiher prices we have a higher concentration of values around the 45 degree line.\n",
    "\n",
    "Therefore ,our model is very good at predicting higher prices.\n",
    "\n",
    "However, for lower ones the situation  is not so amazing.\n",
    "\n",
    "There are much more scattered pointing to the fact that we are not quite geting the prices right.\n",
    "\n",
    "We can include an additional arguement of the scatter plot caled alpha which will show the proportional capacity of the plot.\n",
    "\n",
    "Alpha takes values from 0 to 1 with 1 being the default.\n",
    "\n",
    "Lets set ours to 0.2.\n",
    "\n",
    "You can fiddle with the values as you like\n",
    "\n",
    "Plt.scater(x,y[, alpha]) creates a scatter \n",
    "\n",
    "plot alpha specifies the oacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9dd13-c6c6-4722-bc90-53e3e368873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_hat_test, alpha=0.2)\n",
    "plt.xlabel('Targets (y_test)',size=18)\n",
    "plt.xlabel('Predictions (y_hat_test)',size=18)\n",
    "plt.xlim(6,13)\n",
    "plt.ylim(6,13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f1466-fb8f-4075-82a8-e600ed3eff98",
   "metadata": {},
   "source": [
    "This turns our graph into a hip map of sort.\n",
    "\n",
    "the most saurated the colour, the higher the concentration of points.\n",
    "\n",
    "The paler the colour the lower the concentration of the point.\n",
    "\n",
    "Thus we have a much clearer indication that most of the points are indeed very close to the 45 degree line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ecc30a-b868-4afd-91aa-1c890a1693fc",
   "metadata": {},
   "source": [
    "If you aree wondering how good thees predictions are \n",
    "\n",
    "we will explain in a bit, although is unconventional, we can manually explore what the algorithm came up with.\n",
    "\n",
    "this will give us yet more food for thought about what we have accomplished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f223e-62fe-420b-92b9-193592a5a237",
   "metadata": {},
   "source": [
    "Lets create a new dataframe called df_pf\n",
    "\n",
    "standing for dataframe performance.\n",
    "\n",
    "data in ti will consist of the prediction, \n",
    "\n",
    "hence,y_hat_test while the column name will\n",
    "\n",
    "be simply 'predicton'\n",
    "\n",
    "Unfortunately ,these are the predictions \n",
    "\n",
    "for the log-prices,\n",
    "\n",
    "Normally, we prefer the prices nd not the \n",
    "\n",
    "logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab3586-fd1a-4370-9f7c-173c049fab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pf = pd.DataFrame(y_hat_test, columns=['prediction'])\n",
    "df_pf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f48e6-8102-456d-b8ce-cf4295dab99f",
   "metadata": {},
   "source": [
    "Since the log is the opposite of the \n",
    "\n",
    "exponential, if we take the exponentials of\n",
    "\n",
    "the log prices we will reach the original prices.\n",
    "\n",
    "\n",
    "  exp(In(x)) = [x]  \n",
    "  \n",
    " (for a positive x)\n",
    " \n",
    " log(exp)x)) = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4f971-37a9-474c-8d90-5287195a7c4c",
   "metadata": {},
   "source": [
    "An easy way to achieve that is through the \n",
    "\n",
    "numpy exp method.\n",
    "lets replace that in the exp df decoration\n",
    "\n",
    "\n",
    "np.exp(x)returns the exponential of x (the \n",
    "\n",
    "Euler number 'e' to the power of x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea347c6-9993-4a49-9891-605ec802dfea",
   "metadata": {},
   "source": [
    "lets replace that in the ex df decoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a583a-3892-4df0-a984-c0a505b86c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = pd.DataFrame(np.exp(y_hat_test), colums=['Predictions'])\n",
    "df_pf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca99714-947f-4d00-8abf-65dc403a0680",
   "metadata": {},
   "source": [
    "We now have the\n",
    "\n",
    "predictions expressed as prices.\n",
    "\n",
    "After that we place the target or y_test\n",
    "\n",
    "next to them in the same DataFrame so that \n",
    "\n",
    "we can easily compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303da838-0aac-4f9e-ad72-6a7dec0d3cff",
   "metadata": {},
   "source": [
    "Lets put he exponential of y_test in \n",
    "\n",
    "df_pf['Target'] and check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff6b63-5284-4426-9630-a61e4dbcd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pf['Target'] = np.exp(y_test)\n",
    "pd_pf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe657c2-600b-4108-a3fe-18dfc5966c2e",
   "metadata": {},
   "source": [
    "Unfortunately ,we have alot if missing values.\n",
    "\n",
    "And they are quite randomly spread .\n",
    "\n",
    "To understand issues like this one , we can \n",
    "\n",
    "explore the y_test dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783cc3ad-58fc-48e6-8d67-33a41e1bbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a7a28-03c7-412a-ad1d-30c328c69816",
   "metadata": {},
   "source": [
    "If we observe, it contains indexes.\n",
    "\n",
    "Infact,when we split the data into train \n",
    "\n",
    "and test, the original indexes were \n",
    "\n",
    "preserved.\n",
    "\n",
    "When we add y_test to df_pf, pandas tried\n",
    "\n",
    "to match the indices.\n",
    "\n",
    "What we need is to forget this original \n",
    "\n",
    "indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae6808-a122-44d0-93f7-a3d4da8c9628",
   "metadata": {},
   "source": [
    "The easiest way to achieve that is to \n",
    "\n",
    "overite y_test with y_test, reset index \n",
    "\n",
    "with an arguement drop=True.\n",
    "\n",
    "This operation will reset the index of y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395402b-1c92-4fa2-b118-3a50ecccbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index(drop=True)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3fac6-8e1c-4da9-b6f7-9e79d009ea32",
   "metadata": {},
   "source": [
    "Lets overite the exponential of y_test in pf_pf['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90fdf7-2bf5-4674-b694-1278c8596899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pf['Target'] = np.exp(y_test)\n",
    "df_pf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49585f21-c576-420d-9a37-70ff7c9294f8",
   "metadata": {},
   "source": [
    "We get the result we initially tried to achieve.\n",
    "\n",
    "Now we have the predictions and the target and we can proceed to comparing them.\n",
    "\n",
    "This can be doing manually or since we are coding, we could employ some code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6156a1-c96c-4d91-abb7-ace0f816b9d5",
   "metadata": {},
   "source": [
    "Lets create another column named 'Residual'\n",
    "which will be equal to the difference between the target and the prediction.\n",
    "\n",
    "You can also call it difference if you wish but it will be precisely the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06480835-e8ae-4d7f-9706-6c2cb6e6519a",
   "metadata": {},
   "source": [
    "This whole dataframe makes alot of sense when evaluatuing the regression.\n",
    "\n",
    "The main reason is that OLS(ordinary least square) frame work is based on minimizing SSE.\n",
    "\n",
    "This is achieved by minimizing the sum of squares residuals.\n",
    "\n",
    "Therefore, examing the residual is same as examing the heart of the optimization algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78047017-c042-40a5-9540-8e99bf0d82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pf['Residual'] = df_pf['Target'] - df_pf['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dae46-1895-4156-a51e-54a159df3ce0",
   "metadata": {},
   "source": [
    "Lets add the difference in percentages to make it easier to evaluate the output and the target.\n",
    "\n",
    "Difference in percentages in several different ways but we will be using this formula\n",
    "\n",
    "The residual divided by the target and multiplied by 100 to reach the percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ecba41-cf25-4d92-8905-e7550a64128b",
   "metadata": {},
   "source": [
    "Next is to display the absolute difference in percentages.\n",
    "\n",
    "Whether an observation is off by +1% or\n",
    "\n",
    "-1%, is mostly irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1adc7-096e-44ac-b4fa-66073542522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pf['Difference%'] = np.absolute(df_pf['Residual']/df_pf['Target']*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38bd2c-72c1-490b-8a68-2bf464831b9d",
   "metadata": {},
   "source": [
    "To evaluate the aggregate performance, we can print the descriptive statistics of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147bf9e-2bd8-4692-a460-0e3bf049da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a909ad-7bac-43e5-8885-8ec1794eca9a",
   "metadata": {},
   "source": [
    "Lets gothrough some highlights.\n",
    "\n",
    "The minimum difference in percentages is 0.06%\n",
    "so the output was spot on. \n",
    "\n",
    "The maximum difference in percenage is 512% and this is pretty off mark.\n",
    "\n",
    "Aside the minimum and maximum, we have the percentiles.\n",
    "\n",
    "All of these tells us that for most of our predictions, we got relatively close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56048e0f-0bc2-4b55-9d0d-2becb3c8d9fa",
   "metadata": {},
   "source": [
    "At this point, is important to manually isit the dataframe.\n",
    "\n",
    "Lets sort,df_pf by difference in percentages using the sort_values method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb214fc-d215-418a-8410-c9cf5d1cadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pf.sort_values(by=['Difference%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a6dd5-40ec-4b56-98bf-1978d176b58c",
   "metadata": {},
   "source": [
    "By default , pandas doesnt show us the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6630dba-e2d9-4209-a4a2-6005aa2055d4",
   "metadata": {},
   "source": [
    "If we like to see all rows, we can set the pandas dispaly option to show 999 rows,\n",
    "\n",
    "In additoin to make the dataset more readeable, we can cahnge the display options for float to 2 digits after the dot using the expression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310d1be-a009-4437-91ec-d292db2489af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.set_option('dispaly.float_format', lambda x: '%.2f' % x)\n",
    "df_pf.sort_values(by=['Difference%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21a846-e9e7-4639-86d7-f490ad4f01c5",
   "metadata": {},
   "source": [
    "Revisiting this dataframe manualy, we can see for which type of observations we have\n",
    "\n",
    "got good predictions and for which we are very far off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90facbf1-b497-410e-b81b-e76aa164e356",
   "metadata": {},
   "source": [
    "It is particularly important to explore the ones that have the biggest differnces.\n",
    "\n",
    "Going to the button of the dataframe ,we see that there are few predictions dramatically far off from the observed values.\n",
    "\n",
    "If you look closely at the observed colums , you wil notice that the observed prices are extremely low. 3000,4500,1700 etc\n",
    "\n",
    "As we know is using the  mileage , engine volume, Brand, registration and body type to predict the price of a used car.\n",
    "\n",
    "On average,it is very decent at predicting the price.\n",
    "\n",
    "But for this last sample though it isnt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b921af7-4e26-4281-b267-004bc5f553e9",
   "metadata": {},
   "source": [
    "It is important to note that all residuals for these outliers are negative.\n",
    "\n",
    "Therefore ,the predictions are higher than the target.\n",
    "\n",
    "An explation may be that we are missing an important factor which drives the price of the used car lower.\n",
    "\n",
    "It maybe the model of the car we remove at the beginning of the analysis or it may also be that the car is damaged in some way.\n",
    "\n",
    "A piece of information which which we did not have but we should have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd97442-1263-473b-9cd6-783804b53d42",
   "metadata": {},
   "source": [
    "##### How to make our model better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39391a8e-7cc8-420c-adc9-52913c09dac0",
   "metadata": {},
   "source": [
    "There are different ways we can make our model better.\n",
    "\n",
    "1. We can use a diferent set of variables. \n",
    "\n",
    "2. We can Remove a bigger part of the outliers observations.\n",
    "\n",
    "3. Use differnt kinds of transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb934c-1c75-4e00-9947-8364ba0d3b24",
   "metadata": {},
   "source": [
    "The model we just created is not outstanding .\n",
    "\n",
    "The creation of a truly great model for a certain problem can take months or even years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02596e09-2f13-432b-8252-dfee522a8160",
   "metadata": {},
   "source": [
    "Machine learning requires us to re-visit the model many times before we are truly happy with the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f11b40-358e-4168-8384-9bdb2e0cf1d2",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "You can try to improve the model on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0fad56-97a8-492f-8df8-f73f751d0a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
